<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="CUDA 12 inference backend for Unsloth with multi-GPU support on Kaggle. Deploy fine-tuned models on dual Tesla T4 GPUs with llama.cpp server, GGUF quantization, and Graphistry visualization. Run 70B models with tensor-split architecture."><meta name=author content="Waqas Muhammad"><link href=https://llcuda.github.io/ rel=canonical><link href=guides/quickstart/ rel=next><link rel=icon href=assets/images/logo.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth - llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth</title><link rel=stylesheet href=assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=stylesheets/extra.css><script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#llcuda-v220-cuda12-inference-backend-for-unsloth class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=. title="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" class="md-header__button md-logo" aria-label="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" data-md-component=logo> <img src=assets/images/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=guides/quickstart/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=kaggle/overview/ class=md-tabs__link> Kaggle Dual T4 </a> </li> <li class=md-tabs__item> <a href=tutorials/ class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=architecture/overview/ class=md-tabs__link> Architecture </a> </li> <li class=md-tabs__item> <a href=api/overview/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=unsloth/overview/ class=md-tabs__link> Unsloth Integration </a> </li> <li class=md-tabs__item> <a href=graphistry/overview/ class=md-tabs__link> Graphistry & Visualization </a> </li> <li class=md-tabs__item> <a href=performance/benchmarks/ class=md-tabs__link> Performance </a> </li> <li class=md-tabs__item> <a href=gguf/overview/ class=md-tabs__link> GGUF & Quantization </a> </li> <li class=md-tabs__item> <a href=guides/model-selection/ class=md-tabs__link> Guides </a> </li> <li class=md-tabs__item> <a href=https://github.com/llcuda/llcuda class=md-tabs__link> Links </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=. title="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" class="md-nav__button md-logo" aria-label="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" data-md-component=logo> <img src=assets/images/logo.svg alt=logo> </a> llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth </label> <div class=md-nav__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </label> <a href=. class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Home </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-llcuda-v220 class=md-nav__link> <span class=md-ellipsis> What is llcuda v2.2.0? </span> </a> </li> <li class=md-nav__item> <a href=#core-architecture class=md-nav__link> <span class=md-ellipsis> Core Architecture </span> </a> <nav class=md-nav aria-label="Core Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#split-gpu-configuration class=md-nav__link> <span class=md-ellipsis> Split-GPU Configuration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#quick-start-5-minutes class=md-nav__link> <span class=md-ellipsis> Quick Start (5 Minutes) </span> </a> <nav class=md-nav aria-label="Quick Start (5 Minutes)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#step-1-install-llcuda class=md-nav__link> <span class=md-ellipsis> Step 1: Install llcuda </span> </a> </li> <li class=md-nav__item> <a href=#step-2-verify-dual-t4-setup class=md-nav__link> <span class=md-ellipsis> Step 2: Verify Dual T4 Setup </span> </a> </li> <li class=md-nav__item> <a href=#step-3-run-basic-inference class=md-nav__link> <span class=md-ellipsis> Step 3: Run Basic Inference </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#key-features-of-v220 class=md-nav__link> <span class=md-ellipsis> Key Features of v2.2.0 </span> </a> <nav class=md-nav aria-label="Key Features of v2.2.0"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-multi-gpu-inference-on-kaggle class=md-nav__link> <span class=md-ellipsis> 1. Multi-GPU Inference on Kaggle </span> </a> </li> <li class=md-nav__item> <a href=#2-unsloth-fine-tuning-pipeline class=md-nav__link> <span class=md-ellipsis> 2. Unsloth Fine-Tuning Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#3-split-gpu-architecture-with-graphistry class=md-nav__link> <span class=md-ellipsis> 3. Split-GPU Architecture with Graphistry </span> </a> </li> <li class=md-nav__item> <a href=#4-29-gguf-quantization-formats class=md-nav__link> <span class=md-ellipsis> 4. 29 GGUF Quantization Formats </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#performance-benchmarks class=md-nav__link> <span class=md-ellipsis> Performance Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#tutorial-notebooks-10-kaggle-notebooks class=md-nav__link> <span class=md-ellipsis> Tutorial Notebooks (10 Kaggle Notebooks) </span> </a> </li> <li class=md-nav__item> <a href=#learning-paths class=md-nav__link> <span class=md-ellipsis> Learning Paths </span> </a> </li> <li class=md-nav__item> <a href=#whats-new-in-v220 class=md-nav__link> <span class=md-ellipsis> What's New in v2.2.0 </span> </a> </li> <li class=md-nav__item> <a href=#technical-architecture class=md-nav__link> <span class=md-ellipsis> Technical Architecture </span> </a> </li> <li class=md-nav__item> <a href=#api-reference class=md-nav__link> <span class=md-ellipsis> API Reference </span> </a> </li> <li class=md-nav__item> <a href=#community-support class=md-nav__link> <span class=md-ellipsis> Community &amp; Support </span> </a> </li> <li class=md-nav__item> <a href=#license class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=guides/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=guides/installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=guides/first-steps/ class=md-nav__link> <span class=md-ellipsis> First Steps </span> </a> </li> <li class=md-nav__item> <a href=guides/kaggle-setup/ class=md-nav__link> <span class=md-ellipsis> Kaggle Setup </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Kaggle Dual T4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Kaggle Dual T4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=kaggle/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=kaggle/dual-gpu-setup/ class=md-nav__link> <span class=md-ellipsis> Dual GPU Setup </span> </a> </li> <li class=md-nav__item> <a href=kaggle/multi-gpu-inference/ class=md-nav__link> <span class=md-ellipsis> Multi-GPU Inference </span> </a> </li> <li class=md-nav__item> <a href=kaggle/tensor-split/ class=md-nav__link> <span class=md-ellipsis> Tensor Split Configuration </span> </a> </li> <li class=md-nav__item> <a href=kaggle/large-models/ class=md-nav__link> <span class=md-ellipsis> Large Models (70B) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=tutorials/ class="md-nav__link "> <span class=md-ellipsis> Tutorials </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=tutorials/01-quickstart/ class=md-nav__link> <span class=md-ellipsis> 01 - Quick Start </span> </a> </li> <li class=md-nav__item> <a href=tutorials/02-server-setup/ class=md-nav__link> <span class=md-ellipsis> 02 - Server Setup </span> </a> </li> <li class=md-nav__item> <a href=tutorials/03-multi-gpu/ class=md-nav__link> <span class=md-ellipsis> 03 - Multi-GPU </span> </a> </li> <li class=md-nav__item> <a href=tutorials/04-gguf-quantization/ class=md-nav__link> <span class=md-ellipsis> 04 - GGUF Quantization </span> </a> </li> <li class=md-nav__item> <a href=tutorials/05-unsloth-integration/ class=md-nav__link> <span class=md-ellipsis> 05 - Unsloth Integration </span> </a> </li> <li class=md-nav__item> <a href=tutorials/06-split-gpu-graphistry/ class=md-nav__link> <span class=md-ellipsis> 06 - Split-GPU + Graphistry </span> </a> </li> <li class=md-nav__item> <a href=tutorials/07-openai-api/ class=md-nav__link> <span class=md-ellipsis> 07 - OpenAI API </span> </a> </li> <li class=md-nav__item> <a href=tutorials/08-nccl-pytorch/ class=md-nav__link> <span class=md-ellipsis> 08 - NCCL + PyTorch </span> </a> </li> <li class=md-nav__item> <a href=tutorials/09-large-models/ class=md-nav__link> <span class=md-ellipsis> 09 - Large Models </span> </a> </li> <li class=md-nav__item> <a href=tutorials/10-complete-workflow/ class=md-nav__link> <span class=md-ellipsis> 10 - Complete Workflow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=architecture/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=architecture/split-gpu/ class=md-nav__link> <span class=md-ellipsis> Split-GPU Design </span> </a> </li> <li class=md-nav__item> <a href=architecture/gpu0-llm/ class=md-nav__link> <span class=md-ellipsis> GPU0 - LLM Inference </span> </a> </li> <li class=md-nav__item> <a href=architecture/gpu1-graphistry/ class=md-nav__link> <span class=md-ellipsis> GPU1 - Graphistry </span> </a> </li> <li class=md-nav__item> <a href=architecture/tensor-split-vs-nccl/ class=md-nav__link> <span class=md-ellipsis> Tensor Split vs NCCL </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=api/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=api/client/ class=md-nav__link> <span class=md-ellipsis> LlamaCppClient </span> </a> </li> <li class=md-nav__item> <a href=api/multigpu/ class=md-nav__link> <span class=md-ellipsis> Multi-GPU Config </span> </a> </li> <li class=md-nav__item> <a href=api/gguf/ class=md-nav__link> <span class=md-ellipsis> GGUF Tools </span> </a> </li> <li class=md-nav__item> <a href=api/nccl/ class=md-nav__link> <span class=md-ellipsis> NCCL Integration </span> </a> </li> <li class=md-nav__item> <a href=api/server/ class=md-nav__link> <span class=md-ellipsis> Server Manager </span> </a> </li> <li class=md-nav__item> <a href=api/graphistry/ class=md-nav__link> <span class=md-ellipsis> Graphistry Integration </span> </a> </li> <li class=md-nav__item> <a href=api/models/ class=md-nav__link> <span class=md-ellipsis> Models & Loading </span> </a> </li> <li class=md-nav__item> <a href=api/examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Unsloth Integration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Unsloth Integration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=unsloth/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=unsloth/fine-tuning/ class=md-nav__link> <span class=md-ellipsis> Fine-Tuning Workflow </span> </a> </li> <li class=md-nav__item> <a href=unsloth/gguf-export/ class=md-nav__link> <span class=md-ellipsis> GGUF Export </span> </a> </li> <li class=md-nav__item> <a href=unsloth/deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment Pipeline </span> </a> </li> <li class=md-nav__item> <a href=unsloth/best-practices/ class=md-nav__link> <span class=md-ellipsis> Best Practices </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex=0> <span class=md-ellipsis> Graphistry & Visualization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Graphistry & Visualization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=graphistry/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=graphistry/knowledge-graphs/ class=md-nav__link> <span class=md-ellipsis> Knowledge Graph Extraction </span> </a> </li> <li class=md-nav__item> <a href=graphistry/rapids/ class=md-nav__link> <span class=md-ellipsis> RAPIDS Integration </span> </a> </li> <li class=md-nav__item> <a href=graphistry/split-gpu-setup/ class=md-nav__link> <span class=md-ellipsis> Split-GPU Architecture </span> </a> </li> <li class=md-nav__item> <a href=graphistry/examples/ class=md-nav__link> <span class=md-ellipsis> Visualization Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_9> <label class=md-nav__link for=__nav_9 id=__nav_9_label tabindex=0> <span class=md-ellipsis> Performance </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=false> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> Performance </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=performance/benchmarks/ class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=performance/dual-t4-results/ class=md-nav__link> <span class=md-ellipsis> Dual T4 Results </span> </a> </li> <li class=md-nav__item> <a href=performance/optimization/ class=md-nav__link> <span class=md-ellipsis> Optimization Guide </span> </a> </li> <li class=md-nav__item> <a href=performance/memory/ class=md-nav__link> <span class=md-ellipsis> Memory Management </span> </a> </li> <li class=md-nav__item> <a href=performance/flash-attention/ class=md-nav__link> <span class=md-ellipsis> FlashAttention </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_10> <label class=md-nav__link for=__nav_10 id=__nav_10_label tabindex=0> <span class=md-ellipsis> GGUF & Quantization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_10_label aria-expanded=false> <label class=md-nav__title for=__nav_10> <span class="md-nav__icon md-icon"></span> GGUF & Quantization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=gguf/overview/ class=md-nav__link> <span class=md-ellipsis> GGUF Format Overview </span> </a> </li> <li class=md-nav__item> <a href=gguf/k-quants/ class=md-nav__link> <span class=md-ellipsis> K-Quants Guide </span> </a> </li> <li class=md-nav__item> <a href=gguf/i-quants/ class=md-nav__link> <span class=md-ellipsis> I-Quants Guide </span> </a> </li> <li class=md-nav__item> <a href=gguf/selection/ class=md-nav__link> <span class=md-ellipsis> Quantization Selection </span> </a> </li> <li class=md-nav__item> <a href=gguf/vram-estimation/ class=md-nav__link> <span class=md-ellipsis> VRAM Estimation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_11> <label class=md-nav__link for=__nav_11 id=__nav_11_label tabindex=0> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_11_label aria-expanded=false> <label class=md-nav__title for=__nav_11> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=guides/model-selection/ class=md-nav__link> <span class=md-ellipsis> Model Selection </span> </a> </li> <li class=md-nav__item> <a href=guides/troubleshooting/ class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> </li> <li class=md-nav__item> <a href=guides/faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class=md-nav__item> <a href=guides/build-from-source/ class=md-nav__link> <span class=md-ellipsis> Build from Source </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_12> <label class=md-nav__link for=__nav_12 id=__nav_12_label tabindex=0> <span class=md-ellipsis> Links </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_12_label aria-expanded=false> <label class=md-nav__title for=__nav_12> <span class="md-nav__icon md-icon"></span> Links </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda class=md-nav__link> <span class=md-ellipsis> GitHub Repository </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/releases class=md-nav__link> <span class=md-ellipsis> GitHub Releases </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/releases/tag/v2.2.0 class=md-nav__link> <span class=md-ellipsis> v2.2.0 Release </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/issues class=md-nav__link> <span class=md-ellipsis> Issues & Support </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/blob/main/CHANGELOG.md class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=llcuda-v220-cuda12-inference-backend-for-unsloth>llcuda v2.2.0: CUDA12 Inference Backend for Unsloth<a class=headerlink href=#llcuda-v220-cuda12-inference-backend-for-unsloth title="Permanent link">&para;</a></h1> <div style="text-align: center; margin: 2em 0;"> <img src=https://img.shields.io/badge/version-2.2.0-blue.svg alt="Version 2.2.0"> <img src=https://img.shields.io/badge/python-3.11+-brightgreen.svg alt="Python 3.11+"> <img src=https://img.shields.io/badge/CUDA-12.x-green.svg alt="CUDA 12.x"> <img src=https://img.shields.io/badge/Kaggle-2Ã—T4-orange.svg alt="Kaggle 2Ã—T4"> <img src=https://img.shields.io/badge/license-MIT-blue.svg alt="MIT License"> </div> <p><strong>CUDA12-first inference backend for Unsloth with Graphistry network visualization on Kaggle dual Tesla T4 GPUs.</strong> Fine-tune with Unsloth â†’ Export to GGUF â†’ Deploy on Kaggle â†’ Visualize with Graphistry.</p> <hr> <h2 id=what-is-llcuda-v220><img alt=ğŸš€ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f680.svg title=:rocket:> What is llcuda v2.2.0?<a class=headerlink href=#what-is-llcuda-v220 title="Permanent link">&para;</a></h2> <p>llcuda is a <strong>CUDA 12 inference backend</strong> specifically designed for deploying <a href=https://unsloth.ai/ >Unsloth</a>-fine-tuned models on <strong>Kaggle's dual Tesla T4 GPUs</strong> (30GB total VRAM). It provides:</p> <div class="grid cards"> <ul> <li> <p>:material-gpu: <strong>Dual T4 Architecture</strong></p> <hr> <p>Run on Kaggle's <strong>2Ã— Tesla T4 GPUs</strong> (15GB each)</p> <ul> <li>Native CUDA tensor-split for multi-GPU</li> <li>Support for 70B models with IQ3_XS quantization</li> <li>FlashAttention for 2-3x faster inference</li> <li>961MB pre-built CUDA 12.5 binaries</li> </ul> </li> <li> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 17v2h1a1 1 0 0 1 1 1h7v2h-7a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1H2v-2h7a1 1 0 0 1 1-1h1v-2H5v-1.5c0-1.93 3.13-3.5 7-3.5s7 1.57 7 3.5V17zM12 3a3.5 3.5 0 0 1 3.5 3.5A3.5 3.5 0 0 1 12 10a3.5 3.5 0 0 1-3.5-3.5A3.5 3.5 0 0 1 12 3"/></svg></span> <strong>Split-GPU Design</strong></p> <hr> <p>Unique architecture: <strong>LLM on GPU 0 + Graphistry on GPU 1</strong></p> <ul> <li>GPU 0: llama.cpp server for LLM inference</li> <li>GPU 1: RAPIDS cuGraph + Graphistry visualization</li> <li>Extract knowledge graphs from LLM outputs</li> <li>Visualize millions of nodes and edges</li> </ul> </li> <li> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M11 15H6l7-14v8h5l-7 14z"/></svg></span> <strong>Unsloth Integration</strong></p> <hr> <p>Seamless workflow from training to deployment</p> <ul> <li>Fine-tune with Unsloth (2x faster training)</li> <li>Export to GGUF format with <code>save_pretrained_gguf()</code></li> <li>Deploy with llcuda on Kaggle</li> <li>Complete end-to-end pipeline</li> </ul> </li> <li> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m16 11.78 4.24-7.33 1.73 1-5.23 9.05-6.51-3.75L5.46 19H22v2H2V3h2v14.54L9.5 8z"/></svg></span> <strong>Production Ready</strong></p> <hr> <p>Built for Kaggle production workloads</p> <ul> <li>OpenAI-compatible API via llama-server</li> <li>29 quantization formats (K-quants, I-quants)</li> <li>NCCL support for PyTorch distributed</li> <li>Auto-download binaries from GitHub Releases</li> </ul> </li> </ul> </div> <hr> <h2 id=core-architecture><img alt=ğŸ”¥ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f525.svg title=:fire:> Core Architecture<a class=headerlink href=#core-architecture title="Permanent link">&para;</a></h2> <p>llcuda v2.2.0 implements a unique <strong>split-GPU architecture</strong> for Kaggle's dual T4 environment:</p> <pre class=mermaid><code>graph LR
    A[Unsloth Fine-Tuning] --&gt; B[GGUF Export]
    B --&gt; C[llcuda Deployment]
    C --&gt; D[GPU 0: LLM Inference]
    C --&gt; E[GPU 1: Graphistry Viz]
    D --&gt; F[Knowledge Extraction]
    F --&gt; E
    E --&gt; G[Graph Visualization]

    style D fill:#4CAF50
    style E fill:#2196F3
    style C fill:#FF9800</code></pre> <h3 id=split-gpu-configuration>Split-GPU Configuration<a class=headerlink href=#split-gpu-configuration title="Permanent link">&para;</a></h3> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>â”‚              KAGGLE DUAL T4 SPLIT-GPU ARCHITECTURE             â”‚
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>â”‚                                                                 â”‚
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>â”‚   GPU 0: Tesla T4 (15GB)          GPU 1: Tesla T4 (15GB)       â”‚
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>â”‚   â”‚  llama-server       â”‚         â”‚  RAPIDS cuDF        â”‚     â”‚
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>â”‚   â”‚  GGUF Model         â”‚ â”€â”€â”€â”€â”€â”€â”€&gt;â”‚  cuGraph            â”‚     â”‚
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>â”‚   â”‚  LLM Inference      â”‚  extract â”‚  Graphistry[ai]     â”‚     â”‚
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>â”‚   â”‚  ~5-12GB VRAM       â”‚  graphs  â”‚  Network Viz        â”‚     â”‚
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>â”‚                                                                 â”‚
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>â”‚   â€¢ tensor-split for multi-GPU   â€¢ Millions of nodes/edges    â”‚
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>â”‚   â€¢ FlashAttention enabled       â€¢ GPU-accelerated rendering  â”‚
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>â”‚   â€¢ OpenAI API compatible        â€¢ Interactive exploration    â”‚
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>â”‚                                                                 â”‚
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</span></code></pre></div> <hr> <h2 id=quick-start-5-minutes><img alt=âš¡ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/26a1.svg title=:zap:> Quick Start (5 Minutes)<a class=headerlink href=#quick-start-5-minutes title="Permanent link">&para;</a></h2> <p>Get llcuda v2.2.0 running on Kaggle in just 5 minutes!</p> <h3 id=step-1-install-llcuda>Step 1: Install llcuda<a class=headerlink href=#step-1-install-llcuda title="Permanent link">&para;</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># On Kaggle notebook</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>pip<span class=w> </span>install<span class=w> </span>git+https://github.com/llcuda/llcuda.git@v2.2.0
</span></code></pre></div> <h3 id=step-2-verify-dual-t4-setup>Step 2: Verify Dual T4 Setup<a class=headerlink href=#step-2-verify-dual-t4-setup title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.api.multigpu</span><span class=w> </span><span class=kn>import</span> <span class=n>detect_gpus</span><span class=p>,</span> <span class=n>print_gpu_info</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=c1># Check GPU configuration</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=n>gpus</span> <span class=o>=</span> <span class=n>detect_gpus</span><span class=p>()</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;âœ“ Detected </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>gpus</span><span class=p>)</span><span class=si>}</span><span class=s2> GPUs&quot;</span><span class=p>)</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=n>print_gpu_info</span><span class=p>()</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=c1># Expected output:</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=c1># âœ“ Detected 2 GPUs</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=c1># GPU 0: Tesla T4 (15.0 GB)</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=c1># GPU 1: Tesla T4 (15.0 GB)</span>
</span></code></pre></div> <h3 id=step-3-run-basic-inference>Step 3: Run Basic Inference<a class=headerlink href=#step-3-run-basic-inference title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.server</span><span class=w> </span><span class=kn>import</span> <span class=n>ServerManager</span><span class=p>,</span> <span class=n>ServerConfig</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=c1># Configure for single GPU (GPU 0)</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=n>config</span> <span class=o>=</span> <span class=n>ServerConfig</span><span class=p>(</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>    <span class=n>model_path</span><span class=o>=</span><span class=s2>&quot;model.gguf&quot;</span><span class=p>,</span>  <span class=c1># Your GGUF model</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    <span class=n>n_gpu_layers</span><span class=o>=</span><span class=mi>99</span><span class=p>,</span>          <span class=c1># Offload all layers to GPU</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>    <span class=n>flash_attn</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>          <span class=c1># Enable FlashAttention</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=p>)</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=c1># Start server</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=n>server</span> <span class=o>=</span> <span class=n>ServerManager</span><span class=p>()</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=n>server</span><span class=o>.</span><span class=n>start_with_config</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=n>server</span><span class=o>.</span><span class=n>wait_until_ready</span><span class=p>()</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=c1># Use OpenAI API</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.api</span><span class=w> </span><span class=kn>import</span> <span class=n>LlamaCppClient</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a><span class=n>client</span> <span class=o>=</span> <span class=n>LlamaCppClient</span><span class=p>(</span><span class=s2>&quot;http://localhost:8080&quot;</span><span class=p>)</span>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a><span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&quot;role&quot;</span><span class=p>:</span> <span class=s2>&quot;user&quot;</span><span class=p>,</span> <span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=s2>&quot;Explain quantum computing&quot;</span><span class=p>}],</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>    <span class=n>max_tokens</span><span class=o>=</span><span class=mi>200</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a><span class=p>)</span>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></code></pre></div> <div class="admonition success"> <p class=admonition-title>Auto-Download Binaries</p> <p>CUDA binaries (961 MB) download automatically from <a href=https://github.com/llcuda/llcuda/releases/tag/v2.2.0>GitHub Releases v2.2.0</a> on first import. Cached for future runs!</p> </div> <p><a href=guides/installation/ class="md-button md-button--primary"><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.322.75h1.176a1.75 1.75 0 0 1 1.75 1.749v1.177a10.75 10.75 0 0 1-2.925 7.374l-1.228 1.304a24 24 0 0 1-1.596 1.542v5.038c0 .615-.323 1.184-.85 1.5l-4.514 2.709a.75.75 0 0 1-1.12-.488l-.963-4.572a1.3 1.3 0 0 1-.14-.129L8.04 15.96l-1.994-1.873a1.3 1.3 0 0 1-.129-.14l-4.571-.963a.75.75 0 0 1-.49-1.12l2.71-4.514c.316-.527.885-.85 1.5-.85h5.037a24 24 0 0 1 1.542-1.594l1.304-1.23A10.75 10.75 0 0 1 20.321.75Zm-6.344 4.018v-.001l-1.304 1.23a22.3 22.3 0 0 0-3.255 3.851l-2.193 3.29 1.859 1.744.034.034 1.743 1.858 3.288-2.192a22.3 22.3 0 0 0 3.854-3.257l1.228-1.303a9.25 9.25 0 0 0 2.517-6.346V2.5a.25.25 0 0 0-.25-.25h-1.177a9.25 9.25 0 0 0-6.344 2.518M6.5 21c-1.209 1.209-3.901 1.445-4.743 1.49a.24.24 0 0 1-.18-.067.24.24 0 0 1-.067-.18c.045-.842.281-3.534 1.49-4.743.9-.9 2.6-.9 3.5 0s.9 2.6 0 3.5m-.592-8.588L8.17 9.017q.346-.519.717-1.017H5.066a.25.25 0 0 0-.214.121l-2.167 3.612ZM16 15.112q-.5.372-1.018.718l-3.393 2.262.678 3.223 3.612-2.167a.25.25 0 0 0 .121-.214ZM17.5 8a1.5 1.5 0 1 1-3.001-.001A1.5 1.5 0 0 1 17.5 8"/></svg></span> Full Installation Guide</a> <a href=guides/kaggle-setup/ class=md-button><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 7V5h2V4a2 2 0 0 1 2-2h6v7l2.5-1.5L18 9V2h1c1.05 0 2 .95 2 2v16c0 1.05-.95 2-2 2H7c-1.05 0-2-.95-2-2v-1H3v-2h2v-4H3v-2h2V7zm4 4H5v2h2zm0-4V5H5v2zm0 12v-2H5v2z"/></svg></span> Kaggle Setup Tutorial</a></p> <hr> <h2 id=key-features-of-v220><img alt=â­ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/2b50.svg title=:star:> Key Features of v2.2.0<a class=headerlink href=#key-features-of-v220 title="Permanent link">&para;</a></h2> <h3 id=1-multi-gpu-inference-on-kaggle>1. Multi-GPU Inference on Kaggle<a class=headerlink href=#1-multi-gpu-inference-on-kaggle title="Permanent link">&para;</a></h3> <p>Run models up to <strong>70B parameters</strong> using both T4 GPUs with native CUDA tensor-split:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.api.multigpu</span><span class=w> </span><span class=kn>import</span> <span class=n>kaggle_t4_dual_config</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># Optimized dual T4 configuration</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>config</span> <span class=o>=</span> <span class=n>kaggle_t4_dual_config</span><span class=p>(</span><span class=n>model_size_gb</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>  <span class=c1># For 70B IQ3_XS</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=nb>print</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>to_cli_args</span><span class=p>())</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a><span class=c1># Output: [&#39;-ngl&#39;, &#39;-1&#39;, &#39;--split-mode&#39;, &#39;layer&#39;, &#39;--tensor-split&#39;, &#39;0.5,0.5&#39;, &#39;-fa&#39;]</span>
</span></code></pre></div> <p><strong>Supported Model Sizes on Dual T4 (30GB VRAM):</strong></p> <table> <thead> <tr> <th>Model Size</th> <th>Quantization</th> <th>VRAM Required</th> <th>Fits Dual T4?</th> </tr> </thead> <tbody> <tr> <td>1-3B</td> <td>Q4_K_M</td> <td>2-3 GB</td> <td>âœ… Single T4</td> </tr> <tr> <td>7-8B</td> <td>Q4_K_M</td> <td>5-6 GB</td> <td>âœ… Single T4</td> </tr> <tr> <td>13B</td> <td>Q4_K_M</td> <td>8-9 GB</td> <td>âœ… Single T4</td> </tr> <tr> <td>32-34B</td> <td>Q4_K_M</td> <td>20-22 GB</td> <td>âœ… Dual T4</td> </tr> <tr> <td><strong>70B</strong></td> <td><strong>IQ3_XS</strong></td> <td><strong>25-27 GB</strong></td> <td>âœ… <strong>Dual T4</strong></td> </tr> </tbody> </table> <p><a href=kaggle/multi-gpu-inference/ class=md-button>:material-gpu: Multi-GPU Guide</a></p> <hr> <h3 id=2-unsloth-fine-tuning-pipeline>2. Unsloth Fine-Tuning Pipeline<a class=headerlink href=#2-unsloth-fine-tuning-pipeline title="Permanent link">&para;</a></h3> <p>Complete workflow from fine-tuning to deployment:</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:3><input checked=checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><input id=__tabbed_1_3 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>Step 1: Fine-Tune with Unsloth</label><label for=__tabbed_1_2>Step 2: Export to GGUF</label><label for=__tabbed_1_3>Step 3: Deploy with llcuda</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>from</span><span class=w> </span><span class=nn>unsloth</span><span class=w> </span><span class=kn>import</span> <span class=n>FastLanguageModel</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=c1># Load model for training</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>FastLanguageModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>    <span class=n>model_name</span><span class=o>=</span><span class=s2>&quot;unsloth/Qwen2.5-1.5B-Instruct&quot;</span><span class=p>,</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>    <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>    <span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=p>)</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a><span class=c1># Add LoRA adapters</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=n>model</span> <span class=o>=</span> <span class=n>FastLanguageModel</span><span class=o>.</span><span class=n>get_peft_model</span><span class=p>(</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a>    <span class=n>model</span><span class=p>,</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a>    <span class=n>r</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;q_proj&quot;</span><span class=p>,</span> <span class=s2>&quot;k_proj&quot;</span><span class=p>,</span> <span class=s2>&quot;v_proj&quot;</span><span class=p>],</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span><span id=__span-5-17><a id=__codelineno-5-17 name=__codelineno-5-17 href=#__codelineno-5-17></a><span class=p>)</span>
</span><span id=__span-5-18><a id=__codelineno-5-18 name=__codelineno-5-18 href=#__codelineno-5-18></a>
</span><span id=__span-5-19><a id=__codelineno-5-19 name=__codelineno-5-19 href=#__codelineno-5-19></a><span class=c1># Train your model...</span>
</span><span id=__span-5-20><a id=__codelineno-5-20 name=__codelineno-5-20 href=#__codelineno-5-20></a><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># Export fine-tuned model to GGUF format</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained_gguf</span><span class=p>(</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    <span class=s2>&quot;my_finetuned_model&quot;</span><span class=p>,</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    <span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    <span class=n>quantization_method</span><span class=o>=</span><span class=s2>&quot;q4_k_m&quot;</span>  <span class=c1># Recommended for T4</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=p>)</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=c1># Output: my_finetuned_model-Q4_K_M.gguf</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.server</span><span class=w> </span><span class=kn>import</span> <span class=n>ServerManager</span><span class=p>,</span> <span class=n>ServerConfig</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=c1># Deploy on Kaggle dual T4</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=n>config</span> <span class=o>=</span> <span class=n>ServerConfig</span><span class=p>(</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>    <span class=n>model_path</span><span class=o>=</span><span class=s2>&quot;my_finetuned_model-Q4_K_M.gguf&quot;</span><span class=p>,</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>    <span class=n>n_gpu_layers</span><span class=o>=</span><span class=mi>99</span><span class=p>,</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>    <span class=n>tensor_split</span><span class=o>=</span><span class=s2>&quot;0.5,0.5&quot;</span><span class=p>,</span>  <span class=c1># Use both GPUs</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a>    <span class=n>flash_attn</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a><span class=p>)</span>
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a>
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a><span class=n>server</span> <span class=o>=</span> <span class=n>ServerManager</span><span class=p>()</span>
</span><span id=__span-7-12><a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a><span class=n>server</span><span class=o>.</span><span class=n>start_with_config</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span><span id=__span-7-13><a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a>
</span><span id=__span-7-14><a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a><span class=c1># Now serving at http://localhost:8080 with OpenAI API</span>
</span></code></pre></div> </div> </div> </div> <p><a href=unsloth/overview/ class="md-button md-button--primary"><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m13.13 22.19-1.63-3.83c1.57-.58 3.04-1.36 4.4-2.27zM5.64 12.5l-3.83-1.63 6.1-2.77C7 9.46 6.22 10.93 5.64 12.5M21.61 2.39S16.66.269 11 5.93c-2.19 2.19-3.5 4.6-4.35 6.71-.28.75-.09 1.57.46 2.13l2.13 2.12c.55.56 1.37.74 2.12.46A19.1 19.1 0 0 0 18.07 13c5.66-5.66 3.54-10.61 3.54-10.61m-7.07 7.07c-.78-.78-.78-2.05 0-2.83s2.05-.78 2.83 0c.77.78.78 2.05 0 2.83s-2.05.78-2.83 0m-5.66 7.07-1.41-1.41zM6.24 22l3.64-3.64c-.34-.09-.67-.24-.97-.45L4.83 22zM2 22h1.41l4.77-4.76-1.42-1.41L2 20.59zm0-2.83 4.09-4.08c-.21-.3-.36-.62-.45-.97L2 17.76z"/></svg></span> Unsloth Integration Guide</a></p> <hr> <h3 id=3-split-gpu-architecture-with-graphistry>3. Split-GPU Architecture with Graphistry<a class=headerlink href=#3-split-gpu-architecture-with-graphistry title="Permanent link">&para;</a></h3> <p>Unique capability: <strong>Run LLM inference on GPU 0 while using GPU 1 for RAPIDS/Graphistry visualization</strong></p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.graphistry</span><span class=w> </span><span class=kn>import</span> <span class=n>SplitGPUConfig</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=kn>import</span><span class=w> </span><span class=nn>graphistry</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=c1># Configure split-GPU setup</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=n>config</span> <span class=o>=</span> <span class=n>SplitGPUConfig</span><span class=p>(</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>    <span class=n>llm_gpu</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>      <span class=c1># GPU 0 for llama-server</span>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a>    <span class=n>graph_gpu</span><span class=o>=</span><span class=mi>1</span>     <span class=c1># GPU 1 for Graphistry</span>
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a><span class=p>)</span>
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a>
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a><span class=c1># Set Graphistry to use GPU 1</span>
</span><span id=__span-8-11><a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a><span class=n>graphistry</span><span class=o>.</span><span class=n>register</span><span class=p>(</span>
</span><span id=__span-8-12><a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a>    <span class=n>api</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-8-13><a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a>    <span class=n>protocol</span><span class=o>=</span><span class=s2>&quot;https&quot;</span><span class=p>,</span>
</span><span id=__span-8-14><a id=__codelineno-8-14 name=__codelineno-8-14 href=#__codelineno-8-14></a>    <span class=n>server</span><span class=o>=</span><span class=s2>&quot;hub.graphistry.com&quot;</span>
</span><span id=__span-8-15><a id=__codelineno-8-15 name=__codelineno-8-15 href=#__codelineno-8-15></a><span class=p>)</span>
</span><span id=__span-8-16><a id=__codelineno-8-16 name=__codelineno-8-16 href=#__codelineno-8-16></a>
</span><span id=__span-8-17><a id=__codelineno-8-17 name=__codelineno-8-17 href=#__codelineno-8-17></a><span class=c1># Now run LLM on GPU 0 and visualize graphs on GPU 1</span>
</span></code></pre></div> <p><strong>Use Cases:</strong> - Extract knowledge graphs from LLM outputs â†’ Visualize with Graphistry - Analyze entity relationships in generated text - Interactive exploration of LLM-generated networks - Real-time graph updates from streaming LLM responses</p> <p><a href=graphistry/overview/ class=md-button><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19.5 17c-.13 0-.26 0-.39.04l-1.61-3.25a2.5 2.5 0 0 0-1.75-4.29c-.13 0-.25 0-.39.04l-1.63-3.25c.48-.45.77-1.08.77-1.79a2.5 2.5 0 0 0-5 0c0 .71.29 1.34.76 1.79L8.64 9.54c-.14-.04-.26-.04-.39-.04a2.5 2.5 0 0 0-1.75 4.29l-1.61 3.25C4.76 17 4.63 17 4.5 17a2.5 2.5 0 0 0 0 5A2.5 2.5 0 0 0 7 19.5c0-.7-.29-1.34-.76-1.79l1.62-3.25c.14.04.26.04.39.04s.25 0 .39-.04l1.63 3.25c-.47.45-.77 1.09-.77 1.79a2.5 2.5 0 0 0 5 0A2.5 2.5 0 0 0 12 17c-.13 0-.26 0-.39.04L10 13.79c.46-.45.75-1.08.75-1.79s-.29-1.34-.75-1.79l1.61-3.25c.13.04.26.04.39.04s.26 0 .39-.04L14 10.21c-.45.45-.75 1.09-.75 1.79a2.5 2.5 0 0 0 2.5 2.5c.13 0 .25 0 .39-.04l1.63 3.25c-.47.45-.77 1.09-.77 1.79a2.5 2.5 0 0 0 5 0 2.5 2.5 0 0 0-2.5-2.5"/></svg></span> Graphistry Integration Guide</a></p> <hr> <h3 id=4-29-gguf-quantization-formats>4. 29 GGUF Quantization Formats<a class=headerlink href=#4-29-gguf-quantization-formats title="Permanent link">&para;</a></h3> <p>llcuda supports all llama.cpp quantization types:</p> <div class="tabbed-set tabbed-alternate" data-tabs=2:4><input checked=checked id=__tabbed_2_1 name=__tabbed_2 type=radio><input id=__tabbed_2_2 name=__tabbed_2 type=radio><input id=__tabbed_2_3 name=__tabbed_2 type=radio><input id=__tabbed_2_4 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=__tabbed_2_1>K-Quants (Recommended)</label><label for=__tabbed_2_2>I-Quants (Best Compression)</label><label for=__tabbed_2_3>Legacy Quants</label><label for=__tabbed_2_4>Full Precision</label></div> <div class=tabbed-content> <div class=tabbed-block> <p>Best quality-to-size ratio with double quantization:</p> <ul> <li><strong>Q4_K_M</strong> - 4.8 bpw, best for most models (recommended)</li> <li><strong>Q5_K_M</strong> - 5.7 bpw, higher quality</li> <li><strong>Q6_K</strong> - 6.6 bpw, near FP16 quality</li> <li><strong>Q8_0</strong> - 8.5 bpw, very high quality</li> </ul> </div> <div class=tabbed-block> <p>Importance-matrix quantization for 70B models:</p> <ul> <li><strong>IQ3_XS</strong> - 3.3 bpw, fits 70B on dual T4</li> <li><strong>IQ4_XS</strong> - 4.3 bpw, better quality</li> <li><strong>IQ2_XS</strong> - 2.3 bpw, extreme compression</li> <li><strong>IQ1_S</strong> - 1.6 bpw, smallest possible</li> </ul> </div> <div class=tabbed-block> <p>Standard quantization types:</p> <ul> <li><strong>Q4_0</strong> - 4.5 bpw, legacy format</li> <li><strong>Q5_0</strong> - 5.5 bpw, legacy format</li> <li><strong>Q8_0</strong> - 8.5 bpw, high precision</li> </ul> </div> <div class=tabbed-block> <p>Unquantized formats:</p> <ul> <li><strong>F32</strong> - 32-bit float</li> <li><strong>F16</strong> - 16-bit float</li> <li><strong>BF16</strong> - Brain float 16</li> </ul> </div> </div> </div> <p><a href=gguf/overview/ class=md-button><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 3C7.58 3 4 4.79 4 7s3.58 4 8 4 8-1.79 8-4-3.58-4-8-4M4 9v3c0 2.21 3.58 4 8 4s8-1.79 8-4V9c0 2.21-3.58 4-8 4s-8-1.79-8-4m0 5v3c0 2.21 3.58 4 8 4s8-1.79 8-4v-3c0 2.21-3.58 4-8 4s-8-1.79-8-4"/></svg></span> GGUF Quantization Guide</a></p> <hr> <h2 id=performance-benchmarks><img alt=ğŸ“ˆ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f4c8.svg title=:chart_with_upwards_trend:> Performance Benchmarks<a class=headerlink href=#performance-benchmarks title="Permanent link">&para;</a></h2> <p>Real Kaggle dual T4 performance metrics:</p> <table> <thead> <tr> <th>Model</th> <th>Quantization</th> <th>GPUs</th> <th>Tokens/sec</th> <th>Latency</th> <th>VRAM Usage</th> </tr> </thead> <tbody> <tr> <td>Gemma 2-2B</td> <td>Q4_K_M</td> <td>2Ã— T4</td> <td>~60 tok/s</td> <td>-</td> <td>4 GB</td> </tr> <tr> <td>Qwen2.5-7B</td> <td>Q4_K_M</td> <td>2Ã— T4</td> <td>~35 tok/s</td> <td>-</td> <td>10 GB</td> </tr> <tr> <td>Llama-3.1-70B</td> <td>IQ3_XS</td> <td>2Ã— T4</td> <td>~8-12 tok/s</td> <td>-</td> <td>27 GB</td> </tr> <tr> <td>Gemma 3-1B</td> <td>Q4_K_M</td> <td>1Ã— T4</td> <td>~45 tok/s</td> <td>690ms</td> <td>3 GB</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Performance Optimization</p> <ul> <li>Enable FlashAttention for 2-3x speedup</li> <li>Use tensor-split for models &gt;15GB</li> <li>K-quants provide best quality/speed balance</li> <li>I-quants enable 70B models on 30GB VRAM</li> </ul> </div> <p><a href=performance/benchmarks/ class=md-button><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 16a3 3 0 0 1-3-3c0-1.12.61-2.1 1.5-2.61l9.71-5.62-5.53 9.58c-.5.98-1.51 1.65-2.68 1.65m0-13c1.81 0 3.5.5 4.97 1.32l-2.1 1.21C14 5.19 13 5 12 5a8 8 0 0 0-8 8c0 2.21.89 4.21 2.34 5.65h.01c.39.39.39 1.02 0 1.41s-1.03.39-1.42.01A9.97 9.97 0 0 1 2 13 10 10 0 0 1 12 3m10 10c0 2.76-1.12 5.26-2.93 7.07-.39.38-1.02.38-1.41-.01a.996.996 0 0 1 0-1.41A7.95 7.95 0 0 0 20 13c0-1-.19-2-.54-2.9L20.67 8C21.5 9.5 22 11.18 22 13"/></svg></span> Full Benchmarks</a></p> <hr> <h2 id=tutorial-notebooks-10-kaggle-notebooks><img alt=ğŸ““ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f4d3.svg title=:notebook:> Tutorial Notebooks (10 Kaggle Notebooks)<a class=headerlink href=#tutorial-notebooks-10-kaggle-notebooks title="Permanent link">&para;</a></h2> <p>Complete tutorial series for Kaggle dual T4 environment:</p> <table> <thead> <tr> <th>#</th> <th>Notebook</th> <th>Open in Kaggle</th> <th>Description</th> <th>Time</th> </tr> </thead> <tbody> <tr> <td>01</td> <td><a href=tutorials/01-quickstart/ >Quick Start</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/01-quickstart-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>5-minute introduction</td> <td>5 min</td> </tr> <tr> <td>02</td> <td><a href=tutorials/02-server-setup/ >Server Setup</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/02-llama-server-setup-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>Server configuration &amp; lifecycle</td> <td>15 min</td> </tr> <tr> <td>03</td> <td><a href=tutorials/03-multi-gpu/ >Multi-GPU</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/03-multi-gpu-inference-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>Dual T4 tensor-split</td> <td>20 min</td> </tr> <tr> <td>04</td> <td><a href=tutorials/04-gguf-quantization/ >GGUF Quantization</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/04-gguf-quantization-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>K-quants, I-quants, parsing</td> <td>20 min</td> </tr> <tr> <td>05</td> <td><a href=tutorials/05-unsloth-integration/ >Unsloth Integration</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/05-unsloth-integration-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>Fine-tune â†’ GGUF â†’ Deploy</td> <td>30 min</td> </tr> <tr> <td>06</td> <td><a href=tutorials/06-split-gpu-graphistry/ >Split-GPU + Graphistry</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/06-split-gpu-graphistry-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>LLM + RAPIDS visualization</td> <td>30 min</td> </tr> <tr> <td>07</td> <td><a href=tutorials/07-openai-api/ >OpenAI API</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/07-openai-api-client-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>Drop-in OpenAI SDK</td> <td>15 min</td> </tr> <tr> <td>08</td> <td><a href=tutorials/08-nccl-pytorch/ >NCCL + PyTorch</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/08-nccl-pytorch-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>Distributed PyTorch</td> <td>25 min</td> </tr> <tr> <td>09</td> <td><a href=tutorials/09-large-models/ >Large Models (70B)</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/09-large-models-kaggle-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>70B on dual T4 with IQ3_XS</td> <td>30 min</td> </tr> <tr> <td>10</td> <td><a href=tutorials/10-complete-workflow/ >Complete Workflow</a></td> <td><a href="https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/10-complete-workflow-llcuda-v2.2.0.ipynb"><img alt=Kaggle src=https://kaggle.com/static/images/open-in-kaggle.svg></a></td> <td>End-to-end production</td> <td>45 min</td> </tr> </tbody> </table> <p><a href=tutorials/ class="md-button md-button--primary"><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 7V5h2V4a2 2 0 0 1 2-2h6v7l2.5-1.5L18 9V2h1c1.05 0 2 .95 2 2v16c0 1.05-.95 2-2 2H7c-1.05 0-2-.95-2-2v-1H3v-2h2v-4H3v-2h2V7zm4 4H5v2h2zm0-4V5H5v2zm0 12v-2H5v2z"/></svg></span> View All Tutorials</a></p> <hr> <h2 id=learning-paths><img alt=ğŸ“š class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f4da.svg title=:books:> Learning Paths<a class=headerlink href=#learning-paths title="Permanent link">&para;</a></h2> <p>Choose your path based on experience level:</p> <div class="tabbed-set tabbed-alternate" data-tabs=3:4><input checked=checked id=__tabbed_3_1 name=__tabbed_3 type=radio><input id=__tabbed_3_2 name=__tabbed_3 type=radio><input id=__tabbed_3_3 name=__tabbed_3 type=radio><input id=__tabbed_3_4 name=__tabbed_3 type=radio><div class=tabbed-labels><label for=__tabbed_3_1>Beginner (1 hour)</label><label for=__tabbed_3_2>Intermediate (3 hours)</label><label for=__tabbed_3_3>Advanced (2 hours)</label><label for=__tabbed_3_4>Unsloth Focus (2 hours)</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>01 Quick Start â†’ 02 Server Setup â†’ 03 Multi-GPU
</span></code></pre></div> <p>Perfect for first-time users. Learn the basics of llcuda on Kaggle.</p> </div> <div class=tabbed-block> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>01 â†’ 02 â†’ 03 â†’ 04 â†’ 05 â†’ 06 â†’ 07 â†’ 10
</span></code></pre></div> <p>Complete fundamentals through advanced workflows.</p> </div> <div class=tabbed-block> <div class="language-text highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>01 â†’ 03 â†’ 08 â†’ 09
</span></code></pre></div> <p>Focus on multi-GPU and large model deployment.</p> </div> <div class=tabbed-block> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>01 â†’ 04 â†’ 05 â†’ 10
</span></code></pre></div> <p>Fine-tuning and deployment pipeline.</p> </div> </div> </div> <hr> <h2 id=whats-new-in-v220><img alt=âœ¨ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/2728.svg title=:sparkles:> What's New in v2.2.0<a class=headerlink href=#whats-new-in-v220 title="Permanent link">&para;</a></h2> <div class="admonition success"> <p class=admonition-title>Major Release Highlights</p> <p><strong>Positioned as Unsloth Inference Backend</strong></p> <ul> <li>llcuda is now the official CUDA12 inference backend for Unsloth</li> <li>Seamless workflow: Unsloth (training) â†’ llcuda (inference)</li> <li>Complete Kaggle dual T4 build notebook included</li> </ul> </div> <p><strong>New Features:</strong></p> <ul> <li><strong>Kaggle Dual T4 Build</strong> - Complete build notebook for reproducible binaries</li> <li><strong>Split-GPU Architecture</strong> - LLM on GPU 0 + Graphistry on GPU 1</li> <li><strong>Multi-GPU Clarification</strong> - Native CUDA tensor-split (NOT NCCL)</li> <li><strong>961MB Binary Package</strong> - Pre-built CUDA 12.5 binaries for T4</li> <li><strong>Graphistry Integration</strong> - PyGraphistry for knowledge graph visualization</li> <li><strong>70B Model Support</strong> - IQ3_XS quantization for large models</li> <li><strong>FlashAttention All Quants</strong> - Enabled for all quantization types</li> </ul> <p><strong>Performance:</strong></p> <table> <thead> <tr> <th>Platform</th> <th>GPU</th> <th>Model</th> <th>Tokens/sec</th> </tr> </thead> <tbody> <tr> <td>Kaggle</td> <td>2Ã— T4</td> <td>Gemma 2-2B Q4_K_M</td> <td>~60 tok/s</td> </tr> <tr> <td>Kaggle</td> <td>2Ã— T4</td> <td>Llama 70B IQ3_XS</td> <td>~12 tok/s</td> </tr> </tbody> </table> <p><a class=md-button href=https://github.com/llcuda/llcuda/blob/main/CHANGELOG.md><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 3a2 2 0 0 1 2-2h9.982a2 2 0 0 1 1.414.586l4.018 4.018A2 2 0 0 1 21 7.018V21a2 2 0 0 1-2 2H4.75a.75.75 0 0 1 0-1.5H19a.5.5 0 0 0 .5-.5V8.5h-4a2 2 0 0 1-2-2v-4H5a.5.5 0 0 0-.5.5v6.25a.75.75 0 0 1-1.5 0Zm12-.5v4a.5.5 0 0 0 .5.5h4a.5.5 0 0 0-.146-.336l-4.018-4.018A.5.5 0 0 0 15 2.5"/><path d="M4.53 12.24a.75.75 0 0 1-.039 1.06l-2.639 2.45 2.64 2.45a.75.75 0 1 1-1.022 1.1l-3.23-3a.75.75 0 0 1 0-1.1l3.23-3a.75.75 0 0 1 1.06.04m3.979 1.06a.75.75 0 1 1 1.02-1.1l3.231 3a.75.75 0 0 1 0 1.1l-3.23 3a.75.75 0 1 1-1.021-1.1l2.639-2.45-2.64-2.45Z"/></svg></span> Full Changelog</a></p> <hr> <h2 id=technical-architecture><img alt=âš™ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/2699.svg title=:gear:> Technical Architecture<a class=headerlink href=#technical-architecture title="Permanent link">&para;</a></h2> <p>llcuda v2.2.0 is built on proven technologies:</p> <div class="grid cards"> <ul> <li> <p><strong>llama.cpp Server</strong></p> <hr> <ul> <li>Build 7760 (commit 388ce82)</li> <li>OpenAI-compatible API</li> <li>Native CUDA tensor-split</li> <li>FlashAttention support</li> </ul> </li> <li> <p><strong>CUDA 12.5</strong></p> <hr> <ul> <li>SM 7.5 (Turing) targeting</li> <li>cuBLAS acceleration</li> <li>Static linking</li> <li>961MB binary package</li> </ul> </li> <li> <p><strong>Python 3.11+</strong></p> <hr> <ul> <li>Type-safe APIs</li> <li>Async/await support</li> <li>Modern packaging</li> <li>62KB pip package</li> </ul> </li> <li> <p><strong>RAPIDS + Graphistry</strong></p> <hr> <ul> <li>cuDF for GPU DataFrames</li> <li>cuGraph for network analysis</li> <li>PyGraphistry visualization</li> <li>Millions of nodes/edges</li> </ul> </li> </ul> </div> <hr> <h2 id=api-reference><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M7 7H5a2 2 0 0 0-2 2v8h2v-4h2v4h2V9a2 2 0 0 0-2-2m0 4H5V9h2m7-2h-4v10h2v-4h2a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2m0 4h-2V9h2m6 0v6h1v2h-4v-2h1V9h-1V7h4v2Z"/></svg></span> API Reference<a class=headerlink href=#api-reference title="Permanent link">&para;</a></h2> <p>llcuda provides comprehensive Python APIs:</p> <table> <thead> <tr> <th>Module</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><a href=api/client/ ><code>llcuda.api.client</code></a></td> <td>OpenAI-compatible llama.cpp client</td> </tr> <tr> <td><a href=api/multigpu/ ><code>llcuda.api.multigpu</code></a></td> <td>Multi-GPU configuration for Kaggle</td> </tr> <tr> <td><a href=api/gguf/ ><code>llcuda.api.gguf</code></a></td> <td>GGUF parsing and quantization tools</td> </tr> <tr> <td><a href=api/nccl/ ><code>llcuda.api.nccl</code></a></td> <td>NCCL for distributed PyTorch</td> </tr> <tr> <td><a href=api/server/ ><code>llcuda.server</code></a></td> <td>Server lifecycle management</td> </tr> <tr> <td><a href=api/graphistry/ ><code>llcuda.graphistry</code></a></td> <td>Graphistry integration helpers</td> </tr> </tbody> </table> <p><a href=api/overview/ class=md-button><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8 3a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2H3v2h1a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h2v-2H8v-5a2 2 0 0 0-2-2 2 2 0 0 0 2-2V5h2V3m6 0a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h1v2h-1a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2h-2v-2h2v-5a2 2 0 0 1 2-2 2 2 0 0 1-2-2V5h-2V3z"/></svg></span> Full API Documentation</a></p> <hr> <h2 id=community-support><img alt=ğŸ¤ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f91d.svg title=:handshake:> Community &amp; Support<a class=headerlink href=#community-support title="Permanent link">&para;</a></h2> <ul> <li><strong>GitHub Repository</strong>: <a href=https://github.com/llcuda/llcuda>github.com/llcuda/llcuda</a></li> <li><strong>GitHub Releases</strong>: <a href=https://github.com/llcuda/llcuda/releases/tag/v2.2.0>v2.2.0 Download</a></li> <li><strong>Bug Reports</strong>: <a href=https://github.com/llcuda/llcuda/issues>GitHub Issues</a></li> <li><strong>Email</strong>: <a href=mailto:waqasm86@gmail.com>waqasm86@gmail.com</a></li> </ul> <hr> <h2 id=license><img alt=âš– class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/2696.svg title=:balance_scale:> License<a class=headerlink href=#license title="Permanent link">&para;</a></h2> <p>MIT License - Free for commercial and personal use. See <a href=https://github.com/llcuda/llcuda/blob/main/LICENSE>LICENSE</a>.</p> <hr> <div style="text-align: center; color: #666; font-size: 0.9em; margin-top: 3em;"> Built with â¤ï¸ by <a href=https://github.com/waqasm86>Waqas Muhammad</a> | Powered by <a href=https://github.com/ggml-org/llama.cpp>llama.cpp</a> | Optimized for <a href=https://unsloth.ai>Unsloth</a> & <a href=https://www.graphistry.com>Graphistry</a> </div> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve by <a href=https://github.com/llcuda/llcuda/issues/new target=_blank rel=noopener>opening an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=guides/quickstart/ class="md-footer__link md-footer__link--next" aria-label="Next: Quick Start"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Quick Start </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024-2026 Waqas Muhammad </div> </div> <div class=md-social> <a href=https://github.com/llcuda/llcuda target=_blank rel=noopener title="GitHub - llcuda/llcuda" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/waqasm86 target=_blank rel=noopener title="GitHub - waqasm86" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=mailto:waqasm86@gmail.com target=_blank rel=noopener title="Email - waqasm86@gmail.com" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l208 156a48 48 0 0 0 57.6 0l208-156c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 196v188c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V196L313.6 344.8c-34.1 25.6-81.1 25.6-115.2 0z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"annotate": null, "base": ".", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=assets/javascripts/bundle.79ae519e.min.js></script> <script src=javascripts/mathjax.js></script> <script src=javascripts/schema.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>