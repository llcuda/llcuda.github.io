# Default meta tags for all pages - llcuda v2.2.0
title: llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth | 1B-5B Models on Kaggle Dual T4
description: CUDA 12 inference backend for Unsloth optimized for small GGUF models (1B-5B) on Kaggle dual Tesla T4 GPUs. Split-GPU architecture with LLM inference on GPU 0 and Graphistry neural network visualization on GPU 1. Features 11 comprehensive tutorials including GGUF architecture visualization with 929 nodes and 981 edges.
keywords: llcuda, llcuda v2.2.0, CUDA 12, Tesla T4, Kaggle, dual GPU, LLM inference, Unsloth, GGUF, quantization, llama.cpp, multi-GPU, tensor-split, Graphistry, neural network visualization, GGUF visualization, 1B-5B models, small models, FlashAttention, split-GPU architecture, 929 nodes, 981 edges, attention heads, transformer architecture, PageRank, cuGraph, RAPIDS, interactive dashboards, Llama-3.2-3B
author: Waqas Muhammad
robots: index, follow
og:type: website
og:site_name: llcuda v2.2.0 Documentation
og:locale: en_US
og:image: https://llcuda.github.io/assets/images/social-card.png
twitter:card: summary_large_image
twitter:site: "@waqasm86"
twitter:creator: "@waqasm86"
twitter:image: https://llcuda.github.io/assets/images/social-card.png
