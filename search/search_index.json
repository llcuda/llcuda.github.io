{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llcuda v2.0.6: Tesla T4 CUDA Inference","text":"<p>Fast LLM inference on Tesla T4 GPUs with FlashAttention and Tensor Core optimization. Built exclusively for Google Colab and Tesla T4 hardware with GitHub-only distribution.</p>"},{"location":"#why-llcuda-v206","title":"Why llcuda v2.0.6?","text":"Tesla T4 OptimizedGitHub-Only DistributionGoogle Colab ReadyUnsloth Integration <p>Built specifically for Tesla T4 (SM 7.5) with:</p> <ul> <li>\u2705 FlashAttention support (2-3x faster)</li> <li>\u2705 Tensor Core optimization</li> <li>\u2705 CUDA Graphs for reduced overhead</li> <li>\u2705 134 tokens/sec verified on Gemma 3-1B</li> </ul> <p>No PyPI dependency:</p> <pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <ul> <li>Binaries auto-download from GitHub Releases (266 MB)</li> <li>One-time setup, cached for future use</li> <li>Direct from source, always up-to-date</li> </ul> <p>Perfect for cloud notebooks:</p> <ul> <li>\u2705 Tesla T4 Free tier supported</li> <li>\u2705 One-line install</li> <li>\u2705 Instant inference</li> <li>\u2705 Verified 134 tok/s performance</li> </ul> <p>Seamless workflow:</p> <ul> <li>Fine-tune with Unsloth (2x faster training)</li> <li>Export to GGUF format</li> <li>Deploy with llcuda (fast inference)</li> <li>Production-ready pipeline</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Try llcuda on Google Colab right now!</p>"},{"location":"#60-second-setup","title":"60-Second Setup","text":"<pre><code># Install from GitHub\npip install git+https://github.com/waqasm86/llcuda.git\n\n# Run inference\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n# Expected output: ~134 tokens/sec on Tesla T4\n</code></pre> <p>First Run Downloads</p> <p>CUDA binaries (266 MB) download automatically from GitHub Releases v2.0.6 on first import. Subsequent runs use cached binaries - instant startup!</p>"},{"location":"#verified-performance","title":"Verified Performance","text":"<p>Real Google Colab Tesla T4 results with proven 3x faster performance:</p> Model Quantization Speed Latency VRAM Status Gemma 3-1B Q4_K_M 134 tok/s 690ms 1.2 GB \u2705 Verified Llama 3.2-3B Q4_K_M ~30 tok/s - 2.0 GB Estimated Qwen 2.5-7B Q4_K_M ~18 tok/s - 5.0 GB Estimated Llama 3.1-8B Q4_K_M ~15 tok/s - 5.5 GB Estimated <p>Performance Highlights</p> <ul> <li>3x faster than expected (134 vs 45 tok/s initial estimate)</li> <li>Consistent 130-142 tok/s range across batch inference</li> <li>Full GPU offload (99 layers on T4)</li> <li>FlashAttention + Tensor Cores delivering exceptional results</li> </ul> <p> See Executed Notebook</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Auto-Download</p> <p>Fetch CUDA binaries and GGUF models automatically</p> <ul> <li>GitHub Releases integration</li> <li>HuggingFace model support</li> <li>Smart caching system</li> </ul> </li> <li> <p> Optimized for T4</p> <p>Built specifically for Tesla T4 GPUs</p> <ul> <li>SM 7.5 targeting</li> <li>FlashAttention enabled</li> <li>Tensor Core support</li> </ul> </li> <li> <p> Easy API</p> <p>PyTorch-style inference interface</p> <ul> <li>Single-line model loading</li> <li>Batch processing</li> <li>Streaming support</li> </ul> </li> <li> <p> Production Ready</p> <p>Reliable and well-tested</p> <ul> <li>Comprehensive error handling</li> <li>Silent mode for servers</li> <li>MIT licensed</li> </ul> </li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"Interactive ChatBatch ProcessingGoogle ColabUnsloth Workflow <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == \"exit\":\n        break\n\n    result = engine.infer(user_input, max_tokens=400)\n    print(f\"Assistant: {result.text}\")\n</code></pre> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nprompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks briefly.\",\n    \"Define deep learning concisely.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <pre><code>import llcuda\n\n# Verify GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n\n# Load model\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain artificial intelligence\",\n    max_tokens=300\n)\nprint(result.text)\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <pre><code># Step 1: Fine-tune with Unsloth\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-3-1b-it\",\n    max_seq_length=2048,\n    load_in_4bit=True\n)\n\n# Train your model...\n\n# Step 2: Export to GGUF\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# Step 3: Deploy with llcuda\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"my_model/unsloth.Q4_K_M.gguf\")\n\nresult = engine.infer(\"Your prompt\", max_tokens=200)\nprint(result.text)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start Guide</p> <p>Get started in 5 minutes with step-by-step instructions</p> </li> <li> <p> Installation</p> <p>Detailed installation for Google Colab and local systems</p> </li> <li> <p> Google Colab Tutorial</p> <p>Complete walkthrough with Tesla T4 GPU examples</p> </li> <li> <p> API Reference</p> <p>Full API documentation and advanced usage</p> </li> <li> <p> Performance Benchmarks</p> <p>Detailed benchmarks and optimization tips</p> </li> <li> <p> Jupyter Notebooks</p> <p>Ready-to-run Colab notebooks with examples</p> </li> </ul>"},{"location":"#whats-new-in-v206","title":"What's New in v2.0.6","text":"<ul> <li>GitHub-Only Distribution - Removed PyPI dependency completely</li> <li>Verified Performance - Real Tesla T4 results: 134 tok/s on Gemma 3-1B</li> <li>Updated Bootstrap - Auto-download from GitHub Releases v2.0.6</li> <li>Comprehensive Tutorials - New Colab notebooks with live execution outputs</li> <li>Enhanced Documentation - Complete guides and API reference</li> <li>Same Proven Binaries - Uses stable v2.0.3 CUDA binaries (identical SHA256)</li> </ul> <p> Read Changelog</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Repository: github.com/waqasm86/llcuda</li> <li>GitHub Releases: Releases &amp; Downloads</li> <li>Bug Reports: GitHub Issues</li> <li>Email: waqasm86@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - Free for commercial and personal use.</p>    Built with \u2764\ufe0f by Waqas Muhammad | Powered by llama.cpp | Optimized for Unsloth"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Complete API documentation for llcuda v2.0.6.</p>"},{"location":"api/overview/#main-components","title":"Main Components","text":"<p>llcuda provides a simple, PyTorch-style API for GPU-accelerated LLM inference.</p>"},{"location":"api/overview/#core-classes","title":"Core Classes","text":"Class Purpose Documentation <code>InferenceEngine</code> Main interface for model loading and inference Details <code>InferenceResult</code> Container for inference results with metrics Details"},{"location":"api/overview/#utility-functions","title":"Utility Functions","text":"Function Purpose Documentation <code>check_gpu_compatibility()</code> Verify GPU support Details <code>get_device_properties()</code> Get GPU device information Details"},{"location":"api/overview/#quick-api-reference","title":"Quick API Reference","text":""},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\"What is AI?\", max_tokens=100)\n\n# Access results\nprint(result.text)                    # Generated text\nprint(result.tokens_per_sec)          # Speed in tokens/sec\nprint(result.latency_ms)              # Latency in milliseconds\nprint(result.tokens_generated)        # Number of tokens generated\n</code></pre>"},{"location":"api/overview/#inferenceengine-methods","title":"InferenceEngine Methods","text":""},{"location":"api/overview/#__init__server_urlnone","title":"<code>__init__(server_url=None)</code>","text":"<p>Create a new inference engine instance.</p> <p>Parameters: - <code>server_url</code> (str, optional): Custom llama-server URL. Default: <code>http://127.0.0.1:8090</code></p>"},{"location":"api/overview/#load_modelmodel_path-silentfalse-auto_starttrue-kwargs","title":"<code>load_model(model_path, silent=False, auto_start=True, **kwargs)</code>","text":"<p>Load a GGUF model for inference.</p> <p>Parameters: - <code>model_path</code> (str): Model identifier or path   - HuggingFace: <code>\"unsloth/repo-name:filename.gguf\"</code>   - Registry: <code>\"gemma-3-1b-Q4_K_M\"</code>   - Local: <code>\"/path/to/model.gguf\"</code> - <code>silent</code> (bool): Suppress llama-server output. Default: <code>False</code> - <code>auto_start</code> (bool): Start server automatically. Default: <code>True</code> - <code>**kwargs</code>: Additional options (context_size, gpu_layers, etc.)</p>"},{"location":"api/overview/#inferprompt-max_tokens512-temperature07-kwargs","title":"<code>infer(prompt, max_tokens=512, temperature=0.7, **kwargs)</code>","text":"<p>Run inference on a single prompt.</p> <p>Parameters: - <code>prompt</code> (str): Input text - <code>max_tokens</code> (int): Maximum tokens to generate. Default: 512 - <code>temperature</code> (float): Sampling temperature. Default: 0.7 - <code>top_p</code> (float): Nucleus sampling threshold. Default: 0.9 - <code>top_k</code> (int): Top-k sampling. Default: 40 - <code>stop_sequences</code> (list): Stop generation at these sequences</p> <p>Returns: - <code>InferenceResult</code>: Result object with text and metrics</p>"},{"location":"api/overview/#batch_inferprompts-max_tokens512-kwargs","title":"<code>batch_infer(prompts, max_tokens=512, **kwargs)</code>","text":"<p>Run inference on multiple prompts.</p> <p>Parameters: - <code>prompts</code> (list[str]): List of input texts - <code>max_tokens</code> (int): Maximum tokens per prompt - <code>**kwargs</code>: Same as <code>infer()</code></p> <p>Returns: - <code>list[InferenceResult]</code>: List of results</p>"},{"location":"api/overview/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get aggregated performance metrics.</p> <p>Returns: - <code>dict</code>: Metrics dictionary with throughput and latency stats</p>"},{"location":"api/overview/#inferenceresult-attributes","title":"InferenceResult Attributes","text":"Attribute Type Description <code>text</code> str Generated text <code>tokens_per_sec</code> float Generation speed <code>latency_ms</code> float Total latency in ms <code>tokens_generated</code> int Number of tokens"},{"location":"api/overview/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api/overview/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if current GPU is compatible with llcuda.</p> <p>Returns: <pre><code>{\n    'gpu_name': str,          # e.g., \"Tesla T4\"\n    'compute_capability': str, # e.g., \"7.5\"\n    'compatible': bool,       # True if supported\n    'platform': str          # e.g., \"colab\", \"local\"\n}\n</code></pre></p> <p>Example: <pre><code>compat = llcuda.check_gpu_compatibility()\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} may not work\")\n</code></pre></p>"},{"location":"api/overview/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>InferenceEngine - Complete InferenceEngine documentation</li> <li>Models &amp; GGUF - Model loading and GGUF format</li> <li>GPU &amp; Device - GPU management and compatibility</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"api/overview/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Tutorials</li> <li>Performance Benchmarks</li> </ul>"},{"location":"guides/installation/","title":"Installation Guide","text":"<p>Install llcuda v2.0.6 directly from GitHub - No PyPI needed!</p>"},{"location":"guides/installation/#quick-install","title":"Quick Install","text":""},{"location":"guides/installation/#method-1-direct-from-github-recommended","title":"Method 1: Direct from GitHub (Recommended)","text":"<pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>This single command will:</p> <ul> <li>\u2705 Clone the latest code from GitHub</li> <li>\u2705 Install the Python package</li> <li>\u2705 Auto-download CUDA binaries (266 MB) from GitHub Releases on first import</li> </ul> <p>Recommended for most users</p> <p>This is the easiest method and works perfectly on Google Colab, Kaggle, and local systems.</p>"},{"location":"guides/installation/#method-2-install-from-specific-release","title":"Method 2: Install from Specific Release","text":"<pre><code>pip install https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-2.0.6-py3-none-any.whl\n</code></pre>"},{"location":"guides/installation/#method-3-install-from-source-development","title":"Method 3: Install from Source (Development)","text":"<pre><code>git clone https://github.com/waqasm86/llcuda.git\ncd llcuda\npip install -e .\n</code></pre>"},{"location":"guides/installation/#what-gets-installed","title":"What Gets Installed","text":""},{"location":"guides/installation/#python-package","title":"Python Package","text":"<ul> <li>Source: GitHub repository (main branch or release tag)</li> <li>Size: ~100 KB (Python code only, no binaries)</li> <li>Contents: Core Python package, API, bootstrap code</li> </ul>"},{"location":"guides/installation/#cuda-binaries-auto-downloaded","title":"CUDA Binaries (Auto-Downloaded)","text":"<ul> <li>Source: GitHub Releases v2.0.6</li> <li>URL: <code>llcuda-binaries-cuda12-t4-v2.0.6.tar.gz</code></li> <li>Size: 266 MB (one-time download, cached locally)</li> <li>Triggered: On first <code>import llcuda</code></li> <li>Location: <code>~/.cache/llcuda/</code> or <code>&lt;package&gt;/binaries/</code></li> </ul> <p>Binary Package Contents: <pre><code>llcuda-binaries-cuda12-t4-v2.0.6.tar.gz (266 MB)\n\u251c\u2500\u2500 bin/\n\u2502   \u251c\u2500\u2500 llama-server        (6.5 MB) - Inference server\n\u2502   \u251c\u2500\u2500 llama-cli           (4.2 MB) - Command-line interface\n\u2502   \u251c\u2500\u2500 llama-embedding     (3.3 MB) - Embedding generator\n\u2502   \u251c\u2500\u2500 llama-bench         (581 KB) - Benchmarking tool\n\u2502   \u2514\u2500\u2500 llama-quantize      (434 KB) - Model quantization\n\u2514\u2500\u2500 lib/\n    \u251c\u2500\u2500 libggml-cuda.so     (221 MB) - CUDA kernels + FlashAttention\n    \u251c\u2500\u2500 libllama.so         (2.9 MB) - Llama core library\n    \u2514\u2500\u2500 Other libraries...\n</code></pre></p>"},{"location":"guides/installation/#platform-specific-instructions","title":"Platform-Specific Instructions","text":"Google ColabLocal LinuxKaggleWindows (WSL2)"},{"location":"guides/installation/#google-colab-tesla-t4","title":"Google Colab (Tesla T4)","text":"<p>Perfect for cloud notebooks!</p> <pre><code># 1. Install\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# 2. Import (triggers binary download on first run)\nimport llcuda\n\n# 3. Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")        # Should show: Tesla T4\nprint(f\"Compatible: {compat['compatible']}\") # Should show: True\n\n# 4. Ready to use!\nengine = llcuda.InferenceEngine()\n</code></pre> <p>First Run</p> <p>The first import downloads 266 MB of binaries (takes 1-2 minutes). Subsequent sessions reuse cached binaries - instant startup!</p>"},{"location":"guides/installation/#local-linux-ubuntudebian","title":"Local Linux (Ubuntu/Debian)","text":"<p>Requirements: - Python 3.11+ - CUDA 12.x runtime - Tesla T4 GPU or compatible</p> <pre><code># 1. Ensure CUDA 12 is installed\nnvidia-smi  # Should show CUDA 12.x\n\n# 2. Install llcuda\npip install git+https://github.com/waqasm86/llcuda.git\n\n# 3. Test installation\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n# Output: 2.0.6\n</code></pre> <p>System Dependencies (usually pre-installed): <pre><code># CUDA Runtime (required)\nsudo apt install nvidia-cuda-toolkit\n\n# Python dependencies (installed automatically by pip)\n# - requests\n# - numpy\n</code></pre></p>"},{"location":"guides/installation/#kaggle-notebooks","title":"Kaggle Notebooks","text":"<pre><code># 1. Enable GPU accelerator\n# Settings \u2192 Accelerator \u2192 GPU T4 x2\n\n# 2. Install\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# 3. Import and verify\nimport llcuda\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\n\n# 4. Start using\nengine = llcuda.InferenceEngine()\n</code></pre>"},{"location":"guides/installation/#windows-with-wsl2","title":"Windows with WSL2","text":"<p>Prerequisites: - Windows 11 with WSL2 - NVIDIA GPU with CUDA support - CUDA 12.x installed in WSL2</p> <pre><code># Inside WSL2 terminal\n# 1. Verify CUDA\nnvidia-smi\n\n# 2. Install Python 3.11+\nsudo apt install python3.11 python3-pip\n\n# 3. Install llcuda\npip3 install git+https://github.com/waqasm86/llcuda.git\n\n# 4. Test\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre>"},{"location":"guides/installation/#verification","title":"Verification","text":"<p>After installation, verify everything works:</p> <pre><code>import llcuda\n\n# 1. Check version\nprint(f\"llcuda version: {llcuda.__version__}\")\n# Expected: 2.0.6\n\n# 2. Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compute Capability: SM {compat['compute_capability']}\")\nprint(f\"Platform: {compat['platform']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n\n# 3. Quick inference test\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"What is 2+2?\", max_tokens=20)\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n# Expected on T4: ~134 tok/s\n</code></pre>"},{"location":"guides/installation/#manual-binary-installation-advanced","title":"Manual Binary Installation (Advanced)","text":"<p>If automatic download fails, install binaries manually:</p> <pre><code># 1. Download binary package\nwget https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# 2. Verify checksum\necho \"5a27d2e1a73ae3d2f1d2ba8cf557b76f54200208c8df269b1bd0d9ee176bb49d  llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\" | sha256sum -c\n\n# 3. Extract to cache directory\nmkdir -p ~/.cache/llcuda\ntar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n\n# 4. Or extract to package directory\npython3 -c \"import llcuda; print(llcuda._BIN_DIR)\"\n# Extract to the printed directory\n</code></pre>"},{"location":"guides/installation/#testing-your-installation","title":"Testing Your Installation","text":""},{"location":"guides/installation/#basic-test","title":"Basic Test","text":"<pre><code>import llcuda\n\n# Should not raise any errors\nprint(\"\u2705 llcuda imported successfully\")\n</code></pre>"},{"location":"guides/installation/#gpu-test","title":"GPU Test","text":"<pre><code>compat = llcuda.check_gpu_compatibility()\nassert compat['compatible'], \"GPU not compatible!\"\nprint(f\"\u2705 GPU compatible: {compat['gpu_name']}\")\n</code></pre>"},{"location":"guides/installation/#inference-test","title":"Inference Test","text":"<pre><code>engine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"Hello!\", max_tokens=10)\nassert result.tokens_generated &gt; 0\nprint(f\"\u2705 Inference working: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/installation/#requirements","title":"Requirements","text":""},{"location":"guides/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>CUDA: 12.x runtime</li> <li>GPU: Tesla T4 (SM 7.5) - Primary target</li> <li>RAM: 4 GB minimum</li> <li>Disk: 1 GB free space (for binaries and models)</li> </ul>"},{"location":"guides/installation/#python-dependencies","title":"Python Dependencies","text":"<p>Automatically installed by pip:</p> <pre><code>requests&gt;=2.31.0\nnumpy&gt;=1.24.0\n</code></pre> <p>Optional for development:</p> <pre><code>huggingface-hub&gt;=0.19.0  # For model downloads\n</code></pre>"},{"location":"guides/installation/#upgrading","title":"Upgrading","text":""},{"location":"guides/installation/#upgrade-to-latest-version","title":"Upgrade to Latest Version","text":"<pre><code>pip install --upgrade git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#force-reinstall","title":"Force Reinstall","text":"<pre><code>pip install --upgrade --force-reinstall --no-cache-dir git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#clear-cache-and-reinstall","title":"Clear Cache and Reinstall","text":"<pre><code># Remove cached binaries\nrm -rf ~/.cache/llcuda/\n\n# Reinstall\npip uninstall llcuda -y\npip install git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#uninstallation","title":"Uninstallation","text":"<pre><code># Remove Python package\npip uninstall llcuda -y\n\n# Remove cached binaries\nrm -rf ~/.cache/llcuda/\n\n# Remove package installation\npip cache purge\n</code></pre>"},{"location":"guides/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/installation/#binary-download-fails","title":"Binary Download Fails","text":"<p>Error: <code>RuntimeError: Binary download failed</code></p> <p>Solution: <pre><code># Check internet connection\nimport requests\nresponse = requests.get(\"https://github.com\")\nprint(response.status_code)  # Should be 200\n\n# Try manual download (see Manual Binary Installation above)\n</code></pre></p>"},{"location":"guides/installation/#import-error","title":"Import Error","text":"<p>Error: <code>ModuleNotFoundError: No module named 'llcuda'</code></p> <p>Solution: <pre><code># Verify installation\npip list | grep llcuda\n\n# Reinstall\npip install --force-reinstall git+https://github.com/waqasm86/llcuda.git\n</code></pre></p>"},{"location":"guides/installation/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Error: <code>RuntimeError: No CUDA GPUs detected</code></p> <p>Solution: <pre><code># Verify CUDA is working\nnvidia-smi\n\n# Check GPU visibility\npython3 -c \"import subprocess; print(subprocess.run(['nvidia-smi'], capture_output=True).stdout)\"\n</code></pre></p>"},{"location":"guides/installation/#next-steps","title":"Next Steps","text":"<ul> <li> Quick Start Guide - Get started in 5 minutes</li> <li> Google Colab Tutorial - Complete walkthrough</li> <li> Troubleshooting - Common issues and solutions</li> <li> API Reference - Detailed API documentation</li> </ul> <p>Need help? Open an issue on GitHub</p>"},{"location":"guides/quickstart/","title":"Quick Start","text":"<p>Get started with llcuda v2.0.6 in 5 minutes!</p>"},{"location":"guides/quickstart/#5-minute-quickstart","title":"5-Minute Quickstart","text":""},{"location":"guides/quickstart/#step-1-install-llcuda","title":"Step 1: Install llcuda","text":"<pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>Google Colab Users</p> <p>Add <code>!</code> before the command: <code>!pip install -q git+https://github.com/waqasm86/llcuda.git</code></p>"},{"location":"guides/quickstart/#step-2-import-and-verify","title":"Step 2: Import and Verify","text":"<pre><code>import llcuda\n\n# Check version\nprint(f\"llcuda version: {llcuda.__version__}\")\n# Output: 2.0.6\n\n# Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n</code></pre> <p>First Import</p> <p>First <code>import llcuda</code> downloads CUDA binaries (266 MB) from GitHub Releases. This takes 1-2 minutes. Subsequent imports are instant!</p>"},{"location":"guides/quickstart/#step-3-load-a-model","title":"Step 3: Load a Model","text":"<pre><code># Create inference engine\nengine = llcuda.InferenceEngine()\n\n# Load Gemma 3-1B from Unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # Suppress llama-server output\n)\n\nprint(\"\u2705 Model loaded!\")\n</code></pre>"},{"location":"guides/quickstart/#step-4-run-inference","title":"Step 4: Run Inference","text":"<pre><code># Ask a question\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\n# Print results\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"Latency: {result.latency_ms:.0f}ms\")\n</code></pre> <p>Expected output on Tesla T4: <pre><code>Speed: 134.2 tokens/sec\nLatency: 690ms\n</code></pre></p>"},{"location":"guides/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete, copy-paste ready example:</p> <pre><code>import llcuda\n\n# Initialize engine\nengine = llcuda.InferenceEngine()\n\n# Load model (downloads ~800 MB on first run)\nprint(\"Loading model...\")\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Single inference\nresult = engine.infer(\n    \"What is machine learning?\",\n    max_tokens=150\n)\n\nprint(f\"Response: {result.text}\")\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/quickstart/#interactive-chat","title":"Interactive Chat","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nprint(\"Chat with Gemma 3-1B (type 'exit' to quit)\")\n\nwhile True:\n    user_input = input(\"\\nYou: \")\n    if user_input.lower() == \"exit\":\n        break\n\n    result = engine.infer(user_input, max_tokens=300)\n    print(f\"AI: {result.text}\")\n    print(f\"({result.tokens_per_sec:.1f} tok/s)\")\n</code></pre>"},{"location":"guides/quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Multiple prompts\nprompts = [\n    \"What is AI?\",\n    \"Explain neural networks.\",\n    \"Define deep learning.\"\n]\n\n# Process in batch\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"\\nQ: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/quickstart/#try-on-google-colab","title":"Try on Google Colab","text":"<p>Click the button below to try llcuda in your browser:</p> <p>What this notebook includes:</p> <ul> <li>\u2705 Complete Tesla T4 setup guide</li> <li>\u2705 GPU verification steps</li> <li>\u2705 Binary download walkthrough</li> <li>\u2705 Multiple inference examples</li> <li>\u2705 Performance benchmarking</li> <li>\u2705 Batch processing demo</li> </ul>"},{"location":"guides/quickstart/#expected-performance","title":"Expected Performance","text":"<p>On Google Colab Tesla T4:</p> Task Speed Latency Simple query 134 tok/s ~690ms Code generation 136 tok/s ~1.5s Batch (4 prompts) 135 tok/s avg ~2.4s total <p>These are verified real-world results! See the executed notebook for proof.</p>"},{"location":"guides/quickstart/#pro-tips","title":"Pro Tips","text":""},{"location":"guides/quickstart/#silent-mode","title":"Silent Mode","text":"<p>Suppress llama-server output for cleaner logs:</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # \u2190 Add this!\n)\n</code></pre>"},{"location":"guides/quickstart/#context-manager","title":"Context Manager","text":"<p>Auto-cleanup resources:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n    result = engine.infer(\"Test prompt\", max_tokens=50)\n    print(result.text)\n# Server automatically stopped here\n</code></pre>"},{"location":"guides/quickstart/#check-gpu-before-loading","title":"Check GPU Before Loading","text":"<pre><code># Verify GPU compatibility first\ncompat = llcuda.check_gpu_compatibility()\n\nif not compat['compatible']:\n    print(f\"\u26a0\ufe0f GPU {compat['gpu_name']} may not be compatible\")\n    print(f\"   llcuda is optimized for Tesla T4\")\nelse:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\n    # Proceed with loading model...\n</code></pre>"},{"location":"guides/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/quickstart/#model-download-slow","title":"Model Download Slow?","text":"<p>HuggingFace downloads can be slow. First download is cached:</p> <pre><code># First run: Downloads ~800 MB (2-3 minutes)\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n\n# Subsequent runs: Uses cached model (instant)\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n</code></pre>"},{"location":"guides/quickstart/#out-of-memory","title":"Out of Memory?","text":"<p>Try a smaller model or reduce context:</p> <pre><code># Smaller model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q2_K.gguf\",  # Q2_K instead of Q4_K_M\n    silent=True\n)\n\n# Or reduce context size\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    context_size=2048,  # Default is 4096\n    silent=True\n)\n</code></pre>"},{"location":"guides/quickstart/#binary-download-failed","title":"Binary Download Failed?","text":"<p>Manual installation:</p> <pre><code>wget https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\nmkdir -p ~/.cache/llcuda\ntar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n</code></pre>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Google Colab Tutorial</p> <p>Complete walkthrough with Tesla T4 examples</p> </li> <li> <p> API Reference</p> <p>Full API documentation and advanced features</p> </li> <li> <p> Performance Guide</p> <p>Benchmarks and optimization tips</p> </li> <li> <p> More Examples</p> <p>Additional use cases and code samples</p> </li> </ul> <p>Questions? Check the FAQ or open an issue!</p>"},{"location":"tutorials/gemma-3-1b-colab/","title":"Gemma 3-1B Tutorial - Google Colab","text":"<p>Complete tutorial for running Gemma 3-1B with llcuda v2.0.6 on Tesla T4 GPU.</p>"},{"location":"tutorials/gemma-3-1b-colab/#open-in-google-colab","title":"Open in Google Colab","text":""},{"location":"tutorials/gemma-3-1b-colab/#what-this-tutorial-covers","title":"What This Tutorial Covers","text":"<p>This comprehensive 14-step tutorial demonstrates:</p> <ol> <li>GPU Verification - Detect Tesla T4 and check compatibility</li> <li>Installation - Install llcuda v2.0.6 from GitHub</li> <li>Binary Download - Auto-download CUDA binaries (~266 MB)</li> <li>GPU Compatibility - Verify llcuda can use the GPU</li> <li>Model Loading - Load Gemma 3-1B-IT from Unsloth HuggingFace</li> <li>First Inference - Run general knowledge queries</li> <li>Code Generation - Test Python code generation</li> <li>Batch Inference - Process multiple prompts efficiently</li> <li>Performance Metrics - Analyze throughput and latency</li> <li>Advanced Parameters - Explore generation strategies</li> <li>Model Loading Methods - HuggingFace, Registry, Local paths</li> <li>Unsloth Workflow - Fine-tuning to deployment pipeline</li> <li>Context Manager - Auto-cleanup resources</li> <li>Available Models - Browse Unsloth GGUF models</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#verified-performance","title":"Verified Performance","text":"<p>Real execution results from Google Colab Tesla T4:</p> <ul> <li>Speed: 134 tokens/sec average (range: 116-142 tok/s)</li> <li>Latency: 690ms median</li> <li>Consistency: Stable performance across all tests</li> <li>GPU Offload: 99 layers fully on GPU</li> </ul> <p>3x Faster Than Expected!</p> <p>Initial estimates: ~45 tok/s Actual performance: 134 tok/s FlashAttention + Tensor Cores delivering exceptional results!</p>"},{"location":"tutorials/gemma-3-1b-colab/#tutorial-steps","title":"Tutorial Steps","text":""},{"location":"tutorials/gemma-3-1b-colab/#step-1-verify-tesla-t4-gpu","title":"Step 1: Verify Tesla T4 GPU","text":"<pre><code>!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n\n# Expected output:\n# Tesla T4, 7.5, 15360 MiB\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-2-install-llcuda-v206","title":"Step 2: Install llcuda v2.0.6","text":"<pre><code>!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# \u2705 llcuda v2.0.6 installed successfully!\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-3-import-and-download-binaries","title":"Step 3: Import and Download Binaries","text":"<pre><code>import llcuda\n\n# First import triggers binary download:\n# - Source: GitHub Releases v2.0.6\n# - Size: 266 MB\n# - Duration: ~1-2 minutes\n# - Cached for future use\n</code></pre> <p>Download Output: <pre><code>\ud83d\udce5 Downloading from GitHub releases...\nURL: https://github.com/waqasm86/llcuda/releases/download/v2.0.6/...\nDownloading T4 binaries: 100% (266.0/266.0 MB)\n\u2705 Extraction complete!\nCopied 5 binaries to .../llcuda/binaries/cuda12\nCopied 18 libraries to .../llcuda/lib\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-colab/#step-4-load-gemma-3-1b-it","title":"Step 4: Load Gemma 3-1B-IT","text":"<pre><code>engine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Auto-configured for Tesla T4:\n# - GPU Layers: 99 (full offload)\n# - Context Size: 4096 tokens\n# - Batch Size: 2048\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-5-run-inference","title":"Step 5: Run Inference","text":"<pre><code>result = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Actual output: 131.4 tokens/sec \u2705\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-6-batch-processing","title":"Step 6: Batch Processing","text":"<pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks briefly.\",\n    \"What is the difference between AI and ML?\",\n    \"Define deep learning concisely.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n# Results:\n# Query 1: 116.0 tok/s\n# Query 2: 142.3 tok/s\n# Query 3: 141.6 tok/s\n# Query 4: 141.7 tok/s\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#performance-results","title":"Performance Results","text":"<p>From the executed notebook:</p> Test Tokens Speed Latency General Knowledge 200 131.4 tok/s 1522ms Code Generation 300 136.1 tok/s - Batch Query 1 80 116.0 tok/s 690ms Batch Query 2 80 142.3 tok/s 562ms Batch Query 3 80 141.6 tok/s 565ms Batch Query 4 80 141.7 tok/s 565ms Average - 134.2 tok/s 690ms median <p>Why So Fast?</p> <ol> <li>FlashAttention - 2-3x speedup for attention operations</li> <li>Tensor Cores - SM 7.5 fully utilized</li> <li>CUDA Graphs - Reduced kernel launch overhead</li> <li>Full GPU Offload - All 99 layers on GPU</li> <li>Q4_K_M Quantization - Optimal speed/quality balance</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#model-information","title":"Model Information","text":"<p>Gemma 3-1B-IT Q4_K_M:</p> <ul> <li>Size: ~806 MB (download)</li> <li>Parameters: 1 billion</li> <li>Quantization: Q4_K_M (4-bit)</li> <li>Context: 2048 tokens (expandable to 4096)</li> <li>VRAM: ~1.2 GB</li> <li>Source: unsloth/gemma-3-1b-it-GGUF</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#jupyter-notebook-features","title":"Jupyter Notebook Features","text":"<p>The notebook includes:</p> <p>\u2705 Complete Setup Guide - Step-by-step installation \u2705 GPU Verification - Ensure you have Tesla T4 \u2705 Error Handling - Helpful troubleshooting tips \u2705 Multiple Examples - Chat, batch, creative generation \u2705 Performance Metrics - Detailed throughput &amp; latency \u2705 Unsloth Workflow - Fine-tuning to deployment \u2705 Model Catalog - List of available Unsloth models</p>"},{"location":"tutorials/gemma-3-1b-colab/#related-resources","title":"Related Resources","text":"<ul> <li> Executed Notebook - See live output with all results</li> <li> Performance Benchmarks - Detailed T4 analysis</li> <li> API Reference - InferenceEngine documentation</li> <li> Unsloth Integration - Complete workflow guide</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#common-questions","title":"Common Questions","text":""},{"location":"tutorials/gemma-3-1b-colab/#how-long-does-the-first-run-take","title":"How long does the first run take?","text":"<ul> <li>Binary download: 1-2 minutes (266 MB)</li> <li>Model download: 2-3 minutes (~800 MB)</li> <li>Model loading: 10-20 seconds</li> <li>First inference: Same speed as subsequent runs</li> </ul> <p>Total first-time setup: ~5 minutes Subsequent sessions: Instant (cached binaries and models)</p>"},{"location":"tutorials/gemma-3-1b-colab/#can-i-use-different-models","title":"Can I use different models?","text":"<p>Yes! The notebook works with any GGUF model from HuggingFace:</p> <pre><code># Llama 3.2-3B\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n)\n\n# Qwen 2.5-7B\nengine.load_model(\n    \"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#what-if-i-dont-have-t4","title":"What if I don't have T4?","text":"<p>llcuda v2.0.6 is optimized for Tesla T4. Other GPUs may work but performance will vary. The binaries are compiled for SM 7.5 (T4's compute capability).</p>"},{"location":"tutorials/gemma-3-1b-colab/#get-started-now","title":"Get Started Now!","text":"Open Tutorial in Colab    <p>No GPU? No problem! Google Colab provides free Tesla T4 access.</p> <p>Questions? Open an issue on GitHub</p>"}]}