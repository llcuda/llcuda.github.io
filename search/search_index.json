{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llcuda v2.2.0: CUDA12 Inference Backend for Unsloth","text":"<p>CUDA12-first inference backend for Unsloth with Graphistry network visualization on Kaggle dual Tesla T4 GPUs. Fine-tune with Unsloth \u2192 Export to GGUF \u2192 Deploy on Kaggle \u2192 Visualize with Graphistry.</p>"},{"location":"#what-is-llcuda-v220","title":"What is llcuda v2.2.0?","text":"<p>llcuda is a CUDA 12 inference backend specifically designed for deploying Unsloth-fine-tuned models on Kaggle's dual Tesla T4 GPUs (30GB total VRAM). It provides:</p> <ul> <li> <p>:material-gpu: Dual T4 Architecture</p> <p>Run on Kaggle's 2\u00d7 Tesla T4 GPUs (15GB each)</p> <ul> <li>Native CUDA tensor-split for multi-GPU</li> <li>Support for 70B models with IQ3_XS quantization</li> <li>FlashAttention for 2-3x faster inference</li> <li>961MB pre-built CUDA 12.5 binaries</li> </ul> </li> <li> <p> Split-GPU Design</p> <p>Unique architecture: LLM on GPU 0 + Graphistry on GPU 1</p> <ul> <li>GPU 0: llama.cpp server for LLM inference</li> <li>GPU 1: RAPIDS cuGraph + Graphistry visualization</li> <li>Extract knowledge graphs from LLM outputs</li> <li>Visualize millions of nodes and edges</li> </ul> </li> <li> <p> Unsloth Integration</p> <p>Seamless workflow from training to deployment</p> <ul> <li>Fine-tune with Unsloth (2x faster training)</li> <li>Export to GGUF format with <code>save_pretrained_gguf()</code></li> <li>Deploy with llcuda on Kaggle</li> <li>Complete end-to-end pipeline</li> </ul> </li> <li> <p> Production Ready</p> <p>Built for Kaggle production workloads</p> <ul> <li>OpenAI-compatible API via llama-server</li> <li>29 quantization formats (K-quants, I-quants)</li> <li>NCCL support for PyTorch distributed</li> <li>Auto-download binaries from GitHub Releases</li> </ul> </li> </ul>"},{"location":"#core-architecture","title":"Core Architecture","text":"<p>llcuda v2.2.0 implements a unique split-GPU architecture for Kaggle's dual T4 environment:</p> <pre><code>graph LR\n    A[Unsloth Fine-Tuning] --&gt; B[GGUF Export]\n    B --&gt; C[llcuda Deployment]\n    C --&gt; D[GPU 0: LLM Inference]\n    C --&gt; E[GPU 1: Graphistry Viz]\n    D --&gt; F[Knowledge Extraction]\n    F --&gt; E\n    E --&gt; G[Graph Visualization]\n\n    style D fill:#4CAF50\n    style E fill:#2196F3\n    style C fill:#FF9800</code></pre>"},{"location":"#split-gpu-configuration","title":"Split-GPU Configuration","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              KAGGLE DUAL T4 SPLIT-GPU ARCHITECTURE             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502   GPU 0: Tesla T4 (15GB)          GPU 1: Tesla T4 (15GB)       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502   \u2502  llama-server       \u2502         \u2502  RAPIDS cuDF        \u2502     \u2502\n\u2502   \u2502  GGUF Model         \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  cuGraph            \u2502     \u2502\n\u2502   \u2502  LLM Inference      \u2502  extract \u2502  Graphistry[ai]     \u2502     \u2502\n\u2502   \u2502  ~5-12GB VRAM       \u2502  graphs  \u2502  Network Viz        \u2502     \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                 \u2502\n\u2502   \u2022 tensor-split for multi-GPU   \u2022 Millions of nodes/edges    \u2502\n\u2502   \u2022 FlashAttention enabled       \u2022 GPU-accelerated rendering  \u2502\n\u2502   \u2022 OpenAI API compatible        \u2022 Interactive exploration    \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#quick-start-5-minutes","title":"Quick Start (5 Minutes)","text":"<p>Get llcuda v2.2.0 running on Kaggle in just 5 minutes!</p>"},{"location":"#step-1-install-llcuda","title":"Step 1: Install llcuda","text":"<pre><code># On Kaggle notebook\npip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"#step-2-verify-dual-t4-setup","title":"Step 2: Verify Dual T4 Setup","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\n# Check GPU configuration\ngpus = detect_gpus()\nprint(f\"\u2713 Detected {len(gpus)} GPUs\")\nprint_gpu_info()\n\n# Expected output:\n# \u2713 Detected 2 GPUs\n# GPU 0: Tesla T4 (15.0 GB)\n# GPU 1: Tesla T4 (15.0 GB)\n</code></pre>"},{"location":"#step-3-run-basic-inference","title":"Step 3: Run Basic Inference","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Configure for single GPU (GPU 0)\nconfig = ServerConfig(\n    model_path=\"model.gguf\",  # Your GGUF model\n    n_gpu_layers=99,          # Offload all layers to GPU\n    flash_attn=True,          # Enable FlashAttention\n)\n\n# Start server\nserver = ServerManager()\nserver.start_with_config(config)\nserver.wait_until_ready()\n\n# Use OpenAI API\nfrom llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient(\"http://localhost:8080\")\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    max_tokens=200\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Auto-Download Binaries</p> <p>CUDA binaries (961 MB) download automatically from GitHub Releases v2.2.0 on first import. Cached for future runs!</p> <p> Full Installation Guide  Kaggle Setup Tutorial</p>"},{"location":"#key-features-of-v220","title":"Key Features of v2.2.0","text":""},{"location":"#1-multi-gpu-inference-on-kaggle","title":"1. Multi-GPU Inference on Kaggle","text":"<p>Run models up to 70B parameters using both T4 GPUs with native CUDA tensor-split:</p> <pre><code>from llcuda.api.multigpu import kaggle_t4_dual_config\n\n# Optimized dual T4 configuration\nconfig = kaggle_t4_dual_config(model_size_gb=25)  # For 70B IQ3_XS\n\nprint(config.to_cli_args())\n# Output: ['-ngl', '-1', '--split-mode', 'layer', '--tensor-split', '0.5,0.5', '-fa']\n</code></pre> <p>Supported Model Sizes on Dual T4 (30GB VRAM):</p> Model Size Quantization VRAM Required Fits Dual T4? 1-3B Q4_K_M 2-3 GB \u2705 Single T4 7-8B Q4_K_M 5-6 GB \u2705 Single T4 13B Q4_K_M 8-9 GB \u2705 Single T4 32-34B Q4_K_M 20-22 GB \u2705 Dual T4 70B IQ3_XS 25-27 GB \u2705 Dual T4 <p>:material-gpu: Multi-GPU Guide</p>"},{"location":"#2-unsloth-fine-tuning-pipeline","title":"2. Unsloth Fine-Tuning Pipeline","text":"<p>Complete workflow from fine-tuning to deployment:</p> Step 1: Fine-Tune with UnslothStep 2: Export to GGUFStep 3: Deploy with llcuda <pre><code>from unsloth import FastLanguageModel\n\n# Load model for training\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n)\n\n# Train your model...\ntrainer.train()\n</code></pre> <pre><code># Export fine-tuned model to GGUF format\nmodel.save_pretrained_gguf(\n    \"my_finetuned_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"  # Recommended for T4\n)\n\n# Output: my_finetuned_model-Q4_K_M.gguf\n</code></pre> <pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Deploy on Kaggle dual T4\nconfig = ServerConfig(\n    model_path=\"my_finetuned_model-Q4_K_M.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Use both GPUs\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n\n# Now serving at http://localhost:8080 with OpenAI API\n</code></pre> <p> Unsloth Integration Guide</p>"},{"location":"#3-split-gpu-architecture-with-graphistry","title":"3. Split-GPU Architecture with Graphistry","text":"<p>Unique capability: Run LLM inference on GPU 0 while using GPU 1 for RAPIDS/Graphistry visualization</p> <pre><code>from llcuda.graphistry import SplitGPUConfig\nimport graphistry\n\n# Configure split-GPU setup\nconfig = SplitGPUConfig(\n    llm_gpu=0,      # GPU 0 for llama-server\n    graph_gpu=1     # GPU 1 for Graphistry\n)\n\n# Set Graphistry to use GPU 1\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\"\n)\n\n# Now run LLM on GPU 0 and visualize graphs on GPU 1\n</code></pre> <p>Use Cases: - Extract knowledge graphs from LLM outputs \u2192 Visualize with Graphistry - Analyze entity relationships in generated text - Interactive exploration of LLM-generated networks - Real-time graph updates from streaming LLM responses</p> <p> Graphistry Integration Guide</p>"},{"location":"#4-29-gguf-quantization-formats","title":"4. 29 GGUF Quantization Formats","text":"<p>llcuda supports all llama.cpp quantization types:</p> K-Quants (Recommended)I-Quants (Best Compression)Legacy QuantsFull Precision <p>Best quality-to-size ratio with double quantization:</p> <ul> <li>Q4_K_M - 4.8 bpw, best for most models (recommended)</li> <li>Q5_K_M - 5.7 bpw, higher quality</li> <li>Q6_K - 6.6 bpw, near FP16 quality</li> <li>Q8_0 - 8.5 bpw, very high quality</li> </ul> <p>Importance-matrix quantization for 70B models:</p> <ul> <li>IQ3_XS - 3.3 bpw, fits 70B on dual T4</li> <li>IQ4_XS - 4.3 bpw, better quality</li> <li>IQ2_XS - 2.3 bpw, extreme compression</li> <li>IQ1_S - 1.6 bpw, smallest possible</li> </ul> <p>Standard quantization types:</p> <ul> <li>Q4_0 - 4.5 bpw, legacy format</li> <li>Q5_0 - 5.5 bpw, legacy format</li> <li>Q8_0 - 8.5 bpw, high precision</li> </ul> <p>Unquantized formats:</p> <ul> <li>F32 - 32-bit float</li> <li>F16 - 16-bit float</li> <li>BF16 - Brain float 16</li> </ul> <p> GGUF Quantization Guide</p>"},{"location":"#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Real Kaggle dual T4 performance metrics:</p> Model Quantization GPUs Tokens/sec Latency VRAM Usage Gemma 2-2B Q4_K_M 2\u00d7 T4 ~60 tok/s - 4 GB Qwen2.5-7B Q4_K_M 2\u00d7 T4 ~35 tok/s - 10 GB Llama-3.1-70B IQ3_XS 2\u00d7 T4 ~8-12 tok/s - 27 GB Gemma 3-1B Q4_K_M 1\u00d7 T4 ~45 tok/s 690ms 3 GB <p>Performance Optimization</p> <ul> <li>Enable FlashAttention for 2-3x speedup</li> <li>Use tensor-split for models &gt;15GB</li> <li>K-quants provide best quality/speed balance</li> <li>I-quants enable 70B models on 30GB VRAM</li> </ul> <p> Full Benchmarks</p>"},{"location":"#tutorial-notebooks-10-kaggle-notebooks","title":"Tutorial Notebooks (10 Kaggle Notebooks)","text":"<p>Complete tutorial series for Kaggle dual T4 environment:</p> # Notebook Open in Kaggle Description Time 01 Quick Start 5-minute introduction 5 min 02 Server Setup Server configuration &amp; lifecycle 15 min 03 Multi-GPU Dual T4 tensor-split 20 min 04 GGUF Quantization K-quants, I-quants, parsing 20 min 05 Unsloth Integration Fine-tune \u2192 GGUF \u2192 Deploy 30 min 06 Split-GPU + Graphistry LLM + RAPIDS visualization 30 min 07 OpenAI API Drop-in OpenAI SDK 15 min 08 NCCL + PyTorch Distributed PyTorch 25 min 09 Large Models (70B) 70B on dual T4 with IQ3_XS 30 min 10 Complete Workflow End-to-end production 45 min <p> View All Tutorials</p>"},{"location":"#learning-paths","title":"Learning Paths","text":"<p>Choose your path based on experience level:</p> Beginner (1 hour)Intermediate (3 hours)Advanced (2 hours)Unsloth Focus (2 hours) <pre><code>01 Quick Start \u2192 02 Server Setup \u2192 03 Multi-GPU\n</code></pre> <p>Perfect for first-time users. Learn the basics of llcuda on Kaggle.</p> <pre><code>01 \u2192 02 \u2192 03 \u2192 04 \u2192 05 \u2192 06 \u2192 07 \u2192 10\n</code></pre> <p>Complete fundamentals through advanced workflows.</p> <pre><code>01 \u2192 03 \u2192 08 \u2192 09\n</code></pre> <p>Focus on multi-GPU and large model deployment.</p> <pre><code>01 \u2192 04 \u2192 05 \u2192 10\n</code></pre> <p>Fine-tuning and deployment pipeline.</p>"},{"location":"#whats-new-in-v220","title":"What's New in v2.2.0","text":"<p>Major Release Highlights</p> <p>Positioned as Unsloth Inference Backend</p> <ul> <li>llcuda is now the official CUDA12 inference backend for Unsloth</li> <li>Seamless workflow: Unsloth (training) \u2192 llcuda (inference)</li> <li>Complete Kaggle dual T4 build notebook included</li> </ul> <p>New Features:</p> <ul> <li>Kaggle Dual T4 Build - Complete build notebook for reproducible binaries</li> <li>Split-GPU Architecture - LLM on GPU 0 + Graphistry on GPU 1</li> <li>Multi-GPU Clarification - Native CUDA tensor-split (NOT NCCL)</li> <li>961MB Binary Package - Pre-built CUDA 12.5 binaries for T4</li> <li>Graphistry Integration - PyGraphistry for knowledge graph visualization</li> <li>70B Model Support - IQ3_XS quantization for large models</li> <li>FlashAttention All Quants - Enabled for all quantization types</li> </ul> <p>Performance:</p> Platform GPU Model Tokens/sec Kaggle 2\u00d7 T4 Gemma 2-2B Q4_K_M ~60 tok/s Kaggle 2\u00d7 T4 Llama 70B IQ3_XS ~12 tok/s <p> Full Changelog</p>"},{"location":"#technical-architecture","title":"Technical Architecture","text":"<p>llcuda v2.2.0 is built on proven technologies:</p> <ul> <li> <p>llama.cpp Server</p> <ul> <li>Build 7760 (commit 388ce82)</li> <li>OpenAI-compatible API</li> <li>Native CUDA tensor-split</li> <li>FlashAttention support</li> </ul> </li> <li> <p>CUDA 12.5</p> <ul> <li>SM 7.5 (Turing) targeting</li> <li>cuBLAS acceleration</li> <li>Static linking</li> <li>961MB binary package</li> </ul> </li> <li> <p>Python 3.11+</p> <ul> <li>Type-safe APIs</li> <li>Async/await support</li> <li>Modern packaging</li> <li>62KB pip package</li> </ul> </li> <li> <p>RAPIDS + Graphistry</p> <ul> <li>cuDF for GPU DataFrames</li> <li>cuGraph for network analysis</li> <li>PyGraphistry visualization</li> <li>Millions of nodes/edges</li> </ul> </li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>llcuda provides comprehensive Python APIs:</p> Module Description <code>llcuda.api.client</code> OpenAI-compatible llama.cpp client <code>llcuda.api.multigpu</code> Multi-GPU configuration for Kaggle <code>llcuda.api.gguf</code> GGUF parsing and quantization tools <code>llcuda.api.nccl</code> NCCL for distributed PyTorch <code>llcuda.server</code> Server lifecycle management <code>llcuda.graphistry</code> Graphistry integration helpers <p> Full API Documentation</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Repository: github.com/llcuda/llcuda</li> <li>GitHub Releases: v2.2.0 Download</li> <li>Bug Reports: GitHub Issues</li> <li>Email: waqasm86@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - Free for commercial and personal use. See LICENSE.</p>    Built with \u2764\ufe0f by Waqas Muhammad | Powered by llama.cpp | Optimized for Unsloth &amp; Graphistry"},{"location":"api/device/","title":"GPU Device Management","text":"<p>Complete API for GPU device detection, compatibility checking, and CUDA management in llcuda v2.1.0.</p>"},{"location":"api/device/#overview","title":"Overview","text":"<p>llcuda provides comprehensive GPU device management functions to help you:</p> <ul> <li>Detect CUDA-capable GPUs</li> <li>Check compatibility with llcuda binaries</li> <li>Get device properties and VRAM information</li> <li>Configure optimal inference settings</li> <li>Handle multi-GPU environments</li> </ul>"},{"location":"api/device/#core-functions","title":"Core Functions","text":""},{"location":"api/device/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if your GPU is compatible with llcuda binaries.</p> <p>Signature: <pre><code>def check_gpu_compatibility(min_compute_cap: float = 5.0) -&gt; Dict[str, Any]\n</code></pre></p> <p>Parameters: - <code>min_compute_cap</code> (float, optional): Minimum compute capability required. Default: 5.0</p> <p>Returns: <pre><code>{\n    'compatible': bool,              # Whether GPU is compatible\n    'compute_capability': float,     # GPU compute capability (e.g., 7.5)\n    'gpu_name': str,                 # GPU name (e.g., \"Tesla T4\")\n    'reason': str,                   # Explanation if not compatible\n    'platform': str                  # Detected platform (local/colab/kaggle)\n}\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\n\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\n    print(f\"   Compute Capability: {compat['compute_capability']}\")\n    print(f\"   Platform: {compat['platform']}\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} is not compatible\")\n    print(f\"   Reason: {compat['reason']}\")\n</code></pre></p> <p>Output on Tesla T4: <pre><code>\u2705 Tesla T4 is compatible!\n   Compute Capability: 7.5\n   Platform: colab\n</code></pre></p>"},{"location":"api/device/#detect_cuda","title":"<code>detect_cuda()</code>","text":"<p>Detect CUDA installation and get detailed GPU information.</p> <p>Signature: <pre><code>def detect_cuda() -&gt; Dict[str, Any]\n</code></pre></p> <p>Returns: <pre><code>{\n    'available': bool,     # Whether CUDA is available\n    'version': str,        # CUDA version (e.g., \"12.2\")\n    'gpus': [              # List of GPU information\n        {\n            'name': str,                  # GPU name\n            'memory': str,                # Total VRAM (e.g., \"15360 MiB\")\n            'driver_version': str,        # NVIDIA driver version\n            'compute_capability': str     # Compute capability\n        }\n    ]\n}\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available']:\n    print(f\"CUDA Version: {cuda_info['version']}\")\n    print(f\"Number of GPUs: {len(cuda_info['gpus'])}\")\n\n    for i, gpu in enumerate(cuda_info['gpus']):\n        print(f\"\\nGPU {i}:\")\n        print(f\"  Name: {gpu['name']}\")\n        print(f\"  VRAM: {gpu['memory']}\")\n        print(f\"  Driver: {gpu['driver_version']}\")\n        print(f\"  Compute Capability: {gpu['compute_capability']}\")\nelse:\n    print(\"CUDA is not available\")\n</code></pre></p> <p>Output: <pre><code>CUDA Version: 12.2\nNumber of GPUs: 1\n\nGPU 0:\n  Name: Tesla T4\n  VRAM: 15360 MiB\n  Driver: 535.104.05\n  Compute Capability: 7.5\n</code></pre></p>"},{"location":"api/device/#get_cuda_device_info","title":"<code>get_cuda_device_info()</code>","text":"<p>Get simplified CUDA device information.</p> <p>Signature: <pre><code>def get_cuda_device_info() -&gt; Optional[Dict[str, Any]]\n</code></pre></p> <p>Returns: <pre><code>{\n    'cuda_version': str,   # CUDA version\n    'gpus': list          # List of GPU dictionaries\n}\n# Returns None if CUDA is not available\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\ndevice_info = llcuda.get_cuda_device_info()\n\nif device_info:\n    print(f\"CUDA: {device_info['cuda_version']}\")\n    print(f\"GPUs detected: {len(device_info['gpus'])}\")\nelse:\n    print(\"No CUDA devices found\")\n</code></pre></p>"},{"location":"api/device/#check_cuda_available","title":"<code>check_cuda_available()</code>","text":"<p>Quick check if CUDA is available.</p> <p>Signature: <pre><code>def check_cuda_available() -&gt; bool\n</code></pre></p> <p>Returns: - <code>True</code> if CUDA is available, <code>False</code> otherwise</p> <p>Example: <pre><code>import llcuda\n\nif llcuda.check_cuda_available():\n    print(\"\u2705 CUDA is available\")\n    # Proceed with GPU inference\nelse:\n    print(\"\u274c CUDA not available - CPU mode only\")\n</code></pre></p>"},{"location":"api/device/#supported-gpus","title":"Supported GPUs","text":"<p>llcuda binaries support NVIDIA GPUs with compute capability 5.0 and higher:</p>"},{"location":"api/device/#architecture-support","title":"Architecture Support","text":"Architecture Compute Cap Examples Status Maxwell 5.0 - 5.3 GTX 900, Tesla M40 \u2705 Supported Pascal 6.0 - 6.2 GTX 10xx, Tesla P100 \u2705 Supported Volta 7.0 Tesla V100 \u2705 Supported Turing 7.5 RTX 20xx, Tesla T4, GTX 16xx \u2705 Verified Ampere 8.0 - 8.6 RTX 30xx, A100 \u2705 Supported Ada Lovelace 8.9 RTX 40xx \u2705 Supported Hopper 9.0 H100 \u2705 Supported"},{"location":"api/device/#popular-gpus","title":"Popular GPUs","text":"GPU Model VRAM Compute Cap Recommended Model Size Tesla T4 15 GB 7.5 Up to 7B (Q4_K_M) RTX 3060 12 GB 8.6 Up to 7B (Q4_K_M) RTX 3070 8 GB 8.6 Up to 3B (Q4_K_M) RTX 3080 10 GB 8.6 Up to 7B (Q4_K_M) RTX 3090 24 GB 8.6 Up to 13B (Q4_K_M) RTX 4070 12 GB 8.9 Up to 7B (Q4_K_M) RTX 4090 24 GB 8.9 Up to 13B (Q4_K_M) A100 40 GB 8.0 Up to 30B (Q4_K_M) A100 80 GB 8.0 Up to 70B (Q4_K_M)"},{"location":"api/device/#vram-management","title":"VRAM Management","text":""},{"location":"api/device/#get-available-vram","title":"Get Available VRAM","text":"<pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available'] and cuda_info['gpus']:\n    gpu = cuda_info['gpus'][0]\n    vram_str = gpu['memory']\n\n    # Parse VRAM\n    if 'GiB' in vram_str:\n        vram_gb = float(vram_str.split()[0])\n    elif 'MiB' in vram_str:\n        vram_mb = float(vram_str.split()[0])\n        vram_gb = vram_mb / 1024\n\n    print(f\"Available VRAM: {vram_gb:.1f} GB\")\n</code></pre>"},{"location":"api/device/#vram-recommendations","title":"VRAM Recommendations","text":"<p>Get recommended settings based on available VRAM:</p> <pre><code>from llcuda.utils import get_recommended_gpu_layers\n\n# For a 1.2 GB model with 15 GB VRAM\ngpu_layers = get_recommended_gpu_layers(\n    model_size_gb=1.2,\n    vram_gb=15.0\n)\n\nprint(f\"Recommended GPU layers: {gpu_layers}\")\n# Output: 99 (full GPU offload)\n</code></pre> <p>VRAM to GPU Layers Mapping:</p> Available VRAM Model Size Recommended Layers &gt;= 1.2x model Any 99 (full offload) &gt;= 0.8x model Any 40 (most layers) &gt;= 0.6x model Any 30 (many layers) &gt;= 0.4x model Any 20 (some layers) &gt;= 0.2x model Any 10 (few layers) &lt; 0.2x model Any 0 (CPU only)"},{"location":"api/device/#auto-configuration","title":"Auto-Configuration","text":""},{"location":"api/device/#auto_configure_for_model","title":"<code>auto_configure_for_model()</code>","text":"<p>Automatically configure optimal settings for your GPU and model.</p> <p>Signature: <pre><code>from llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\ndef auto_configure_for_model(\n    model_path: Path,\n    vram_gb: Optional[float] = None\n) -&gt; Dict[str, Any]\n</code></pre></p> <p>Parameters: - <code>model_path</code> (Path): Path to GGUF model file - <code>vram_gb</code> (float, optional): VRAM in GB (auto-detected if not provided)</p> <p>Returns: <pre><code>{\n    'gpu_layers': int,      # Number of layers to offload\n    'ctx_size': int,        # Context window size\n    'batch_size': int,      # Batch size for processing\n    'ubatch_size': int,     # Micro-batch size\n    'n_parallel': int       # Parallel sequences\n}\n</code></pre></p> <p>Example: <pre><code>from llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Auto-configure for model\nmodel_path = Path(\"/path/to/model.gguf\")\nsettings = auto_configure_for_model(model_path)\n\nprint(\"Recommended settings:\")\nprint(f\"  GPU Layers: {settings['gpu_layers']}\")\nprint(f\"  Context Size: {settings['ctx_size']}\")\nprint(f\"  Batch Size: {settings['batch_size']}\")\nprint(f\"  Micro-batch Size: {settings['ubatch_size']}\")\n\n# Use settings with InferenceEngine\nengine.load_model(\n    str(model_path),\n    gpu_layers=settings['gpu_layers'],\n    ctx_size=settings['ctx_size'],\n    batch_size=settings['batch_size'],\n    ubatch_size=settings['ubatch_size']\n)\n</code></pre></p> <p>Output on Tesla T4 (15 GB): <pre><code>\u2713 Auto-configured for 15.0 GB VRAM\n  GPU Layers: 99\n  Context Size: 4096\n  Batch Size: 2048\n  Micro-batch Size: 512\n</code></pre></p>"},{"location":"api/device/#platform-detection","title":"Platform Detection","text":"<p>llcuda automatically detects the execution environment:</p> <pre><code>import llcuda\n\ncompat = llcuda.check_gpu_compatibility()\nplatform = compat['platform']\n\nif platform == 'colab':\n    print(\"Running on Google Colab\")\n    print(\"Expected GPU: Tesla T4\")\nelif platform == 'kaggle':\n    print(\"Running on Kaggle\")\n    print(\"Expected GPU: Tesla P100 or T4\")\nelse:\n    print(\"Running on local machine\")\n</code></pre> <p>Detected Platforms:</p> <ul> <li><code>colab</code> - Google Colab</li> <li><code>kaggle</code> - Kaggle Notebooks</li> <li><code>local</code> - Local machine or other cloud</li> </ul>"},{"location":"api/device/#multi-gpu-support","title":"Multi-GPU Support","text":""},{"location":"api/device/#selecting-specific-gpu","title":"Selecting Specific GPU","text":"<pre><code>import os\n\n# Use GPU 0 only\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Use GPU 1 only\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Use GPUs 0 and 2\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n\n# Then import llcuda\nimport llcuda\n</code></pre>"},{"location":"api/device/#checking-multiple-gpus","title":"Checking Multiple GPUs","text":"<pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available']:\n    num_gpus = len(cuda_info['gpus'])\n    print(f\"Detected {num_gpus} GPU(s)\")\n\n    for i, gpu in enumerate(cuda_info['gpus']):\n        print(f\"\\nGPU {i}: {gpu['name']}\")\n        print(f\"  VRAM: {gpu['memory']}\")\n        print(f\"  Compute: {gpu['compute_capability']}\")\n</code></pre>"},{"location":"api/device/#environment-setup","title":"Environment Setup","text":""},{"location":"api/device/#setup_environment","title":"<code>setup_environment()</code>","text":"<p>Automatically configure environment variables for optimal performance.</p> <pre><code>from llcuda.utils import setup_environment\n\n# Setup environment\nenv_vars = setup_environment()\n\nprint(\"Environment configured:\")\nfor key, value in env_vars.items():\n    print(f\"  {key}: {value}\")\n</code></pre> <p>Configured Variables:</p> <ul> <li><code>LD_LIBRARY_PATH</code> - Shared library path (Linux)</li> <li><code>CUDA_VISIBLE_DEVICES</code> - Visible GPUs</li> <li><code>LLAMA_CPP_DIR</code> - llama.cpp installation directory</li> </ul>"},{"location":"api/device/#system-information","title":"System Information","text":""},{"location":"api/device/#print_system_info","title":"<code>print_system_info()</code>","text":"<p>Print comprehensive system information for debugging.</p> <pre><code>from llcuda.utils import print_system_info\n\nprint_system_info()\n</code></pre> <p>Output: <pre><code>============================================================\nllcuda System Information\n============================================================\n\nPython:\n  Version: 3.10.12 (main, Nov 20 2023, 15:14:05)\n  Executable: /usr/bin/python3\n\nOperating System:\n  System: Linux\n  Release: 5.15.0-91-generic\n  Machine: x86_64\n\nCUDA:\n  Available: True\n  Version: 12.2\n  GPUs: 1\n    GPU 0: Tesla T4\n      Memory: 15360 MiB\n      Driver: 535.104.05\n      Compute: 7.5\n\nGGUF Models Found: 3\n  - gemma-3-1b-it-Q4_K_M.gguf (872.5 MB)\n  - llama-3.2-3b-Q4_K_M.gguf (1856.2 MB)\n  - qwen2.5-7b-Q4_K_M.gguf (4368.7 MB)\n\n============================================================\n</code></pre></p>"},{"location":"api/device/#common-patterns","title":"Common Patterns","text":""},{"location":"api/device/#complete-gpu-verification","title":"Complete GPU Verification","text":"<pre><code>import llcuda\n\ndef verify_gpu_setup():\n    \"\"\"Verify GPU setup before running inference.\"\"\"\n\n    # Check CUDA availability\n    if not llcuda.check_cuda_available():\n        print(\"\u274c CUDA not available\")\n        return False\n\n    # Get detailed info\n    cuda_info = llcuda.detect_cuda()\n    print(f\"\u2705 CUDA {cuda_info['version']} detected\")\n\n    # Check compatibility\n    compat = llcuda.check_gpu_compatibility()\n\n    if not compat['compatible']:\n        print(f\"\u274c {compat['gpu_name']} is not compatible\")\n        print(f\"   {compat['reason']}\")\n        return False\n\n    print(f\"\u2705 {compat['gpu_name']} is compatible\")\n    print(f\"   Compute Capability: {compat['compute_capability']}\")\n    print(f\"   Platform: {compat['platform']}\")\n\n    # Get VRAM info\n    gpu = cuda_info['gpus'][0]\n    print(f\"   VRAM: {gpu['memory']}\")\n\n    return True\n\n# Use it\nif verify_gpu_setup():\n    print(\"\\n\ud83d\ude80 Ready for inference!\")\n    # Proceed with model loading...\nelse:\n    print(\"\\n\u26a0\ufe0f GPU setup incomplete\")\n</code></pre>"},{"location":"api/device/#auto-configure-and-load","title":"Auto-Configure and Load","text":"<pre><code>import llcuda\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nif not compat['compatible']:\n    raise RuntimeError(f\"GPU not compatible: {compat['reason']}\")\n\n# Auto-configure\nmodel_path = Path(\"model.gguf\")\nsettings = auto_configure_for_model(model_path)\n\n# Create engine with optimal settings\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    str(model_path),\n    **settings,  # Use all auto-configured settings\n    silent=True\n)\n\nprint(\"\u2705 Model loaded with optimal settings!\")\n</code></pre>"},{"location":"api/device/#error-handling","title":"Error Handling","text":""},{"location":"api/device/#handle-no-gpu","title":"Handle No GPU","text":"<pre><code>import llcuda\n\ntry:\n    compat = llcuda.check_gpu_compatibility()\n\n    if not compat['compatible']:\n        raise RuntimeError(compat['reason'])\n\n    # Proceed with GPU inference\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"model.gguf\", gpu_layers=99)\n\nexcept RuntimeError as e:\n    print(f\"GPU Error: {e}\")\n    print(\"Falling back to CPU mode...\")\n\n    # Load with CPU\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"model.gguf\", gpu_layers=0)\n</code></pre>"},{"location":"api/device/#handle-insufficient-vram","title":"Handle Insufficient VRAM","text":"<pre><code>import llcuda\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\nmodel_path = Path(\"large-model.gguf\")\n\ntry:\n    # Try auto-configuration\n    settings = auto_configure_for_model(model_path)\n\n    if settings['gpu_layers'] == 0:\n        print(\"\u26a0\ufe0f Insufficient VRAM for GPU offload\")\n        print(\"   Using CPU mode\")\n    elif settings['gpu_layers'] &lt; 99:\n        print(f\"\u26a0\ufe0f Partial GPU offload: {settings['gpu_layers']} layers\")\n        print(\"   Consider using a smaller model for better performance\")\n\n    # Load with recommended settings\n    engine = llcuda.InferenceEngine()\n    engine.load_model(str(model_path), **settings)\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"Try a smaller model or more aggressive quantization\")\n</code></pre>"},{"location":"api/device/#see-also","title":"See Also","text":"<ul> <li>API Overview - Complete API reference</li> <li>InferenceEngine - Inference API</li> <li>Models &amp; GGUF - Model management</li> <li>Performance Guide - Optimization techniques</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"api/examples/","title":"Code Examples","text":"<p>Complete, production-ready code examples for common llcuda use cases.</p>"},{"location":"api/examples/#quick-reference","title":"Quick Reference","text":"Example Use Case Complexity Basic Inference Single question-answer Beginner Chat Application Interactive conversation Beginner Batch Processing Process multiple prompts Beginner Streaming Inference Real-time token generation Intermediate Custom Parameters Fine-tune generation Intermediate Context Manager Auto-cleanup resources Intermediate Error Handling Production-ready code Advanced Benchmarking Measure performance Advanced"},{"location":"api/examples/#basic-inference","title":"Basic Inference","text":"<p>Simple question-answer inference.</p> <pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\n# Print results\nprint(f\"Response: {result.text}\")\nprint(f\"\\nPerformance:\")\nprint(f\"  Speed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"  Latency: {result.latency_ms:.0f}ms\")\nprint(f\"  Tokens: {result.tokens_generated}\")\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Response: Quantum computing uses quantum mechanics principles...\n\nPerformance:\n  Speed: 134.2 tokens/sec\n  Latency: 690ms\n  Tokens: 93\n</code></pre></p>"},{"location":"api/examples/#chat-application","title":"Chat Application","text":"<p>Interactive chat with conversation loop.</p> <pre><code>import llcuda\n\ndef chat_application():\n    \"\"\"Interactive chat application with Gemma 3-1B.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n\n    print(\"Loading Gemma 3-1B model...\")\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    print(\"\\n\ud83e\udd16 Chat with Gemma 3-1B\")\n    print(\"Type 'exit' to quit, 'clear' to reset metrics\\n\")\n\n    while True:\n        # Get user input\n        user_input = input(\"You: \").strip()\n\n        # Handle commands\n        if user_input.lower() == 'exit':\n            print(\"\\nGoodbye!\")\n            break\n\n        if user_input.lower() == 'clear':\n            engine.reset_metrics()\n            print(\"\u2705 Metrics reset\\n\")\n            continue\n\n        if not user_input:\n            continue\n\n        # Generate response\n        result = engine.infer(\n            user_input,\n            max_tokens=300,\n            temperature=0.7\n        )\n\n        # Display response\n        print(f\"\\n\ud83e\udd16 AI: {result.text}\")\n        print(f\"   ({result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms)\\n\")\n\n    # Show final metrics\n    metrics = engine.get_metrics()\n    print(\"\\n\ud83d\udcca Session Statistics:\")\n    print(f\"  Total requests: {metrics['throughput']['total_requests']}\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Run the chat app\nif __name__ == \"__main__\":\n    chat_application()\n</code></pre> <p>Sample Interaction: <pre><code>You: What is machine learning?\n\n\ud83e\udd16 AI: Machine learning is a subset of artificial intelligence that enables\n   computers to learn from data without explicit programming...\n   (134.5 tok/s, 685ms)\n\nYou: Give me an example\n\n\ud83e\udd16 AI: A common example is email spam filtering. The system learns to\n   identify spam by analyzing thousands of emails...\n   (136.2 tok/s, 702ms)\n\nYou: exit\n\n\ud83d\udcca Session Statistics:\n  Total requests: 2\n  Total tokens: 184\n  Avg speed: 135.2 tok/s\n  Avg latency: 694ms\n</code></pre></p>"},{"location":"api/examples/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently.</p> <pre><code>import llcuda\nimport time\n\ndef batch_processing_example():\n    \"\"\"Process multiple prompts with performance tracking.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define prompts\n    prompts = [\n        \"What is artificial intelligence?\",\n        \"Explain neural networks briefly.\",\n        \"What is deep learning?\",\n        \"Define machine learning.\",\n        \"What are transformers in AI?\",\n        \"Explain backpropagation.\",\n        \"What is gradient descent?\",\n        \"Define overfitting in ML.\"\n    ]\n\n    print(f\"Processing {len(prompts)} prompts...\\n\")\n\n    # Reset metrics\n    engine.reset_metrics()\n\n    # Process batch\n    start_time = time.time()\n    results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n    total_time = time.time() - start_time\n\n    # Display results\n    for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n        print(f\"{i}. Q: {prompt}\")\n        print(f\"   A: {result.text[:100]}...\")\n        print(f\"   Performance: {result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms\\n\")\n\n    # Show aggregate metrics\n    metrics = engine.get_metrics()\n    print(\"\ud83d\udcca Batch Processing Summary:\")\n    print(f\"  Prompts processed: {len(prompts)}\")\n    print(f\"  Total time: {total_time:.2f}s\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg throughput: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n    print(f\"  P95 latency: {metrics['latency']['p95_ms']:.0f}ms\")\n    print(f\"  Requests/sec: {len(prompts) / total_time:.2f}\")\n\n# Run batch processing\nif __name__ == \"__main__\":\n    batch_processing_example()\n</code></pre> <p>Expected Output: <pre><code>Processing 8 prompts...\n\n1. Q: What is artificial intelligence?\n   A: Artificial intelligence (AI) is the simulation of human intelligence...\n   Performance: 134.8 tok/s, 685ms\n\n2. Q: Explain neural networks briefly.\n   A: Neural networks are computational models inspired by the human brain...\n   Performance: 135.2 tok/s, 692ms\n\n[...]\n\n\ud83d\udcca Batch Processing Summary:\n  Prompts processed: 8\n  Total time: 5.52s\n  Total tokens: 592\n  Avg throughput: 134.5 tok/s\n  Avg latency: 690ms\n  P95 latency: 725ms\n  Requests/sec: 1.45\n</code></pre></p>"},{"location":"api/examples/#streaming-inference","title":"Streaming Inference","text":"<p>Stream tokens as they're generated (simulation).</p> <pre><code>import llcuda\nimport time\n\ndef streaming_inference_example():\n    \"\"\"Demonstrate streaming inference with callback.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define callback for streaming\n    def stream_callback(chunk):\n        \"\"\"Print each chunk as it arrives.\"\"\"\n        print(chunk, end='', flush=True)\n\n    prompt = \"Write a short story about a robot learning to paint\"\n\n    print(\"\ud83e\udd16 Generating story (streaming):\\n\")\n    print(\"AI: \", end='', flush=True)\n\n    # Stream inference\n    result = engine.infer_stream(\n        prompt,\n        callback=stream_callback,\n        max_tokens=200,\n        temperature=0.8\n    )\n\n    # Show metrics\n    print(f\"\\n\\n\ud83d\udcca Performance:\")\n    print(f\"  Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"  Latency: {result.latency_ms:.0f}ms\")\n    print(f\"  Tokens: {result.tokens_generated}\")\n\n# Run streaming example\nif __name__ == \"__main__\":\n    streaming_inference_example()\n</code></pre> <p>Note: Current implementation simulates streaming. True token-by-token streaming will be available in a future release.</p>"},{"location":"api/examples/#custom-generation-parameters","title":"Custom Generation Parameters","text":"<p>Fine-tune generation with custom parameters.</p> <pre><code>import llcuda\n\ndef custom_parameters_example():\n    \"\"\"Demonstrate different generation strategies.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    prompt = \"Once upon a time in a futuristic city\"\n\n    # Strategy 1: Deterministic (low temperature)\n    print(\"1\ufe0f\u20e3 Deterministic Generation (temp=0.1):\")\n    result1 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.1,\n        top_p=0.9,\n        top_k=10\n    )\n    print(f\"{result1.text}\\n\")\n\n    # Strategy 2: Balanced (default)\n    print(\"2\ufe0f\u20e3 Balanced Generation (temp=0.7):\")\n    result2 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.7,\n        top_p=0.9,\n        top_k=40\n    )\n    print(f\"{result2.text}\\n\")\n\n    # Strategy 3: Creative (high temperature)\n    print(\"3\ufe0f\u20e3 Creative Generation (temp=1.2):\")\n    result3 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.2,\n        top_p=0.95,\n        top_k=100\n    )\n    print(f\"{result3.text}\\n\")\n\n    # Strategy 4: Very creative (high temp + nucleus sampling)\n    print(\"4\ufe0f\u20e3 Very Creative (temp=1.5, top_p=0.95):\")\n    result4 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.5,\n        top_p=0.95,\n        top_k=200\n    )\n    print(f\"{result4.text}\\n\")\n\n    # Compare performance\n    print(\"\ud83d\udcca Performance Comparison:\")\n    for i, result in enumerate([result1, result2, result3, result4], 1):\n        print(f\"  Strategy {i}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Run custom parameters example\nif __name__ == \"__main__\":\n    custom_parameters_example()\n</code></pre> <p>Parameter Guide:</p> Parameter Range Effect Use Case <code>temperature</code> 0.1 - 0.3 Deterministic, focused Code, facts <code>temperature</code> 0.6 - 0.8 Balanced creativity General chat <code>temperature</code> 1.0 - 1.5 Very creative Stories, brainstorming <code>top_p</code> 0.9 - 0.95 Nucleus sampling Quality control <code>top_k</code> 10 - 200 Diversity limit Token variety"},{"location":"api/examples/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Automatic resource cleanup.</p> <pre><code>import llcuda\n\ndef context_manager_example():\n    \"\"\"Use context manager for automatic cleanup.\"\"\"\n\n    # Context manager ensures server cleanup\n    with llcuda.InferenceEngine() as engine:\n        # Load model\n        engine.load_model(\n            \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n            silent=True\n        )\n\n        # Run inferences\n        prompts = [\n            \"What is Python?\",\n            \"What is JavaScript?\",\n            \"What is Rust?\"\n        ]\n\n        for prompt in prompts:\n            result = engine.infer(prompt, max_tokens=50)\n            print(f\"Q: {prompt}\")\n            print(f\"A: {result.text}\\n\")\n\n        # Get final metrics\n        metrics = engine.get_metrics()\n        print(f\"Total tokens: {metrics['throughput']['total_tokens']}\")\n\n    # Server automatically stopped here\n    print(\"\u2705 Server cleaned up automatically\")\n\n# Run context manager example\nif __name__ == \"__main__\":\n    context_manager_example()\n</code></pre>"},{"location":"api/examples/#robust-error-handling","title":"Robust Error Handling","text":"<p>Production-ready error handling.</p> <pre><code>import llcuda\nfrom llcuda import InferenceEngine\n\ndef robust_inference(prompt: str, max_retries: int = 3):\n    \"\"\"Robust inference with error handling and retries.\"\"\"\n\n    engine = None\n\n    try:\n        # Check GPU compatibility\n        compat = llcuda.check_gpu_compatibility()\n        if not compat['compatible']:\n            raise RuntimeError(\n                f\"GPU {compat['gpu_name']} is not compatible: {compat['reason']}\"\n            )\n\n        # Initialize engine\n        engine = InferenceEngine()\n\n        # Load model with error handling\n        try:\n            engine.load_model(\n                \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n                silent=True,\n                auto_start=True\n            )\n        except FileNotFoundError as e:\n            print(f\"Model not found: {e}\")\n            print(\"Please download the model first\")\n            return None\n        except RuntimeError as e:\n            print(f\"Server failed to start: {e}\")\n            return None\n\n        # Run inference with retries\n        for attempt in range(max_retries):\n            result = engine.infer(prompt, max_tokens=200)\n\n            if result.success:\n                return {\n                    'text': result.text,\n                    'tokens_per_sec': result.tokens_per_sec,\n                    'latency_ms': result.latency_ms,\n                    'success': True\n                }\n            else:\n                print(f\"Attempt {attempt + 1} failed: {result.error_message}\")\n                if attempt &lt; max_retries - 1:\n                    print(f\"Retrying... ({max_retries - attempt - 1} attempts left)\")\n                    import time\n                    time.sleep(1)\n\n        # All retries failed\n        return {\n            'text': None,\n            'error': 'All retry attempts failed',\n            'success': False\n        }\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {\n            'text': None,\n            'error': str(e),\n            'success': False\n        }\n\n    finally:\n        # Cleanup\n        if engine is not None:\n            engine.unload_model()\n\n# Example usage\nif __name__ == \"__main__\":\n    result = robust_inference(\"What is quantum computing?\")\n\n    if result and result['success']:\n        print(f\"\u2705 Success!\")\n        print(f\"Response: {result['text']}\")\n        print(f\"Speed: {result['tokens_per_sec']:.1f} tok/s\")\n    else:\n        print(f\"\u274c Failed: {result['error'] if result else 'Unknown error'}\")\n</code></pre>"},{"location":"api/examples/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Comprehensive performance measurement.</p> <pre><code>import llcuda\nimport time\nimport statistics\n\ndef benchmark_inference(num_runs: int = 10):\n    \"\"\"Benchmark inference performance.\"\"\"\n\n    # Initialize\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Warmup\n    print(\"Warming up...\")\n    for _ in range(3):\n        engine.infer(\"Warmup prompt\", max_tokens=10)\n\n    # Benchmark\n    print(f\"Running {num_runs} iterations...\\n\")\n\n    engine.reset_metrics()\n    latencies = []\n    throughputs = []\n\n    test_prompt = \"Explain the concept of recursion in programming\"\n\n    for i in range(num_runs):\n        start = time.time()\n        result = engine.infer(test_prompt, max_tokens=100)\n        elapsed = (time.time() - start) * 1000  # Convert to ms\n\n        latencies.append(result.latency_ms)\n        throughputs.append(result.tokens_per_sec)\n\n        print(f\"Run {i+1}/{num_runs}: \"\n              f\"{result.tokens_per_sec:.1f} tok/s, \"\n              f\"{result.latency_ms:.0f}ms\")\n\n    # Calculate statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83d\udcca Benchmark Results\")\n    print(\"=\"*60)\n\n    print(\"\\nThroughput (tokens/sec):\")\n    print(f\"  Mean:   {statistics.mean(throughputs):.2f}\")\n    print(f\"  Median: {statistics.median(throughputs):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(throughputs):.2f}\")\n    print(f\"  Min:    {min(throughputs):.2f}\")\n    print(f\"  Max:    {max(throughputs):.2f}\")\n\n    print(\"\\nLatency (ms):\")\n    print(f\"  Mean:   {statistics.mean(latencies):.2f}\")\n    print(f\"  Median: {statistics.median(latencies):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(latencies):.2f}\")\n    print(f\"  Min:    {min(latencies):.2f}\")\n    print(f\"  Max:    {max(latencies):.2f}\")\n\n    # Percentiles\n    sorted_latencies = sorted(latencies)\n    p50_idx = len(sorted_latencies) // 2\n    p95_idx = int(len(sorted_latencies) * 0.95)\n    p99_idx = int(len(sorted_latencies) * 0.99)\n\n    print(\"\\nLatency Percentiles:\")\n    print(f\"  P50:    {sorted_latencies[p50_idx]:.2f}ms\")\n    print(f\"  P95:    {sorted_latencies[p95_idx]:.2f}ms\")\n    print(f\"  P99:    {sorted_latencies[p99_idx]:.2f}ms\")\n\n    # Get metrics from engine\n    metrics = engine.get_metrics()\n    print(f\"\\nTotal tokens generated: {metrics['throughput']['total_tokens']}\")\n    print(f\"Total requests: {metrics['throughput']['total_requests']}\")\n\n    print(\"=\"*60)\n\n# Run benchmark\nif __name__ == \"__main__\":\n    benchmark_inference(num_runs=10)\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Warming up...\nRunning 10 iterations...\n\nRun 1/10: 134.2 tok/s, 690ms\nRun 2/10: 136.5 tok/s, 685ms\nRun 3/10: 133.8 tok/s, 695ms\n[...]\n\n============================================================\n\ud83d\udcca Benchmark Results\n============================================================\n\nThroughput (tokens/sec):\n  Mean:   134.52\n  Median: 134.30\n  Stdev:  1.24\n  Min:    132.80\n  Max:    136.50\n\nLatency (ms):\n  Mean:   692.45\n  Median: 690.00\n  Stdev:  8.32\n  Min:    685.00\n  Max:    710.00\n\nLatency Percentiles:\n  P50:    690.00ms\n  P95:    705.00ms\n  P99:    710.00ms\n\nTotal tokens generated: 940\nTotal requests: 10\n============================================================\n</code></pre></p>"},{"location":"api/examples/#advanced-custom-chat-engine","title":"Advanced: Custom Chat Engine","text":"<p>Using the ChatEngine for conversations.</p> <pre><code>from llcuda import InferenceEngine\nfrom llcuda.chat import ChatEngine\n\ndef advanced_chat_example():\n    \"\"\"Advanced chat with conversation history.\"\"\"\n\n    # Initialize inference engine\n    engine = InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Create chat engine with system prompt\n    chat = ChatEngine(\n        engine,\n        system_prompt=\"You are a helpful AI coding assistant specialized in Python.\",\n        max_history=20,\n        max_tokens=200,\n        temperature=0.7\n    )\n\n    # Conversation\n    chat.add_user_message(\"How do I read a file in Python?\")\n    response1 = chat.complete()\n    print(f\"AI: {response1}\\n\")\n\n    chat.add_user_message(\"Can you show me an example?\")\n    response2 = chat.complete()\n    print(f\"AI: {response2}\\n\")\n\n    chat.add_user_message(\"What about writing to a file?\")\n    response3 = chat.complete()\n    print(f\"AI: {response3}\\n\")\n\n    # Get conversation history\n    history = chat.get_history()\n    print(f\"Conversation has {len(history)} messages\")\n\n    # Save conversation\n    chat.save_history(\"conversation.json\")\n    print(\"\u2705 Conversation saved to conversation.json\")\n\n    # Token count\n    token_count = chat.count_tokens()\n    print(f\"Approximate token count: {token_count}\")\n\n# Run advanced chat\nif __name__ == \"__main__\":\n    advanced_chat_example()\n</code></pre>"},{"location":"api/examples/#see-also","title":"See Also","text":"<ul> <li>API Overview - Complete API reference</li> <li>InferenceEngine - Detailed engine documentation</li> <li>Quick Start - Getting started guide</li> <li>Tutorials - Step-by-step tutorials</li> <li>Performance - Benchmark results</li> </ul>"},{"location":"api/inference-engine/","title":"InferenceEngine API Reference","text":"<p>Complete API documentation for the <code>InferenceEngine</code> class, the main interface for llcuda inference.</p>"},{"location":"api/inference-engine/#class-overview","title":"Class Overview","text":"<pre><code>class InferenceEngine:\n    \"\"\"\n    High-level Python interface for LLM inference with CUDA acceleration.\n\n    Provides automatic server management, model loading, and inference APIs.\n    \"\"\"\n</code></pre>"},{"location":"api/inference-engine/#constructor","title":"Constructor","text":""},{"location":"api/inference-engine/#__init__server_urlhttp1270018090","title":"<code>__init__(server_url=\"http://127.0.0.1:8090\")</code>","text":"<p>Initialize a new inference engine.</p> <p>Parameters:</p> Parameter Type Default Description <code>server_url</code> <code>str</code> <code>\"http://127.0.0.1:8090\"</code> URL of llama-server backend <p>Example:</p> <pre><code>import llcuda\n\n# Default URL\nengine = llcuda.InferenceEngine()\n\n# Custom port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n\n# Remote server\nengine = llcuda.InferenceEngine(server_url=\"http://192.168.1.100:8090\")\n</code></pre>"},{"location":"api/inference-engine/#methods","title":"Methods","text":""},{"location":"api/inference-engine/#load_model","title":"<code>load_model()</code>","text":"<p>Load a GGUF model for inference with automatic configuration.</p> <pre><code>def load_model(\n    model_name_or_path: str,\n    gpu_layers: Optional[int] = None,\n    ctx_size: Optional[int] = None,\n    auto_start: bool = True,\n    auto_configure: bool = True,\n    n_parallel: int = 1,\n    verbose: bool = True,\n    interactive_download: bool = True,\n    silent: bool = False,\n    **kwargs\n) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_name_or_path</code> <code>str</code> Required Model registry name, local path, or HF repo <code>gpu_layers</code> <code>Optional[int]</code> <code>None</code> Number of layers to offload to GPU (None=auto) <code>ctx_size</code> <code>Optional[int]</code> <code>None</code> Context window size (None=auto) <code>auto_start</code> <code>bool</code> <code>True</code> Automatically start llama-server if not running <code>auto_configure</code> <code>bool</code> <code>True</code> Auto-detect optimal settings <code>n_parallel</code> <code>int</code> <code>1</code> Number of parallel sequences <code>verbose</code> <code>bool</code> <code>True</code> Print status messages <code>interactive_download</code> <code>bool</code> <code>True</code> Ask before downloading models <code>silent</code> <code>bool</code> <code>False</code> Suppress llama-server output <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional server parameters <p>Returns:</p> <ul> <li><code>bool</code> - True if model loaded successfully</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code> - Model file not found</li> <li><code>ValueError</code> - Model download cancelled</li> <li><code>ConnectionError</code> - Server not running and auto_start=False</li> <li><code>RuntimeError</code> - Server failed to start</li> </ul> <p>Loading Methods:</p> Registry NameHuggingFace SyntaxLocal Path <pre><code># Auto-download from HuggingFace registry\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre> <pre><code># Direct HF download\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre> <pre><code># Load local GGUF file\nengine.load_model(\"/path/to/model.gguf\")\n</code></pre> <p>Example:</p> <pre><code># Auto-configuration (recommended)\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    auto_configure=True,\n    verbose=True\n)\n\n# Manual configuration\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=35,\n    ctx_size=4096,\n    batch_size=512,\n    ubatch_size=128,\n    auto_configure=False\n)\n\n# Silent mode\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    silent=True,  # No llama-server output\n    verbose=False # No status messages\n)\n</code></pre>"},{"location":"api/inference-engine/#infer","title":"<code>infer()</code>","text":"<p>Run inference on a single prompt.</p> <pre><code>def infer(\n    prompt: str,\n    max_tokens: int = 128,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    top_k: int = 40,\n    seed: int = 0,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; InferResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompt</code> <code>str</code> Required Input prompt text <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>0.7</code> Sampling temperature (0.0-2.0) <code>top_p</code> <code>float</code> <code>0.9</code> Nucleus sampling threshold <code>top_k</code> <code>int</code> <code>40</code> Top-k sampling limit <code>seed</code> <code>int</code> <code>0</code> Random seed (0=random) <code>stop_sequences</code> <code>Optional[List[str]]</code> <code>None</code> Stop generation at these sequences <p>Returns:</p> <ul> <li><code>InferResult</code> - Result object with text and metrics</li> </ul> <p>Example:</p> <pre><code># Basic inference\nresult = engine.infer(\n    prompt=\"What is AI?\",\n    max_tokens=100\n)\nprint(result.text)\n\n# Advanced parameters\nresult = engine.infer(\n    prompt=\"Write a poem about\",\n    max_tokens=200,\n    temperature=0.9,  # More creative\n    top_p=0.95,\n    top_k=50,\n    seed=42,\n    stop_sequences=[\"\\n\\n\", \"###\"]\n)\n\n# Check results\nif result.success:\n    print(f\"Generated: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"Latency: {result.latency_ms:.0f} ms\")\nelse:\n    print(f\"Error: {result.error_message}\")\n</code></pre>"},{"location":"api/inference-engine/#batch_infer","title":"<code>batch_infer()</code>","text":"<p>Run batch inference on multiple prompts.</p> <pre><code>def batch_infer(\n    prompts: List[str],\n    max_tokens: int = 128,\n    **kwargs\n) -&gt; List[InferResult]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompts</code> <code>List[str]</code> Required List of input prompts <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens per prompt <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters (temperature, top_p, etc.) <p>Returns:</p> <ul> <li><code>List[InferResult]</code> - List of result objects</li> </ul> <p>Example:</p> <pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"What is deep learning?\"\n]\n\nresults = engine.batch_infer(\n    prompts,\n    max_tokens=100,\n    temperature=0.7\n)\n\nfor i, result in enumerate(results):\n    print(f\"Prompt {i+1}: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"api/inference-engine/#infer_stream","title":"<code>infer_stream()</code>","text":"<p>Run streaming inference with real-time callbacks.</p> <pre><code>def infer_stream(\n    prompt: str,\n    callback: Callable[[str], None],\n    max_tokens: int = 128,\n    temperature: float = 0.7,\n    **kwargs\n) -&gt; InferResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompt</code> <code>str</code> Required Input prompt text <code>callback</code> <code>Callable</code> Required Function called for each chunk <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>0.7</code> Sampling temperature <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters <p>Returns:</p> <ul> <li><code>InferResult</code> - Complete result after streaming</li> </ul> <p>Example:</p> <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    prompt=\"Write a story about AI:\",\n    callback=print_chunk,\n    max_tokens=200,\n    temperature=0.8\n)\n\nprint(f\"\\n\\nTotal speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"api/inference-engine/#check_server","title":"<code>check_server()</code>","text":"<p>Check if llama-server is running and accessible.</p> <pre><code>def check_server() -&gt; bool\n</code></pre> <p>Returns:</p> <ul> <li><code>bool</code> - True if server is accessible, False otherwise</li> </ul> <p>Example:</p> <pre><code>if engine.check_server():\n    print(\"Server is running\")\nelse:\n    print(\"Server is not running\")\n    # Optionally start it\n    engine.load_model(\"model.gguf\", auto_start=True)\n</code></pre>"},{"location":"api/inference-engine/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get current performance metrics.</p> <pre><code>def get_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Returns:</p> <ul> <li><code>dict</code> - Dictionary with latency, throughput, and GPU metrics</li> </ul> <p>Return Structure:</p> <pre><code>{\n    'latency': {\n        'mean_ms': float,\n        'p50_ms': float,\n        'p95_ms': float,\n        'p99_ms': float,\n        'min_ms': float,\n        'max_ms': float,\n        'sample_count': int\n    },\n    'throughput': {\n        'total_tokens': int,\n        'total_requests': int,\n        'tokens_per_sec': float,\n        'requests_per_sec': float\n    }\n}\n</code></pre> <p>Example:</p> <pre><code>metrics = engine.get_metrics()\n\nprint(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"Total requests: {metrics['throughput']['total_requests']}\")\n</code></pre>"},{"location":"api/inference-engine/#reset_metrics","title":"<code>reset_metrics()</code>","text":"<p>Reset performance metrics counters.</p> <pre><code>def reset_metrics() -&gt; None\n</code></pre> <p>Example:</p> <pre><code># Reset before benchmark\nengine.reset_metrics()\n\n# Run tests\nfor i in range(100):\n    engine.infer(f\"Test {i}\", max_tokens=50)\n\n# Get clean metrics\nmetrics = engine.get_metrics()\nprint(f\"Benchmark: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n</code></pre>"},{"location":"api/inference-engine/#unload_model","title":"<code>unload_model()</code>","text":"<p>Unload the current model and stop server.</p> <pre><code>def unload_model() -&gt; None\n</code></pre> <p>Example:</p> <pre><code># Unload when done\nengine.unload_model()\n\n# Load different model\nengine.load_model(\"other-model.gguf\", auto_start=True)\n</code></pre>"},{"location":"api/inference-engine/#properties","title":"Properties","text":""},{"location":"api/inference-engine/#is_loaded","title":"<code>is_loaded</code>","text":"<p>Check if a model is currently loaded.</p> <pre><code>@property\ndef is_loaded() -&gt; bool\n</code></pre> <p>Example:</p> <pre><code>if engine.is_loaded:\n    print(\"Model is loaded\")\n    result = engine.infer(\"Test\", max_tokens=10)\nelse:\n    print(\"No model loaded\")\n    engine.load_model(\"model.gguf\")\n</code></pre>"},{"location":"api/inference-engine/#context-manager-support","title":"Context Manager Support","text":"<p><code>InferenceEngine</code> supports context manager protocol for automatic cleanup.</p> <p>Example:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\n    result = engine.infer(\"Hello!\", max_tokens=50)\n    print(result.text)\n\n# Server automatically stops when exiting context\n</code></pre>"},{"location":"api/inference-engine/#inferresult-class","title":"InferResult Class","text":"<p>Result object returned by inference methods.</p>"},{"location":"api/inference-engine/#properties_1","title":"Properties","text":"Property Type Description <code>success</code> <code>bool</code> Whether inference succeeded <code>text</code> <code>str</code> Generated text <code>tokens_generated</code> <code>int</code> Number of tokens generated <code>latency_ms</code> <code>float</code> Inference latency in milliseconds <code>tokens_per_sec</code> <code>float</code> Generation throughput <code>error_message</code> <code>str</code> Error message if failed"},{"location":"api/inference-engine/#example","title":"Example:","text":"<pre><code>result = engine.infer(\"Test\", max_tokens=50)\n\n# Access properties\nprint(f\"Text: {result.text}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Tokens: {result.tokens_generated}\")\nprint(f\"Latency: {result.latency_ms:.0f} ms\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# String representation\nprint(str(result))  # Returns result.text\n\n# Repr\nprint(repr(result))\n# Output: InferResult(tokens=50, latency=745.2ms, throughput=134.2 tok/s)\n</code></pre>"},{"location":"api/inference-engine/#complete-example","title":"Complete Example","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model with auto-configuration\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n\n# Single inference\nresult = engine.infer(\n    prompt=\"What is machine learning?\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Batch inference\nprompts = [\n    \"Explain AI\",\n    \"What are neural networks?\",\n    \"Define deep learning\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\nfor i, r in enumerate(results):\n    print(f\"\\nPrompt {i+1}: {r.text}\")\n\n# Get metrics\nmetrics = engine.get_metrics()\nprint(f\"\\nTotal speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n\n# Cleanup\nengine.unload_model()\n</code></pre>"},{"location":"api/inference-engine/#see-also","title":"See Also","text":"<ul> <li>Models API - Model management</li> <li>Device API - GPU device management</li> <li>Examples - More code examples</li> <li>Performance Guide - Optimization tips</li> </ul>"},{"location":"api/models/","title":"Models API Reference","text":"<p>API documentation for model management, discovery, and downloading in llcuda.</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>The <code>llcuda.models</code> module provides utilities for:</p> <ul> <li>Loading models from registry or HuggingFace</li> <li>Downloading and caching GGUF models</li> <li>Getting model metadata and information</li> <li>Recommending optimal inference settings</li> </ul>"},{"location":"api/models/#functions","title":"Functions","text":""},{"location":"api/models/#load_model_smart","title":"<code>load_model_smart()</code>","text":"<p>Smart model loading with automatic download and path resolution.</p> <pre><code>def load_model_smart(\n    model_name_or_path: str,\n    interactive: bool = True\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_name_or_path</code> <code>str</code> Required Model name, HF repo, or local path <code>interactive</code> <code>bool</code> <code>True</code> Ask for confirmation before downloading <p>Returns:</p> <ul> <li><code>Path</code> - Path to model file</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import load_model_smart\n\n# Load from registry\nmodel_path = load_model_smart(\"gemma-3-1b-Q4_K_M\")\n\n# Load from HuggingFace\nmodel_path = load_model_smart(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Load local file\nmodel_path = load_model_smart(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"api/models/#download_model","title":"<code>download_model()</code>","text":"<p>Download a model from HuggingFace.</p> <pre><code>def download_model(\n    repo_id: str,\n    filename: str,\n    cache_dir: Optional[str] = None\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>repo_id</code> <code>str</code> Required HuggingFace repository ID <code>filename</code> <code>str</code> Required Model filename to download <code>cache_dir</code> <code>Optional[str]</code> <code>None</code> Custom cache directory <p>Returns:</p> <ul> <li><code>Path</code> - Path to downloaded model</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import download_model\n\n# Download from HuggingFace\nmodel_path = download_model(\n    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n    filename=\"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"api/models/#list_registry_models","title":"<code>list_registry_models()</code>","text":"<p>List available models in the registry.</p> <pre><code>def list_registry_models() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Returns:</p> <ul> <li><code>List[Dict]</code> - List of model information dictionaries</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import list_registry_models\n\nmodels = list_registry_models()\nfor model in models:\n    print(f\"{model['name']}: {model['description']}\")\n</code></pre>"},{"location":"api/models/#classes","title":"Classes","text":""},{"location":"api/models/#modelinfo","title":"<code>ModelInfo</code>","text":"<p>Extract metadata from GGUF models.</p> <pre><code>class ModelInfo:\n    def __init__(self, filepath: str)\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>filepath</code> <code>Path</code> Path to GGUF file <code>architecture</code> <code>Optional[str]</code> Model architecture (e.g., \"llama\", \"gemma\") <code>parameter_count</code> <code>Optional[int]</code> Estimated parameter count <code>context_length</code> <code>Optional[int]</code> Maximum context length <code>quantization</code> <code>Optional[str]</code> Quantization type <code>file_size_mb</code> <code>float</code> File size in MB <p>Methods:</p>"},{"location":"api/models/#get_recommended_settings","title":"<code>get_recommended_settings()</code>","text":"<p>Get recommended inference settings based on model and hardware.</p> <pre><code>def get_recommended_settings(\n    vram_gb: float = 8.0\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>vram_gb</code> <code>float</code> <code>8.0</code> Available VRAM in GB <p>Returns:</p> <pre><code>{\n    'gpu_layers': int,\n    'ctx_size': int,\n    'batch_size': int,\n    'ubatch_size': int\n}\n</code></pre> <p>Example:</p> <pre><code>from llcuda.models import ModelInfo\n\n# Load model info\ninfo = ModelInfo(\"gemma-3-1b-Q4_K_M.gguf\")\n\nprint(f\"Architecture: {info.architecture}\")\nprint(f\"Parameters: {info.parameter_count}B\")\nprint(f\"Context: {info.context_length}\")\nprint(f\"Size: {info.file_size_mb:.1f} MB\")\n\n# Get recommended settings for T4 (15GB)\nsettings = info.get_recommended_settings(vram_gb=15.0)\nprint(f\"Recommended gpu_layers: {settings['gpu_layers']}\")\nprint(f\"Recommended ctx_size: {settings['ctx_size']}\")\n</code></pre>"},{"location":"api/models/#model-registry","title":"Model Registry","text":"<p>llcuda includes a built-in model registry with popular models:</p> <pre><code>REGISTRY = {\n    \"gemma-3-1b-Q4_K_M\": {\n        \"repo\": \"unsloth/gemma-3-1b-it-GGUF\",\n        \"file\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n        \"size\": \"700 MB\",\n        \"description\": \"Gemma 3 1B Instruct, Q4_K_M quantized\"\n    },\n    \"llama-3.2-3b-Q4_K_M\": {\n        \"repo\": \"unsloth/Llama-3.2-3B-Instruct-GGUF\",\n        \"file\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n        \"size\": \"1.9 GB\",\n        \"description\": \"Llama 3.2 3B Instruct, Q4_K_M\"\n    }\n}\n</code></pre>"},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>InferenceEngine API</li> <li>Model Selection Guide</li> <li>GGUF Format</li> </ul>"},{"location":"api/new-apis/","title":"New APIs (v2.1+)","text":"<p>llcuda v2.1+ introduces four comprehensive API modules for advanced LLM inference optimization.</p>"},{"location":"api/new-apis/#overview","title":"Overview","text":"<p>The new APIs provide:</p> <ol> <li>Quantization - NF4, GGUF conversion, dynamic quantization</li> <li>Unsloth Integration - Seamless fine-tuning to deployment</li> <li>CUDA Optimization - Tensor Cores, CUDA Graphs, Triton kernels</li> <li>Advanced Inference - FlashAttention, KV-cache, batch optimization</li> </ol>"},{"location":"api/new-apis/#quick-examples","title":"Quick Examples","text":""},{"location":"api/new-apis/#quantization","title":"Quantization","text":"<pre><code>from llcuda.quantization import DynamicQuantizer\n\n# Auto-select optimal quantization\nquantizer = DynamicQuantizer(target_vram_gb=12.0)\nconfig = quantizer.recommend_config(model_size_gb=3.0)\n\nprint(f\"Use: {config['quant_type']}\")  # Q4_K_M\n</code></pre>"},{"location":"api/new-apis/#unsloth-integration","title":"Unsloth Integration","text":"<pre><code>from llcuda.unsloth import export_to_llcuda\n\n# Export fine-tuned model\nexport_to_llcuda(\n    model=model,\n    tokenizer=tokenizer,\n    output_path=\"model.gguf\",\n    quant_type=\"Q4_K_M\"\n)\n</code></pre>"},{"location":"api/new-apis/#cuda-optimization","title":"CUDA Optimization","text":"<pre><code>from llcuda.cuda import enable_tensor_cores\n\n# Enable Tensor Cores (2-4x speedup)\nenable_tensor_cores(dtype=torch.float16)\n</code></pre>"},{"location":"api/new-apis/#advanced-inference","title":"Advanced Inference","text":"<pre><code>from llcuda.inference import get_optimal_context_length\n\n# Get optimal context for your VRAM\nctx_len = get_optimal_context_length(\n    model_size_b=3.0,\n    available_vram_gb=12.0,\n    use_flash_attention=True\n)\n</code></pre>"},{"location":"api/new-apis/#detailed-documentation","title":"Detailed Documentation","text":"<p>For complete API reference, see:</p> <ul> <li>Quantization API</li> <li>Unsloth Integration</li> <li>CUDA Optimization</li> <li>Advanced Inference</li> </ul>"},{"location":"api/new-apis/#performance-impact","title":"Performance Impact","text":"Optimization Benefit Tensor Cores 2-4x speedup CUDA Graphs 20-40% latency \u2193 FlashAttention 2-3x for long ctx Q4_K_M Quant 8.5x compression"},{"location":"api/new-apis/#migration-from-v20","title":"Migration from v2.0","text":"<p>No breaking changes! All v2.0 code still works.</p> <p>Before (v2.0): <pre><code>import llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n</code></pre></p> <p>After (v2.1+) - Same code + optional optimizations: <pre><code>import llcuda\nfrom llcuda.cuda import enable_tensor_cores\n\nenable_tensor_cores()  # NEW: 2-4x faster!\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n</code></pre></p>"},{"location":"api/new-apis/#complete-workflow","title":"Complete Workflow","text":"<pre><code>from unsloth import FastLanguageModel\nfrom llcuda.unsloth import export_to_llcuda\nfrom llcuda.cuda import enable_tensor_cores\nimport llcuda\n\n# 1. Train with Unsloth\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"base\")\n# ... training ...\n\n# 2. Export to GGUF\nexport_to_llcuda(model, tokenizer, \"model.gguf\")\n\n# 3. Deploy with optimizations\nenable_tensor_cores()\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n\n# 4. Infer\nresult = engine.infer(\"Hello!\")\nprint(f\"{result.text} ({result.tokens_per_sec:.1f} tok/s)\")\n</code></pre>"},{"location":"api/new-apis/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Quick Start Guide</li> <li>Read the Complete API Reference</li> <li>Explore Examples</li> </ul>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Complete API documentation for llcuda v2.1.0.</p>"},{"location":"api/overview/#main-components","title":"Main Components","text":"<p>llcuda provides a simple, PyTorch-style API for GPU-accelerated LLM inference.</p>"},{"location":"api/overview/#core-classes","title":"Core Classes","text":"Class Purpose Documentation <code>InferenceEngine</code> Main interface for model loading and inference Details <code>InferenceResult</code> Container for inference results with metrics Details"},{"location":"api/overview/#utility-functions","title":"Utility Functions","text":"Function Purpose Documentation <code>check_gpu_compatibility()</code> Verify GPU support Details <code>get_device_properties()</code> Get GPU device information Details"},{"location":"api/overview/#quick-api-reference","title":"Quick API Reference","text":""},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\"What is AI?\", max_tokens=100)\n\n# Access results\nprint(result.text)                    # Generated text\nprint(result.tokens_per_sec)          # Speed in tokens/sec\nprint(result.latency_ms)              # Latency in milliseconds\nprint(result.tokens_generated)        # Number of tokens generated\n</code></pre>"},{"location":"api/overview/#inferenceengine-methods","title":"InferenceEngine Methods","text":""},{"location":"api/overview/#__init__server_urlnone","title":"<code>__init__(server_url=None)</code>","text":"<p>Create a new inference engine instance.</p> <p>Parameters: - <code>server_url</code> (str, optional): Custom llama-server URL. Default: <code>http://127.0.0.1:8090</code></p>"},{"location":"api/overview/#load_modelmodel_path-silentfalse-auto_starttrue-kwargs","title":"<code>load_model(model_path, silent=False, auto_start=True, **kwargs)</code>","text":"<p>Load a GGUF model for inference.</p> <p>Parameters: - <code>model_path</code> (str): Model identifier or path   - HuggingFace: <code>\"unsloth/repo-name:filename.gguf\"</code>   - Registry: <code>\"gemma-3-1b-Q4_K_M\"</code>   - Local: <code>\"/path/to/model.gguf\"</code> - <code>silent</code> (bool): Suppress llama-server output. Default: <code>False</code> - <code>auto_start</code> (bool): Start server automatically. Default: <code>True</code> - <code>**kwargs</code>: Additional options (context_size, gpu_layers, etc.)</p>"},{"location":"api/overview/#inferprompt-max_tokens512-temperature07-kwargs","title":"<code>infer(prompt, max_tokens=512, temperature=0.7, **kwargs)</code>","text":"<p>Run inference on a single prompt.</p> <p>Parameters: - <code>prompt</code> (str): Input text - <code>max_tokens</code> (int): Maximum tokens to generate. Default: 512 - <code>temperature</code> (float): Sampling temperature. Default: 0.7 - <code>top_p</code> (float): Nucleus sampling threshold. Default: 0.9 - <code>top_k</code> (int): Top-k sampling. Default: 40 - <code>stop_sequences</code> (list): Stop generation at these sequences</p> <p>Returns: - <code>InferenceResult</code>: Result object with text and metrics</p>"},{"location":"api/overview/#batch_inferprompts-max_tokens512-kwargs","title":"<code>batch_infer(prompts, max_tokens=512, **kwargs)</code>","text":"<p>Run inference on multiple prompts.</p> <p>Parameters: - <code>prompts</code> (list[str]): List of input texts - <code>max_tokens</code> (int): Maximum tokens per prompt - <code>**kwargs</code>: Same as <code>infer()</code></p> <p>Returns: - <code>list[InferenceResult]</code>: List of results</p>"},{"location":"api/overview/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get aggregated performance metrics.</p> <p>Returns: - <code>dict</code>: Metrics dictionary with throughput and latency stats</p>"},{"location":"api/overview/#inferenceresult-attributes","title":"InferenceResult Attributes","text":"Attribute Type Description <code>text</code> str Generated text <code>tokens_per_sec</code> float Generation speed <code>latency_ms</code> float Total latency in ms <code>tokens_generated</code> int Number of tokens"},{"location":"api/overview/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api/overview/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if current GPU is compatible with llcuda.</p> <p>Returns: <pre><code>{\n    'gpu_name': str,          # e.g., \"Tesla T4\"\n    'compute_capability': str, # e.g., \"7.5\"\n    'compatible': bool,       # True if supported\n    'platform': str          # e.g., \"colab\", \"local\"\n}\n</code></pre></p> <p>Example: <pre><code>compat = llcuda.check_gpu_compatibility()\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} may not work\")\n</code></pre></p>"},{"location":"api/overview/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>InferenceEngine - Complete InferenceEngine documentation</li> <li>Models &amp; GGUF - Model loading and GGUF format</li> <li>GPU &amp; Device - GPU management and compatibility</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"api/overview/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Tutorials</li> <li>Performance Benchmarks</li> </ul>"},{"location":"architecture/gpu0-llm/","title":"GPU 0 - LLM Inference","text":"<p>Configure GPU 0 for llama.cpp server.</p>"},{"location":"architecture/gpu0-llm/#setup","title":"Setup","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,        # All layers on GPU 0\n    flash_attn=True,\n)\n\n# llama-server uses GPU 0 by default\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"architecture/gpu0-llm/#vram-usage","title":"VRAM Usage","text":"Model Quant VRAM on GPU 0 1-3B Q4_K_M 2-4 GB 7B Q4_K_M 5-6 GB 13B Q4_K_M 8-9 GB"},{"location":"architecture/gpu0-llm/#performance","title":"Performance","text":"<ul> <li>FlashAttention: 2-3x speedup</li> <li>Tensor Cores: FP16/TF32 acceleration</li> <li>Context: Up to 8192 tokens</li> </ul>"},{"location":"architecture/gpu1-graphistry/","title":"GPU 1 - Graphistry","text":"<p>Use GPU 1 for RAPIDS + Graphistry visualization.</p>"},{"location":"architecture/gpu1-graphistry/#setup","title":"Setup","text":"<pre><code>import graphistry\nimport cudf\n\n# Configure Graphistry for GPU 1\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Register Graphistry\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=\"YOUR_KEY\"\n)\n</code></pre>"},{"location":"architecture/gpu1-graphistry/#workflow","title":"Workflow","text":"<ol> <li> <p>Extract from LLM (GPU 0)    <pre><code># Get entities from LLM\nentities = llm_client.extract_entities(text)\n</code></pre></p> </li> <li> <p>Build Graph (GPU 1)    <pre><code># Create graph with cuDF\nnodes_df = cudf.DataFrame(entities)\nedges_df = cudf.DataFrame(relationships)\n</code></pre></p> </li> <li> <p>Visualize (GPU 1)    <pre><code># Render with Graphistry\ng = graphistry.edges(edges_df).nodes(nodes_df)\ng.plot()\n</code></pre></p> </li> </ol>"},{"location":"architecture/gpu1-graphistry/#vram-usage","title":"VRAM Usage","text":"<ul> <li>cuDF: 1-3 GB</li> <li>cuGraph: 2-5 GB</li> <li>Graphistry: 1-4 GB</li> <li>Total: 4-12 GB on GPU 1</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>llcuda v2.2.0 architecture for Kaggle dual T4.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         llcuda v2.2.0 Stack            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Python API (llcuda.api.*)              \u2502\n\u2502  \u251c\u2500 client.py (OpenAI-compatible)      \u2502\n\u2502  \u251c\u2500 multigpu.py (Dual T4 config)       \u2502\n\u2502  \u251c\u2500 gguf.py (Quantization tools)       \u2502\n\u2502  \u2514\u2500 nccl.py (PyTorch distributed)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Server Manager (llcuda.server)         \u2502\n\u2502  \u2514\u2500 Lifecycle management                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  llama.cpp Server (C++/CUDA)            \u2502\n\u2502  \u251c\u2500 Build 7760 (commit 388ce82)        \u2502\n\u2502  \u251c\u2500 OpenAI API endpoints                \u2502\n\u2502  \u2514\u2500 Native CUDA tensor-split            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  CUDA 12.5 / cuBLAS                     \u2502\n\u2502  \u251c\u2500 FlashAttention kernels              \u2502\n\u2502  \u251c\u2500 Tensor Core optimization            \u2502\n\u2502  \u2514\u2500 SM 7.5 (Turing)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Kaggle Dual T4 (30GB VRAM)             \u2502\n\u2502  \u251c\u2500 GPU 0: LLM Inference                \u2502\n\u2502  \u2514\u2500 GPU 1: Graphistry/RAPIDS            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":"<ul> <li>Python APIs: High-level interfaces</li> <li>Server Manager: Process lifecycle  </li> <li>llama.cpp: CUDA inference engine</li> <li>Split-GPU: Dual GPU coordination</li> </ul> <p>See: - Split-GPU Design - GPU0 - LLM - GPU1 - Graphistry</p>"},{"location":"architecture/split-gpu/","title":"Split-GPU Architecture","text":"<p>Run LLM on GPU 0 + Graphistry on GPU 1.</p>"},{"location":"architecture/split-gpu/#architecture","title":"Architecture","text":"<pre><code>GPU 0 (T4 - 15GB)          GPU 1 (T4 - 15GB)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  llama-server    \u2502      \u2502  RAPIDS cuDF     \u2502\n\u2502  GGUF Model      \u2502 \u2500\u2500\u2500\u2500&gt;\u2502  cuGraph         \u2502\n\u2502  LLM Inference   \u2502      \u2502  Graphistry[ai]  \u2502\n\u2502  ~5-12GB VRAM    \u2502      \u2502  Network Viz     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/split-gpu/#configuration","title":"Configuration","text":"<pre><code>from llcuda.graphistry import SplitGPUConfig\nimport os\n\n# Assign GPUs\nconfig = SplitGPUConfig(\n    llm_gpu=0,      # GPU 0 for LLM\n    graph_gpu=1     # GPU 1 for Graphistry\n)\n\n# Set CUDA device for llama-server\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Start llama-server on GPU 0\n# (Graphistry will use GPU 1)\n</code></pre>"},{"location":"architecture/split-gpu/#use-cases","title":"Use Cases","text":"<ol> <li>Knowledge Graph Extraction</li> <li>LLM generates entities/relationships</li> <li> <p>Graphistry visualizes graphs</p> </li> <li> <p>Interactive Analysis</p> </li> <li>LLM answers questions</li> <li> <p>Graphistry shows data patterns</p> </li> <li> <p>Multi-Modal Workflows</p> </li> <li>Text generation (GPU 0)</li> <li>Graph analytics (GPU 1)</li> </ol>"},{"location":"architecture/tensor-split-vs-nccl/","title":"Tensor Split vs NCCL","text":"<p>Understanding multi-GPU approaches.</p>"},{"location":"architecture/tensor-split-vs-nccl/#tensor-split-llamacpp","title":"Tensor Split (llama.cpp)","text":"<p>What: Native CUDA layer distribution Used by: llama.cpp server Purpose: Inference parallelism</p> <pre><code># llama.cpp uses tensor-split\nconfig = ServerConfig(\n    tensor_split=\"0.5,0.5\",\n    split_mode=\"layer\",\n)\n</code></pre>"},{"location":"architecture/tensor-split-vs-nccl/#nccl-pytorch","title":"NCCL (PyTorch)","text":"<p>What: Multi-GPU communication primitives Used by: PyTorch distributed training Purpose: Training parallelism</p> <pre><code># PyTorch uses NCCL\nimport torch.distributed as dist\ndist.init_process_group(backend=\"nccl\")\n</code></pre>"},{"location":"architecture/tensor-split-vs-nccl/#key-differences","title":"Key Differences","text":"Feature Tensor Split NCCL Purpose Inference Training Backend llama.cpp PyTorch Communication Direct CUDA Collectives Use Case Model too large Distributed training"},{"location":"architecture/tensor-split-vs-nccl/#when-to-use-each","title":"When to Use Each","text":"<ul> <li>Tensor Split: Run 70B model on dual T4</li> <li>NCCL: Train model with DDP</li> </ul> <p>See: Tutorial 08 - NCCL</p>"},{"location":"gguf/k-quants/","title":"K-Quants Guide","text":"<p>Understanding K-quantization formats.</p>"},{"location":"gguf/k-quants/#what-are-k-quants","title":"What are K-Quants?","text":"<p>K-quants use \"double quantization\": 1. Quantize weights 2. Quantize the quantization parameters</p> <p>Result: Better quality-to-size ratio</p>"},{"location":"gguf/k-quants/#available-k-quants","title":"Available K-Quants","text":""},{"location":"gguf/k-quants/#q4_k_m-recommended","title":"Q4_K_M (Recommended)","text":"<ul> <li>Bits per weight: 4.8</li> <li>Size: ~40% of FP16</li> <li>Quality: Excellent for most uses</li> <li>Use case: Default choice</li> </ul>"},{"location":"gguf/k-quants/#q5_k_m","title":"Q5_K_M","text":"<ul> <li>Bits per weight: 5.7</li> <li>Size: ~48% of FP16</li> <li>Quality: Higher than Q4_K_M</li> <li>Use case: Quality-sensitive</li> </ul>"},{"location":"gguf/k-quants/#q6_k","title":"Q6_K","text":"<ul> <li>Bits per weight: 6.6</li> <li>Size: ~55% of FP16</li> <li>Quality: Near FP16</li> <li>Use case: Maximum quality</li> </ul>"},{"location":"gguf/k-quants/#q8_0","title":"Q8_0","text":"<ul> <li>Bits per weight: 8.5</li> <li>Size: ~70% of FP16</li> <li>Quality: Virtually lossless</li> <li>Use case: Research, validation</li> </ul>"},{"location":"gguf/k-quants/#size-comparison","title":"Size Comparison","text":"Model FP16 Q4_K_M Q5_K_M Q6_K Q8_0 1B 2GB 0.8GB 1GB 1.1GB 1.4GB 7B 14GB 5.6GB 6.7GB 7.7GB 9.8GB 13B 26GB 10.4GB 12.5GB 14.3GB 18.2GB 70B 140GB 56GB 67GB 77GB 98GB"},{"location":"gguf/overview/","title":"GGUF Format Overview","text":"<p>Understanding GGUF quantization in llcuda.</p>"},{"location":"gguf/overview/#what-is-gguf","title":"What is GGUF?","text":"<p>GGUF (GPT-Generated Unified Format): - Binary model format - Efficient quantization - Fast loading - llama.cpp native format</p>"},{"location":"gguf/overview/#quantization-types","title":"Quantization Types","text":""},{"location":"gguf/overview/#k-quants-recommended","title":"K-Quants (Recommended)","text":"<ul> <li>Q4_K_M: 4.8 bpw, best for most models</li> <li>Q5_K_M: 5.7 bpw, higher quality</li> <li>Q6_K: 6.6 bpw, near FP16</li> <li>Q8_0: 8.5 bpw, very high quality</li> </ul>"},{"location":"gguf/overview/#i-quants-compression","title":"I-Quants (Compression)","text":"<ul> <li>IQ3_XS: 3.3 bpw, for 70B models</li> <li>IQ4_XS: 4.3 bpw, better quality</li> <li>IQ2_XS: 2.3 bpw, extreme compression</li> </ul>"},{"location":"gguf/overview/#legacy","title":"Legacy","text":"<ul> <li>Q4_0: 4.5 bpw</li> <li>Q5_0: 5.5 bpw</li> </ul>"},{"location":"gguf/overview/#selection-guide","title":"Selection Guide","text":"VRAM Model Size Recommended Quant 5GB 1-3B Q4_K_M 10GB 7-8B Q4_K_M 15GB 13B Q4_K_M 30GB 70B IQ3_XS <p>See: - K-Quants Guide - I-Quants Guide - Selection Guide</p>"},{"location":"graphistry/knowledge-graphs/","title":"Knowledge Graph Extraction","text":"<p>Extract knowledge graphs from LLM outputs.</p>"},{"location":"graphistry/knowledge-graphs/#workflow","title":"Workflow","text":""},{"location":"graphistry/knowledge-graphs/#1-generate-text-gpu-0","title":"1. Generate Text (GPU 0)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract entities and relationships from: ...\"\n    }]\n)\n\ntext = response.choices[0].message.content\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#2-parse-entities","title":"2. Parse Entities","text":"<pre><code>import json\n\n# Parse LLM output\ndata = json.loads(text)\nentities = data['entities']\nrelationships = data['relationships']\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#3-build-graph-gpu-1","title":"3. Build Graph (GPU 1)","text":"<pre><code>import cudf\n\nnodes_df = cudf.DataFrame(entities)\nedges_df = cudf.DataFrame(relationships)\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#4-visualize-gpu-1","title":"4. Visualize (GPU 1)","text":"<pre><code>import graphistry\n\ng = graphistry.edges(edges_df).nodes(nodes_df)\ng.plot()\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#use-cases","title":"Use Cases","text":"<ul> <li>Document analysis</li> <li>Semantic networks</li> <li>Entity relationship mapping</li> <li>Knowledge base visualization</li> </ul>"},{"location":"graphistry/overview/","title":"Graphistry Integration","text":"<p>GPU-accelerated graph visualization with llcuda.</p>"},{"location":"graphistry/overview/#what-is-graphistry","title":"What is Graphistry?","text":"<p>PyGraphistry provides: - GPU-accelerated graph rendering - Millions of nodes/edges - Interactive exploration - RAPIDS integration (cuDF, cuGraph)</p>"},{"location":"graphistry/overview/#split-gpu-architecture","title":"Split-GPU Architecture","text":"<pre><code>GPU 0: llcuda (LLM)  \u2192  GPU 1: Graphistry (Viz)\n</code></pre>"},{"location":"graphistry/overview/#quick-start","title":"Quick Start","text":"<pre><code>import graphistry\nimport cudf\n\n# Configure for GPU 1\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Register Graphistry\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=\"YOUR_KEY\"\n)\n\n# Create graph\nnodes = cudf.DataFrame({\n    'id': [1, 2, 3],\n    'label': ['A', 'B', 'C']\n})\n\nedges = cudf.DataFrame({\n    'src': [1, 2],\n    'dst': [2, 3]\n})\n\n# Visualize\ng = graphistry.edges(edges, 'src', 'dst').nodes(nodes, 'id')\ng.plot()\n</code></pre> <p>See: - Knowledge Graphs - RAPIDS Integration - Examples</p>"},{"location":"guides/faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about llcuda v2.1.0.</p>"},{"location":"guides/faq/#general-questions","title":"General Questions","text":""},{"location":"guides/faq/#what-is-llcuda","title":"What is llcuda?","text":"<p>llcuda is a Python library for fast LLM inference on NVIDIA GPUs, specifically optimized for Tesla T4. It provides:</p> <ul> <li>Pre-built CUDA binaries with FlashAttention</li> <li>One-step installation from GitHub</li> <li>134 tokens/sec on Gemma 3-1B (verified)</li> <li>Simple Python API for inference</li> <li>Auto-downloading of models and binaries</li> </ul>"},{"location":"guides/faq/#why-tesla-t4-only","title":"Why Tesla T4 only?","text":"<p>llcuda v2.1.0 is optimized exclusively for Tesla T4 (compute capability 7.5) to maximize performance:</p> <ul> <li>Tensor Core optimizations for SM 7.5</li> <li>FlashAttention tuned for Turing architecture</li> <li>Binary size reduction (266 MB vs 500+ MB for multi-GPU)</li> <li>Guaranteed compatibility</li> </ul> <p>For other GPUs, use llcuda v1.2.2 which supports SM 5.0-8.9.</p>"},{"location":"guides/faq/#how-does-llcuda-compare-to-other-solutions","title":"How does llcuda compare to other solutions?","text":"Solution Speed (Gemma 3-1B) Setup Ease of Use llcuda v2.1.0 134 tok/s 1 min Excellent transformers 45 tok/s 5 min Good vLLM 85 tok/s 10 min Moderate llama.cpp CLI 128 tok/s 15 min Moderate <p>llcuda is 3x faster than PyTorch and easiest to set up.</p>"},{"location":"guides/faq/#installation","title":"Installation","text":""},{"location":"guides/faq/#how-do-i-install-llcuda","title":"How do I install llcuda?","text":"<pre><code>pip install git+https://github.com/llcuda/llcuda.git\n</code></pre> <p>Binaries auto-download on first import (~266 MB).</p>"},{"location":"guides/faq/#do-i-need-to-install-cuda-toolkit","title":"Do I need to install CUDA Toolkit?","text":"<p>No! llcuda includes all necessary CUDA binaries. You only need:</p> <ul> <li>NVIDIA driver (pre-installed in Google Colab)</li> <li>CUDA runtime (pre-installed in Colab)</li> <li>Python 3.11+</li> </ul>"},{"location":"guides/faq/#can-i-install-from-pypi","title":"Can I install from PyPI?","text":"<p>llcuda v2.1.0 is GitHub-only for now. Use: <pre><code>pip install git+https://github.com/llcuda/llcuda.git\n</code></pre></p>"},{"location":"guides/faq/#why-do-binaries-download-on-first-import","title":"Why do binaries download on first import?","text":"<p>To keep the pip package small (~62 KB), CUDA binaries (266 MB) download automatically on first import from GitHub Releases. This is a one-time download, then cached locally.</p>"},{"location":"guides/faq/#compatibility","title":"Compatibility","text":""},{"location":"guides/faq/#which-gpus-are-supported","title":"Which GPUs are supported?","text":"<p>llcuda v2.1.0: Tesla T4 only (SM 7.5)</p> <p>llcuda v1.2.2: All GPUs with SM 5.0+ (Maxwell through Ada Lovelace)</p>"},{"location":"guides/faq/#can-i-use-llcuda-on-cpu","title":"Can I use llcuda on CPU?","text":"<p>Yes, but not recommended. Set <code>gpu_layers=0</code> for CPU mode. Performance drops from 134 tok/s to ~8 tok/s.</p>"},{"location":"guides/faq/#does-llcuda-work-on-windows","title":"Does llcuda work on Windows?","text":"<p>llcuda v2.1.0 is Linux-only (Google Colab, Ubuntu). For Windows, compile from source or use WSL2.</p>"},{"location":"guides/faq/#what-python-versions-are-supported","title":"What Python versions are supported?","text":"<p>Python 3.11+ is required. Tested on Python 3.10, 3.11, and 3.12.</p>"},{"location":"guides/faq/#what-cuda-versions-are-supported","title":"What CUDA versions are supported?","text":"<p>CUDA 12.0+ required. Tested with CUDA 12.2, 12.4.</p>"},{"location":"guides/faq/#models","title":"Models","text":""},{"location":"guides/faq/#which-models-can-i-use","title":"Which models can I use?","text":"<p>Any GGUF model compatible with llama.cpp:</p> <ul> <li>Gemma (1B, 2B, 3B, 7B)</li> <li>Llama (3.1, 3.2, 3.3)</li> <li>Qwen (1.5B, 7B, 14B)</li> <li>Mistral (7B, 8x7B)</li> <li>Phi (2, 3)</li> </ul>"},{"location":"guides/faq/#what-quantization-should-i-use","title":"What quantization should I use?","text":"<p>Q4_K_M for best performance/quality balance on T4:</p> <ul> <li>Speed: 134 tok/s</li> <li>VRAM: 1.2 GB (Gemma 3-1B)</li> <li>Quality: &lt; 1% degradation</li> </ul> <p>Other options: - Q5_K_M: Better quality, 18% slower - Q8_0: Best quality, 44% slower</p>"},{"location":"guides/faq/#how-do-i-load-a-model-from-huggingface","title":"How do I load a model from HuggingFace?","text":"<pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"guides/faq/#can-i-use-my-fine-tuned-models","title":"Can I use my fine-tuned models?","text":"<p>Yes! Export to GGUF using Unsloth:</p> <pre><code># After fine-tuning with Unsloth\nmodel.save_pretrained_gguf(\n    \"my-model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# Load with llcuda\nengine.load_model(\"my-model-Q4_K_M.gguf\")\n</code></pre> <p>See Unsloth Integration for details.</p>"},{"location":"guides/faq/#performance","title":"Performance","text":""},{"location":"guides/faq/#what-performance-can-i-expect","title":"What performance can I expect?","text":"<p>On Tesla T4 with Q4_K_M quantization:</p> <ul> <li>Gemma 3-1B: 134 tok/s (verified)</li> <li>Llama 3.2-3B: ~48 tok/s (estimated)</li> <li>Qwen 2.5-7B: ~21 tok/s (estimated)</li> <li>Llama 3.1-8B: ~19 tok/s (estimated)</li> </ul>"},{"location":"guides/faq/#why-is-my-inference-slow","title":"Why is my inference slow?","text":"<p>Common causes:</p> <ol> <li>Not using T4: Other GPUs need v1.2.2</li> <li>Low GPU offload: Set <code>gpu_layers=99</code></li> <li>Wrong quantization: Use Q4_K_M</li> <li>Large context: Reduce <code>ctx_size</code> to 2048</li> <li>CPU mode: Check <code>nvidia-smi</code> shows GPU usage</li> </ol> <p>See Troubleshooting for solutions.</p>"},{"location":"guides/faq/#how-can-i-optimize-performance","title":"How can I optimize performance?","text":"<pre><code># Optimal configuration for T4\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,        # Full GPU offload\n    ctx_size=2048,        # Balanced context\n    batch_size=512,       # Optimal batch\n    ubatch_size=128,\n    auto_configure=True   # Let llcuda optimize\n)\n</code></pre> <p>See Performance Tutorial for details.</p>"},{"location":"guides/faq/#does-llcuda-support-batching","title":"Does llcuda support batching?","text":"<p>Yes: <pre><code>prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = engine.batch_infer(prompts, max_tokens=100)\n</code></pre></p> <p>For concurrent requests, use <code>n_parallel</code>: <pre><code>engine.load_model(\"model.gguf\", n_parallel=4)\n</code></pre></p>"},{"location":"guides/faq/#memory","title":"Memory","text":""},{"location":"guides/faq/#how-much-vram-do-i-need","title":"How much VRAM do I need?","text":"<p>Depends on model size and quantization:</p> Model Q4_K_M Q5_K_M Q8_0 1B 1.2 GB 1.5 GB 2.5 GB 3B 2.0 GB 2.4 GB 4.2 GB 7B 5.0 GB 6.0 GB 10 GB 8B 5.5 GB 6.5 GB 11 GB <p>Tesla T4 has 15 GB, sufficient for models up to 7-8B.</p>"},{"location":"guides/faq/#can-i-run-multiple-models-simultaneously","title":"Can I run multiple models simultaneously?","text":"<p>Yes, on different ports:</p> <pre><code># Model 1\nengine1 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nengine1.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Model 2\nengine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\nengine2.load_model(\"llama-3.2-3b-Q4_K_M\")\n</code></pre> <p>Watch total VRAM usage with <code>nvidia-smi</code>.</p>"},{"location":"guides/faq/#what-if-i-run-out-of-vram","title":"What if I run out of VRAM?","text":"<ol> <li>Use smaller model (1B instead of 3B)</li> <li>Use Q4_K_M instead of Q8_0</li> <li>Reduce <code>gpu_layers</code> (e.g., 20 instead of 99)</li> <li>Reduce <code>ctx_size</code> (e.g., 1024 instead of 4096)</li> <li>Close other GPU applications</li> </ol>"},{"location":"guides/faq/#usage","title":"Usage","text":""},{"location":"guides/faq/#how-do-i-run-inference","title":"How do I run inference?","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\nresult = engine.infer(\"What is AI?\", max_tokens=100)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/faq/#can-i-stream-outputs","title":"Can I stream outputs?","text":"<p>Yes: <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    \"Write a story:\",\n    callback=print_chunk,\n    max_tokens=200\n)\n</code></pre></p>"},{"location":"guides/faq/#how-do-i-stop-generation-early","title":"How do I stop generation early?","text":"<p>Use <code>stop_sequences</code>: <pre><code>result = engine.infer(\n    \"List items:\",\n    max_tokens=200,\n    stop_sequences=[\"\\n\\n\", \"###\"]\n)\n</code></pre></p>"},{"location":"guides/faq/#can-i-control-randomness","title":"Can I control randomness?","text":"<p>Yes, with <code>temperature</code> and <code>seed</code>: <pre><code># Deterministic\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.1,\n    seed=42\n)\n\n# Creative\nresult = engine.infer(\n    \"Prompt\",\n    temperature=1.0,\n    top_k=100\n)\n</code></pre></p>"},{"location":"guides/faq/#google-colab","title":"Google Colab","text":""},{"location":"guides/faq/#does-llcuda-work-in-google-colab","title":"Does llcuda work in Google Colab?","text":"<p>Yes! llcuda is optimized for Colab T4:</p> <pre><code># In Colab\n!pip install git+https://github.com/llcuda/llcuda.git\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n</code></pre>"},{"location":"guides/faq/#how-do-i-get-t4-in-colab","title":"How do I get T4 in Colab?","text":"<p>Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU &gt; GPU type &gt; T4</p>"},{"location":"guides/faq/#do-i-need-colab-pro","title":"Do I need Colab Pro?","text":"<p>No, but Colab Pro provides:</p> <ul> <li>Guaranteed T4 access</li> <li>Longer runtime (24h vs 12h)</li> <li>More RAM</li> <li>Priority execution</li> </ul> <p>Free tier works but T4 availability varies.</p>"},{"location":"guides/faq/#can-i-save-models-between-sessions","title":"Can I save models between sessions?","text":"<p>Models cache to <code>~/.cache/llcuda/</code>. In Colab, this resets. Use:</p> <pre><code># Save to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy model\n!cp ~/.cache/llcuda/models/gemma-3-1b*.gguf /content/drive/MyDrive/\n\n# Next session: load from Drive\nengine.load_model(\"/content/drive/MyDrive/gemma-3-1b-Q4_K_M.gguf\")\n</code></pre>"},{"location":"guides/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/faq/#import-fails-with-no-module-named-llcuda","title":"Import fails with \"No module named llcuda\"","text":"<pre><code># Reinstall\npip uninstall llcuda -y\npip install git+https://github.com/llcuda/llcuda.git\n</code></pre>"},{"location":"guides/faq/#binary-download-fails","title":"Binary download fails","text":"<p>See Troubleshooting Guide</p>"},{"location":"guides/faq/#server-wont-start","title":"Server won't start","text":"<p>Check port 8090 availability or use different port: <pre><code>engine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/faq/#performance-is-slow","title":"Performance is slow","text":"<p>See Performance Troubleshooting</p>"},{"location":"guides/faq/#contributing","title":"Contributing","text":""},{"location":"guides/faq/#can-i-contribute-to-llcuda","title":"Can I contribute to llcuda?","text":"<p>Yes! Contributions welcome:</p> <ul> <li>Bug reports: GitHub Issues</li> <li>Feature requests: Open an issue</li> <li>Code: Fork and submit PR</li> <li>Documentation: Help improve docs</li> </ul>"},{"location":"guides/faq/#how-do-i-build-binaries","title":"How do I build binaries?","text":"<p>See Build Binaries Tutorial</p>"},{"location":"guides/faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<p>Open a GitHub Issue with:</p> <ul> <li>llcuda version</li> <li>GPU model</li> <li>CUDA version</li> <li>Python version</li> <li>Error message</li> <li>Minimal reproducible code</li> </ul>"},{"location":"guides/faq/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start</li> <li>First Steps</li> <li>Troubleshooting</li> <li>Performance Optimization</li> <li>API Reference</li> </ul>"},{"location":"guides/faq/#still-have-questions","title":"Still have questions?","text":"<p>Ask on GitHub Discussions or open an issue.</p>"},{"location":"guides/first-steps/","title":"First Steps","text":"<p>Your first steps with llcuda v2.2.0 on Kaggle.</p>"},{"location":"guides/first-steps/#1-load-a-model","title":"1. Load a Model","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Basic configuration\nconfig = ServerConfig(\n    model_path=\"/path/to/model.gguf\",\n    n_gpu_layers=99,  # Offload all to GPU\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"guides/first-steps/#2-make-your-first-request","title":"2. Make Your First Request","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n    ],\n    max_tokens=200\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"guides/first-steps/#3-explore-notebooks","title":"3. Explore Notebooks","text":"<p>Try the tutorial notebooks: - 01 - Quick Start - 02 - Server Setup - 03 - Multi-GPU</p>"},{"location":"guides/gguf-format/","title":"GGUF Format Guide","text":"<p>Complete guide to the GGUF (GPT-Generated Unified Format) model format used by llcuda v2.1.0.</p>"},{"location":"guides/gguf-format/#what-is-gguf","title":"What is GGUF?","text":"<p>GGUF (GPT-Generated Unified Format) is a binary format for storing large language models developed by the llama.cpp project.</p>"},{"location":"guides/gguf-format/#key-features","title":"Key Features","text":"<p>\u2705 Single-file distribution - Everything in one portable file \u2705 Efficient storage - Compact binary format with compression \u2705 Memory mapping - Fast loading without full RAM allocation \u2705 Quantization support - Multiple precision levels (INT4, INT8, FP16) \u2705 Metadata included - Model architecture, tokenizer, and configuration \u2705 Cross-platform - Works on Linux, macOS, Windows \u2705 GPU acceleration - Full CUDA support for inference</p>"},{"location":"guides/gguf-format/#why-gguf","title":"Why GGUF?","text":"<p>GGUF replaced the older GGML format and offers significant improvements:</p> Feature GGML (Old) GGUF (Current) Metadata External files Embedded Versioning Limited Full versioning Tokenizer Separate file Included Architecture Hard-coded Dynamic Compatibility Breaking changes Forward compatible"},{"location":"guides/gguf-format/#gguf-file-structure","title":"GGUF File Structure","text":"<p>A GGUF file contains:</p> <pre><code>GGUF File (.gguf)\n\u251c\u2500\u2500 Header (magic number, version)\n\u251c\u2500\u2500 Metadata (KV pairs)\n\u2502   \u251c\u2500\u2500 Architecture (llama, gemma, qwen, etc.)\n\u2502   \u251c\u2500\u2500 Model parameters (layers, heads, etc.)\n\u2502   \u251c\u2500\u2500 Tokenizer (vocabulary, special tokens)\n\u2502   \u251c\u2500\u2500 Quantization method\n\u2502   \u2514\u2500\u2500 Author, license, source\n\u251c\u2500\u2500 Tensor info (names, shapes, offsets)\n\u2514\u2500\u2500 Tensor data (model weights)\n</code></pre>"},{"location":"guides/gguf-format/#example-gguf-metadata","title":"Example GGUF Metadata","text":"<pre><code>import llcuda\nfrom llcuda.gguf_parser import GGUFReader\n\n# Read GGUF metadata\nreader = GGUFReader(\"gemma-3-1b-it-Q4_K_M.gguf\")\n\nprint(f\"Architecture: {reader.architecture}\")\nprint(f\"Parameter count: {reader.parameter_count}\")\nprint(f\"Quantization: {reader.quantization}\")\nprint(f\"Context length: {reader.context_length}\")\n</code></pre>"},{"location":"guides/gguf-format/#quantization-types","title":"Quantization Types","text":"<p>GGUF supports multiple quantization methods that trade off quality for size and speed.</p>"},{"location":"guides/gguf-format/#quantization-comparison","title":"Quantization Comparison","text":"Type Bits Size Multiplier Quality Speed Use Case F16 16 1.0x (largest) Best Slowest Reference quality Q8_0 8 0.5x Excellent Slow High quality needed Q6_K 6 0.4x Very good Medium Balanced Q5_K_M 5 0.35x Good Medium-fast Good balance Q4_K_M 4 0.25x Good Fast Recommended Q4_K_S 4 0.25x Acceptable Fast Smaller variant Q3_K_M 3 0.2x Fair Very fast Experimental Q2_K 2 0.15x (smallest) Poor Fastest Testing only"},{"location":"guides/gguf-format/#recommended-q4_k_m","title":"Recommended: Q4_K_M","text":"<p>For Tesla T4 GPUs, Q4_K_M provides the best balance:</p> <p>\u2705 Good quality - Minimal accuracy loss vs FP16 \u2705 Fast inference - 134 tok/s on Gemma 3-1B \u2705 Small size - 4 bits per parameter \u2705 Low VRAM - Fits larger models in 16 GB</p> <p>Example sizes for Gemma 3-1B: - F16: ~2.6 GB - Q8_0: ~1.4 GB - Q4_K_M: ~650 MB \u2190 Recommended - Q2_K: ~400 MB</p>"},{"location":"guides/gguf-format/#using-gguf-models-with-llcuda","title":"Using GGUF Models with llcuda","text":""},{"location":"guides/gguf-format/#method-1-from-huggingface-recommended","title":"Method 1: From HuggingFace (Recommended)","text":"<p>Load directly from Unsloth or other HuggingFace repositories:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Load from Unsloth repository\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Format: repo_id:filename\n</code></pre> <p>Popular Unsloth GGUF models: - <code>unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf</code></p>"},{"location":"guides/gguf-format/#method-2-from-local-file","title":"Method 2: From Local File","text":"<p>Use a downloaded GGUF file:</p> <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"guides/gguf-format/#method-3-from-url","title":"Method 3: From URL","text":"<p>Direct download from any URL:</p> <pre><code>engine.load_model(\n    \"https://huggingface.co/user/repo/resolve/main/model.gguf\"\n)\n</code></pre>"},{"location":"guides/gguf-format/#converting-models-to-gguf","title":"Converting Models to GGUF","text":""},{"location":"guides/gguf-format/#from-pytorchhuggingface","title":"From PyTorch/HuggingFace","text":"<p>Use the <code>convert_hf_to_gguf.py</code> script from llama.cpp:</p> <pre><code># Clone llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n\n# Install dependencies\npip install -r requirements.txt\n\n# Convert model\npython convert_hf_to_gguf.py \\\n    /path/to/huggingface/model \\\n    --outfile model-f16.gguf \\\n    --outtype f16\n</code></pre>"},{"location":"guides/gguf-format/#from-unsloth-fine-tuned-models","title":"From Unsloth Fine-Tuned Models","text":"<p>Export directly from Unsloth:</p> <pre><code>from unsloth import FastLanguageModel\n\n# After fine-tuning\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"  # Creates Q4_K_M GGUF\n)\n\n# Output: my_model/unsloth.Q4_K_M.gguf\n</code></pre> <p>Supported quantization methods: - <code>\"f16\"</code> - Full precision - <code>\"q8_0\"</code> - 8-bit quantization - <code>\"q6_k\"</code> - 6-bit K-quant - <code>\"q5_k_m\"</code> - 5-bit K-quant medium - <code>\"q4_k_m\"</code> - 4-bit K-quant medium (recommended) - <code>\"q4_k_s\"</code> - 4-bit K-quant small - <code>\"q3_k_m\"</code> - 3-bit K-quant medium - <code>\"q2_k\"</code> - 2-bit K-quant</p>"},{"location":"guides/gguf-format/#quantizing-existing-gguf","title":"Quantizing Existing GGUF","text":"<p>Convert between quantization levels:</p> <pre><code># Using llama-quantize (included with llcuda binaries)\n~/.cache/llcuda/bin/llama-quantize \\\n    model-f16.gguf \\\n    model-q4_k_m.gguf \\\n    Q4_K_M\n</code></pre> <p>Available quantization types: <pre><code>Q4_0, Q4_1, Q5_0, Q5_1, Q8_0\nQ4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K\nIQ1_S, IQ2_XXS, IQ2_XS, IQ2_S, IQ3_XXS, IQ3_S\n</code></pre></p>"},{"location":"guides/gguf-format/#gguf-inspection-tools","title":"GGUF Inspection Tools","text":""},{"location":"guides/gguf-format/#using-llcuda","title":"Using llcuda","text":"<pre><code>from llcuda.gguf_parser import GGUFReader\n\nreader = GGUFReader(\"model.gguf\")\n\nprint(f\"Architecture: {reader.architecture}\")\nprint(f\"Quantization: {reader.quantization}\")\nprint(f\"Parameter count: {reader.parameter_count:,}\")\nprint(f\"Context length: {reader.context_length}\")\nprint(f\"Embedding size: {reader.embedding_size}\")\nprint(f\"Layers: {reader.num_layers}\")\nprint(f\"Heads: {reader.num_heads}\")\nprint(f\"File size: {reader.file_size / 1024**3:.2f} GB\")\n</code></pre>"},{"location":"guides/gguf-format/#using-llamacpp-tools","title":"Using llama.cpp Tools","text":"<pre><code># Check GGUF metadata\n~/.cache/llcuda/bin/llama-cli \\\n    --model model.gguf \\\n    --verbose\n</code></pre>"},{"location":"guides/gguf-format/#model-compatibility","title":"Model Compatibility","text":""},{"location":"guides/gguf-format/#supported-architectures","title":"Supported Architectures","text":"<p>llcuda v2.1.0 supports these model architectures via GGUF:</p> <p>\u2705 LLaMA (LLaMA, LLaMA-2, LLaMA-3, LLaMA-3.1, LLaMA-3.2) \u2705 Gemma (Gemma, Gemma-2, Gemma-3) \u2705 Qwen (Qwen, Qwen-2, Qwen-2.5) \u2705 Mistral (Mistral, Mistral-7B) \u2705 Mixtral (Mixtral 8x7B, 8x22B) \u2705 Phi (Phi-2, Phi-3) \u2705 Yi (Yi-6B, Yi-34B) \u2705 StableLM (StableLM-2, StableLM-3)</p>"},{"location":"guides/gguf-format/#checking-compatibility","title":"Checking Compatibility","text":"<pre><code>import llcuda\n\n# Check if model is compatible\ncompat = llcuda.check_model_compatibility(\"model.gguf\")\n\nprint(f\"Compatible: {compat['compatible']}\")\nprint(f\"Architecture: {compat['architecture']}\")\nprint(f\"Warnings: {compat.get('warnings', [])}\")\n</code></pre>"},{"location":"guides/gguf-format/#gguf-best-practices","title":"GGUF Best Practices","text":""},{"location":"guides/gguf-format/#1-choose-right-quantization","title":"1. Choose Right Quantization","text":"<p>For Tesla T4: - Small models (1-3B): Q4_K_M or Q5_K_M - Medium models (7-8B): Q4_K_M (fits in VRAM) - Large models (13B+): Q4_K_M or Q3_K_M (if needed)</p>"},{"location":"guides/gguf-format/#2-verify-gguf-integrity","title":"2. Verify GGUF Integrity","text":"<pre><code>from llcuda.gguf_parser import GGUFReader\n\ntry:\n    reader = GGUFReader(\"model.gguf\")\n    print(\"\u2705 Valid GGUF file\")\nexcept Exception as e:\n    print(f\"\u274c Invalid GGUF: {e}\")\n</code></pre>"},{"location":"guides/gguf-format/#3-test-before-production","title":"3. Test Before Production","text":"<pre><code># Quick test\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", silent=True)\n\nresult = engine.infer(\"Test prompt\", max_tokens=20)\nprint(f\"Output: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/gguf-format/#4-optimize-storage","title":"4. Optimize Storage","text":"<p>Use Q4_K_M for distribution: - Smaller download size - Faster loading - Good quality - Better inference speed</p>"},{"location":"guides/gguf-format/#gguf-vs-other-formats","title":"GGUF vs Other Formats","text":"Format Size Speed Compatibility Ease of Use GGUF Small Fast llama.cpp \u2705 Easy SafeTensors Large Medium PyTorch Medium PyTorch (.pt) Large Medium PyTorch only Medium ONNX Large Fast ONNX Runtime Complex TensorRT Custom Fastest NVIDIA only Complex <p>Why GGUF for llcuda: - \u2705 Smallest file size (with quantization) - \u2705 Fast inference on CPU and GPU - \u2705 Single-file distribution - \u2705 Works with llama.cpp ecosystem - \u2705 Easy to share and deploy</p>"},{"location":"guides/gguf-format/#finding-gguf-models","title":"Finding GGUF Models","text":""},{"location":"guides/gguf-format/#unsloth-huggingface","title":"Unsloth HuggingFace","text":"<p>Most popular source for GGUF models:</p> <p>https://huggingface.co/unsloth</p> <p>Example repositories: - <code>unsloth/gemma-3-1b-it-GGUF</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF</code> - <code>unsloth/Meta-Llama-3.1-8B-Instruct-GGUF</code></p>"},{"location":"guides/gguf-format/#thebloke-legacy","title":"TheBloke (Legacy)","text":"<p>Older GGUF models (pre-Unsloth era):</p> <p>https://huggingface.co/TheBloke</p>"},{"location":"guides/gguf-format/#bartowski","title":"Bartowski","text":"<p>Recent high-quality quantizations:</p> <p>https://huggingface.co/bartowski</p>"},{"location":"guides/gguf-format/#troubleshooting-gguf-issues","title":"Troubleshooting GGUF Issues","text":""},{"location":"guides/gguf-format/#issue-invalid-gguf-magic-number","title":"Issue: Invalid GGUF Magic Number","text":"<p>Error: <code>Invalid GGUF file: wrong magic number</code></p> <p>Solution: - File is corrupted or incomplete - Re-download the GGUF file - Verify SHA256 checksum</p>"},{"location":"guides/gguf-format/#issue-unsupported-quantization","title":"Issue: Unsupported Quantization","text":"<p>Error: <code>Quantization type not supported</code></p> <p>Solution: - Use Q4_K_M, Q5_K_M, or Q8_0 - Avoid experimental quantizations (IQ types) - Re-quantize with llama-quantize</p>"},{"location":"guides/gguf-format/#issue-model-too-large","title":"Issue: Model Too Large","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solution: - Use lower quantization (Q4_K_M instead of Q8_0) - Use smaller model variant - Clear GPU cache before loading</p>"},{"location":"guides/gguf-format/#advanced-gguf-topics","title":"Advanced GGUF Topics","text":""},{"location":"guides/gguf-format/#custom-metadata","title":"Custom Metadata","text":"<p>Add custom metadata to GGUF:</p> <pre><code>from llcuda.gguf_parser import GGUFWriter\n\nwriter = GGUFWriter(\"output.gguf\")\nwriter.add_metadata(\"author\", \"Your Name\")\nwriter.add_metadata(\"description\", \"Fine-tuned for specific task\")\nwriter.add_metadata(\"license\", \"MIT\")\nwriter.finalize()\n</code></pre>"},{"location":"guides/gguf-format/#merging-gguf-models","title":"Merging GGUF Models","text":"<p>Combine multiple LoRA adapters (experimental):</p> <pre><code># Using llama.cpp tools\nllama-export-lora \\\n    base-model.gguf \\\n    lora-adapter.gguf \\\n    merged-model.gguf\n</code></pre>"},{"location":"guides/gguf-format/#references","title":"References","text":"<ul> <li>GGUF Specification: github.com/ggerganov/ggml/blob/master/docs/gguf.md</li> <li>llama.cpp: github.com/ggerganov/llama.cpp</li> <li>Unsloth GGUF Export: docs.unsloth.ai/basics/saving-to-gguf</li> </ul>"},{"location":"guides/gguf-format/#next-steps","title":"Next Steps","text":"<ul> <li> Model Selection Guide - Choose the right model</li> <li> Quick Start - Start using GGUF models</li> <li> Performance - Benchmark GGUF models</li> <li> Unsloth Integration - Create GGUF from fine-tuned models</li> </ul> <p>GGUF makes LLM deployment simple and efficient! \ud83d\ude80</p>"},{"location":"guides/installation/","title":"Installation Guide","text":"<p>Complete installation guide for llcuda v2.2.0 on Kaggle dual T4 GPUs.</p>"},{"location":"guides/installation/#requirements","title":"Requirements","text":""},{"location":"guides/installation/#hardware","title":"Hardware","text":"Component Requirement GPU NVIDIA Tesla T4 (Kaggle 2\u00d7 T4) VRAM 15GB (single T4) or 30GB (dual T4) RAM 16GB+ recommended"},{"location":"guides/installation/#software","title":"Software","text":"Component Requirement Python 3.11 or higher CUDA 12.x runtime OS Linux (Ubuntu 20.04+, Kaggle) pip 23.0+"},{"location":"guides/installation/#kaggle-installation-recommended","title":"Kaggle Installation (Recommended)","text":""},{"location":"guides/installation/#step-1-configure-notebook-settings","title":"Step 1: Configure Notebook Settings","text":"<ol> <li>Go to kaggle.com/code</li> <li>Create new notebook</li> <li>Settings \u2192 Accelerator \u2192 GPU T4 \u00d7 2 \u2705</li> <li>Settings \u2192 Internet \u2192 On \u2705</li> </ol>"},{"location":"guides/installation/#step-2-install-llcuda","title":"Step 2: Install llcuda","text":"<pre><code># Install from GitHub v2.2.0\npip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/installation/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\n# Check version\nprint(f\"llcuda version: {llcuda.__version__}\")  # 2.2.0\n\n# Verify dual T4 setup\ngpus = detect_gpus()\nprint(f\"Detected {len(gpus)} GPUs\")\nprint_gpu_info()\n</code></pre> <p>Expected output: <pre><code>llcuda version: 2.2.0\nDetected 2 GPUs\n\nGPU 0: Tesla T4\n  Memory: 15.0 / 15.0 GB\n  Compute Capability: 7.5\n\nGPU 1: Tesla T4\n  Memory: 15.0 / 15.0 GB\n  Compute Capability: 7.5\n</code></pre></p>"},{"location":"guides/installation/#binary-download","title":"Binary Download","text":"<p>On first import, llcuda automatically downloads CUDA binaries:</p> <ul> <li>Size: 961 MB</li> <li>Source: GitHub Releases v2.2.0</li> <li>SHA256: Automatically verified</li> <li>Cache: <code>~/.cache/llcuda/</code></li> </ul>"},{"location":"guides/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start</li> <li>Kaggle Setup</li> <li>First Steps</li> </ul>"},{"location":"guides/kaggle-setup/","title":"Kaggle Setup Guide","text":"<p>Complete guide for setting up llcuda v2.2.0 on Kaggle with dual T4 GPUs.</p>"},{"location":"guides/kaggle-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kaggle account</li> <li>Phone verification (for GPU access)</li> </ul>"},{"location":"guides/kaggle-setup/#step-1-create-notebook","title":"Step 1: Create Notebook","text":"<ol> <li>Go to kaggle.com/code</li> <li>Click \"New Notebook\"</li> <li>Choose \"Notebook\" type</li> </ol>"},{"location":"guides/kaggle-setup/#step-2-configure-gpu","title":"Step 2: Configure GPU","text":"<ol> <li>Click Settings (gear icon)</li> <li>Accelerator \u2192 Select GPU T4 x 2</li> <li>Internet \u2192 Toggle On</li> <li>Persistence \u2192 Optional: Enable for faster startups</li> </ol>"},{"location":"guides/kaggle-setup/#step-3-install-llcuda","title":"Step 3: Install llcuda","text":"<pre><code>!pip install -q git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/kaggle-setup/#step-4-verify-setup","title":"Step 4: Verify Setup","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\nprint(f\"llcuda v{llcuda.__version__}\")\nprint_gpu_info()\n</code></pre>"},{"location":"guides/kaggle-setup/#step-5-test-inference","title":"Step 5: Test Inference","text":"<p>Run the Quick Start notebook to verify everything works.</p>"},{"location":"guides/kaggle-setup/#kaggle-limits","title":"Kaggle Limits","text":"<ul> <li>Session Duration: 12 hours maximum</li> <li>Disk Space: 73 GB available</li> <li>VRAM: 30 GB total (2\u00d7 15GB T4)</li> <li>Internet: Required for pip installs</li> </ul>"},{"location":"guides/kaggle-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-GPU Guide</li> <li>Tutorial Notebooks</li> </ul>"},{"location":"guides/model-selection/","title":"Model Selection Guide","text":"<p>Choose the right model and quantization for your use case with llcuda v2.1.0.</p>"},{"location":"guides/model-selection/#quick-recommendations","title":"Quick Recommendations","text":""},{"location":"guides/model-selection/#for-tesla-t4-15-gb","title":"For Tesla T4 (15 GB)","text":"Priority Model Quantization Speed VRAM Quality Speed Gemma 3-1B Q4_K_M 134 tok/s 1.2 GB Excellent Balance Llama 3.2-3B Q4_K_M 48 tok/s 2.0 GB Very good Quality Qwen 2.5-7B Q4_K_M 21 tok/s 5.0 GB Excellent"},{"location":"guides/model-selection/#for-limited-vram-8-gb","title":"For Limited VRAM (&lt; 8 GB)","text":"GPU VRAM Recommended Model Quantization Expected Speed 4 GB Gemma 3-1B Q4_0 ~140 tok/s 6 GB Gemma 3-1B Q4_K_M ~134 tok/s 8 GB Llama 3.2-3B Q4_K_M ~48 tok/s"},{"location":"guides/model-selection/#model-size-comparison","title":"Model Size Comparison","text":""},{"location":"guides/model-selection/#performance-vs-quality-trade-off","title":"Performance vs Quality Trade-off","text":"Model Family Size Params Tokens/sec (T4) VRAM Best For Gemma 3 1B 1.2B 134 1.2 GB Interactive apps, chatbots Llama 3.2 3B 3.2B 48 2.0 GB Balanced performance Qwen 2.5 7B 7.6B 21 5.0 GB Quality-focused tasks Llama 3.1 8B 8.0B 19 5.5 GB Production quality Mistral 7B 7.2B 22 5.2 GB Code generation"},{"location":"guides/model-selection/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"guides/model-selection/#1b-models-best-for-speed","title":"1B Models (Best for Speed)","text":"<p>Gemma 3-1B-it</p> <ul> <li>Speed: 134 tok/s (Q4_K_M)</li> <li>VRAM: 1.2 GB</li> <li>Strengths:</li> <li>Fastest inference</li> <li>Excellent for interactive chat</li> <li>Low VRAM requirements</li> <li>Good quality for size</li> <li>Weaknesses:</li> <li>Limited reasoning on complex tasks</li> <li>Shorter context understanding</li> <li>Use Cases:</li> <li>Customer service chatbots</li> <li>Quick Q&amp;A systems</li> <li>Real-time code assistance</li> <li>Mobile/edge deployment</li> </ul> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#3b-models-balanced","title":"3B Models (Balanced)","text":"<p>Llama 3.2-3B-Instruct</p> <ul> <li>Speed: 48 tok/s (Q4_K_M)</li> <li>VRAM: 2.0 GB</li> <li>Strengths:</li> <li>Good balance of speed/quality</li> <li>Better reasoning than 1B</li> <li>Handles complex instructions</li> <li>Still fast enough for real-time</li> <li>Weaknesses:</li> <li>3x slower than 1B models</li> <li>Higher VRAM usage</li> <li>Use Cases:</li> <li>Content generation</li> <li>Code explanation</li> <li>Document summarization</li> <li>Educational applications</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#7b-models-quality-focused","title":"7B Models (Quality-Focused)","text":"<p>Qwen 2.5-7B-Instruct</p> <ul> <li>Speed: 21 tok/s (Q4_K_M)</li> <li>VRAM: 5.0 GB</li> <li>Strengths:</li> <li>Excellent quality</li> <li>Strong reasoning abilities</li> <li>Great for complex tasks</li> <li>Multilingual support</li> <li>Weaknesses:</li> <li>6x slower than 1B</li> <li>Requires 5+ GB VRAM</li> <li>Use Cases:</li> <li>Research and analysis</li> <li>Complex reasoning tasks</li> <li>Technical documentation</li> <li>Multi-step problem solving</li> </ul> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n</code></pre> <p>Llama 3.1-8B-Instruct</p> <ul> <li>Speed: 19 tok/s (Q4_K_M)</li> <li>VRAM: 5.5 GB</li> <li>Strengths:</li> <li>State-of-the-art quality</li> <li>Excellent instruction following</li> <li>Long context support (128K)</li> <li>Multilingual</li> <li>Use Cases:</li> <li>Production applications</li> <li>API services</li> <li>Complex workflows</li> <li>Enterprise deployments</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#quantization-guide","title":"Quantization Guide","text":""},{"location":"guides/model-selection/#understanding-quantization-types","title":"Understanding Quantization Types","text":"Quantization Bits Speed Quality VRAM File Size Recommendation Q2_K 2.5 Fastest 85% Lowest ~30% Prototyping only Q3_K_M 3.5 Very fast 92% Very low ~40% Emergency low VRAM Q4_0 4.0 Fast 97% Low ~45% Speed priority Q4_K_M 4.5 Fast 99% Medium ~50% \u2705 Recommended Q5_K_M 5.5 Moderate 99.5% Medium-high ~60% Quality critical Q6_K 6.5 Slow 99.8% High ~70% Rarely needed Q8_0 8.0 Slower 99.95% Very high ~85% Development only F16 16.0 Slowest 100% Maximum 100% Not recommended"},{"location":"guides/model-selection/#choosing-quantization","title":"Choosing Quantization","text":"<p>For most users: <pre><code># Q4_K_M: Best overall choice\nengine.load_model(\n    \"model-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For speed-critical applications: <pre><code># Q4_0: 3-5% faster, slightly lower quality\nengine.load_model(\n    \"model-Q4_0.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For quality-critical work: <pre><code># Q5_K_M: Better quality, 20% slower\nengine.load_model(\n    \"model-Q5_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For extreme VRAM constraints: <pre><code># Q3_K_M: Smallest usable quantization\nengine.load_model(\n    \"model-Q3_K_M.gguf\",\n    silent=True\n)\n</code></pre></p>"},{"location":"guides/model-selection/#popular-model-collections","title":"Popular Model Collections","text":""},{"location":"guides/model-selection/#unsloth-models-recommended","title":"Unsloth Models (Recommended)","text":"<p>Unsloth provides optimized GGUF models on HuggingFace:</p> <p>Gemma Models: <pre><code># Gemma 3-1B (Best for speed)\n\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n\n# Gemma 2-2B\n\"unsloth/gemma-2-2b-it-GGUF:gemma-2-2b-it-Q4_K_M.gguf\"\n\n# Gemma 2-9B\n\"unsloth/gemma-2-9b-it-GGUF:gemma-2-9b-it-Q4_K_M.gguf\"\n</code></pre></p> <p>Llama Models: <pre><code># Llama 3.2-1B\n\"unsloth/Llama-3.2-1B-Instruct-GGUF:Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.2-3B\n\"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.1-8B\n\"unsloth/Llama-3.1-8B-Instruct-GGUF:Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n</code></pre></p> <p>Mistral Models: <pre><code># Mistral 7B v0.3\n\"unsloth/Mistral-7B-Instruct-v0.3-GGUF:Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n\n# Mistral Nemo 12B\n\"unsloth/Mistral-Nemo-Instruct-2407-GGUF:Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#official-huggingface-models","title":"Official HuggingFace Models","text":"<p>Qwen Models: <pre><code># Qwen 2.5-7B (Excellent quality)\n\"Qwen/Qwen2.5-7B-Instruct-GGUF:qwen2.5-7b-instruct-q4_k_m.gguf\"\n\n# Qwen 2.5-14B\n\"Qwen/Qwen2.5-14B-Instruct-GGUF:qwen2.5-14b-instruct-q4_k_m.gguf\"\n</code></pre></p> <p>Phi Models: <pre><code># Phi 3.5-Mini (3.8B)\n\"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#vram-requirements","title":"VRAM Requirements","text":""},{"location":"guides/model-selection/#model-size-to-vram-mapping","title":"Model Size to VRAM Mapping","text":"<p>For Q4_K_M quantization:</p> Model Size Q4_K_M VRAM Q5_K_M VRAM Q8_0 VRAM ctx=2048 1B 1.2 GB 1.5 GB 2.5 GB Add +0.3 GB 3B 2.0 GB 2.4 GB 4.2 GB Add +0.3 GB 7B 5.0 GB 6.2 GB 9.5 GB Add +0.5 GB 8B 5.5 GB 6.8 GB 10.2 GB Add +0.5 GB 13B 9.0 GB 11.0 GB 16.5 GB Add +0.8 GB"},{"location":"guides/model-selection/#gpu-recommendations","title":"GPU Recommendations","text":"GPU VRAM Max Model (Q4_K_M) Recommended Model Tesla T4 15 GB 7B 1B (speed) or 7B (quality) RTX 3060 12 GB 7B 3B RTX 3070 8 GB 3B 1B RTX 3080 10 GB 7B 3B RTX 3090 24 GB 13B 7B RTX 4070 12 GB 7B 3B RTX 4090 24 GB 13B 7B or 13B A100 40 GB 30B 13B A100 80 GB 70B 30B"},{"location":"guides/model-selection/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"guides/model-selection/#interactive-chatbots","title":"Interactive Chatbots","text":"<p>Priority: Speed, low latency</p> <p>Recommended: - Gemma 3-1B Q4_K_M (134 tok/s) - Llama 3.2-1B Q4_K_M (140 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#code-generation","title":"Code Generation","text":"<p>Priority: Accuracy, context understanding</p> <p>Recommended: - Qwen 2.5-7B Q4_K_M (21 tok/s) - Llama 3.1-8B Q4_K_M (19 tok/s)</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    ctx_size=4096,  # Longer context for code\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#document-summarization","title":"Document Summarization","text":"<p>Priority: Context length, quality</p> <p>Recommended: - Llama 3.1-8B Q4_K_M (128K context) - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=8192,  # Long documents\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#question-answering","title":"Question Answering","text":"<p>Priority: Accuracy, speed</p> <p>Recommended: - Llama 3.2-3B Q4_K_M (48 tok/s) - Gemma 3-1B Q4_K_M (134 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#content-generation","title":"Content Generation","text":"<p>Priority: Creativity, quality</p> <p>Recommended: - Qwen 2.5-7B Q5_K_M - Llama 3.1-8B Q5_K_M</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q5_K_M\",\n    ctx_size=4096,\n    silent=True\n)\n\n# Use creative generation settings\nresult = engine.infer(\n    prompt,\n    temperature=1.0,\n    top_p=0.95,\n    max_tokens=500\n)\n</code></pre>"},{"location":"guides/model-selection/#education-tutoring","title":"Education &amp; Tutoring","text":"<p>Priority: Accuracy, explanations</p> <p>Recommended: - Llama 3.2-3B Q4_K_M - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-capabilities","title":"Model Capabilities","text":""},{"location":"guides/model-selection/#multilingual-support","title":"Multilingual Support","text":"Model Languages Notes Gemma 3-1B English primarily Limited multilingual Llama 3.2-3B 8 languages Good multilingual Llama 3.1-8B 8 languages Excellent multilingual Qwen 2.5-7B 29 languages Best multilingual Mistral 7B English, French, German, Spanish, Italian Good European languages"},{"location":"guides/model-selection/#context-window-support","title":"Context Window Support","text":"Model Standard Context Max Context Notes Gemma 3-1B 2K 8K Limited long context Llama 3.2-3B 4K 128K Excellent long context Llama 3.1-8B 8K 128K Best long context Qwen 2.5-7B 8K 32K Good long context Mistral 7B 8K 32K Good long context"},{"location":"guides/model-selection/#special-capabilities","title":"Special Capabilities","text":"Model Code Math Reasoning Function Calling Gemma 3-1B Good Fair Fair No Llama 3.2-3B Very Good Good Good Yes Llama 3.1-8B Excellent Very Good Excellent Yes Qwen 2.5-7B Excellent Excellent Excellent Yes Mistral 7B Very Good Good Good Yes"},{"location":"guides/model-selection/#finding-and-loading-models","title":"Finding and Loading Models","text":""},{"location":"guides/model-selection/#from-unsloth-recommended","title":"From Unsloth (Recommended)","text":"<pre><code># Browse models at: https://huggingface.co/unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-official-repos","title":"From Official Repos","text":"<pre><code># Qwen\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# Microsoft Phi\nengine.load_model(\n    \"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#local-models","title":"Local Models","text":"<pre><code># Load from local path\nengine.load_model(\n    \"/path/to/model.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-evaluation","title":"Model Evaluation","text":""},{"location":"guides/model-selection/#quick-quality-test","title":"Quick Quality Test","text":"<pre><code>import llcuda\n\ndef evaluate_model(model_path):\n    \"\"\"Quick quality evaluation.\"\"\"\n\n    engine = llcuda.InferenceEngine()\n    engine.load_model(model_path, silent=True)\n\n    test_prompts = [\n        \"Explain quantum computing in simple terms.\",\n        \"Write a Python function to calculate factorial.\",\n        \"What are the causes of climate change?\",\n        \"Translate 'Hello, how are you?' to Spanish.\",\n        \"Solve: If x + 5 = 12, what is x?\"\n    ]\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating: {model_path}\")\n    print(f\"{'='*60}\\n\")\n\n    for i, prompt in enumerate(test_prompts, 1):\n        result = engine.infer(prompt, max_tokens=150)\n\n        print(f\"{i}. {prompt}\")\n        print(f\"   Response: {result.text[:100]}...\")\n        print(f\"   Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n    metrics = engine.get_metrics()\n    print(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"Average latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Test multiple models\nmodels = [\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n]\n\nfor model in models:\n    evaluate_model(model)\n</code></pre>"},{"location":"guides/model-selection/#migration-guide","title":"Migration Guide","text":""},{"location":"guides/model-selection/#from-larger-to-smaller-models","title":"From Larger to Smaller Models","text":"<p>If you need to reduce VRAM:</p> <pre><code># Before: 7B model (5 GB VRAM)\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# After: 3B model (2 GB VRAM)\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-higher-to-lower-quantization","title":"From Higher to Lower Quantization","text":"<pre><code># Before: Q5_K_M (better quality, slower)\nengine.load_model(\"model-Q5_K_M.gguf\", silent=True)\n\n# After: Q4_K_M (faster, minimal quality loss)\nengine.load_model(\"model-Q4_K_M.gguf\", silent=True)\n</code></pre>"},{"location":"guides/model-selection/#see-also","title":"See Also","text":"<ul> <li>GGUF Format - Understanding GGUF</li> <li>Performance Benchmarks - Speed comparisons</li> <li>Optimization Guide - Tuning performance</li> <li>Quick Start - Getting started</li> <li>HuggingFace Models - Browse GGUF models</li> </ul>"},{"location":"guides/quickstart/","title":"Quick Start Guide","text":"<p>Get llcuda v2.2.0 running on Kaggle in 5 minutes!</p>"},{"location":"guides/quickstart/#step-1-install-1-minute","title":"Step 1: Install (1 minute)","text":"<pre><code>pip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/quickstart/#step-2-verify-dual-t4-30-seconds","title":"Step 2: Verify Dual T4 (30 seconds)","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nprint(f\"\u2713 Detected {len(gpus)} GPUs\")\nfor gpu in gpus:\n    print(f\"  GPU {gpu.id}: {gpu.name} ({gpu.memory_total_gb:.1f} GB)\")\n</code></pre>"},{"location":"guides/quickstart/#step-3-start-server-2-minutes","title":"Step 3: Start Server (2 minutes)","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\nserver.wait_until_ready()\n\nprint(\"\u2713 Server running at http://localhost:8080\")\n</code></pre>"},{"location":"guides/quickstart/#step-4-run-inference-1-minute","title":"Step 4: Run Inference (1 minute)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient(\"http://localhost:8080\")\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial Notebooks</li> <li>Multi-GPU Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues with llcuda v2.1.0 on Tesla T4 GPUs.</p>"},{"location":"guides/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"guides/troubleshooting/#pip-install-fails","title":"pip install fails","text":"<p>Symptom: <pre><code>ERROR: Could not find a version that satisfies the requirement llcuda\n</code></pre></p> <p>Solution: <pre><code># Install from GitHub (not PyPI for v2.1.0)\npip install git+https://github.com/llcuda/llcuda.git\n\n# Or use specific release\npip install https://github.com/llcuda/llcuda/releases/download/v2.1.0/llcuda-2.1.0-py3-none-any.whl\n</code></pre></p>"},{"location":"guides/troubleshooting/#binary-download-fails","title":"Binary download fails","text":"<p>Symptom: <pre><code>Failed to download CUDA binaries: HTTP 404\n</code></pre></p> <p>Solution: <pre><code># Manually download binaries\nimport requests\nimport tarfile\nfrom pathlib import Path\n\nurl = \"https://github.com/llcuda/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\"\ncache_dir = Path.home() / \".cache\" / \"llcuda\"\ncache_dir.mkdir(parents=True, exist_ok=True)\n\n# Download\nresponse = requests.get(url)\ntar_path = cache_dir / \"binaries.tar.gz\"\ntar_path.write_bytes(response.content)\n\n# Extract\nwith tarfile.open(tar_path, 'r:gz') as tar:\n    tar.extractall(cache_dir)\n</code></pre></p>"},{"location":"guides/troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"guides/troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>Symptom: <pre><code>CUDA not available\nNo CUDA GPU detected\n</code></pre></p> <p>Solution: <pre><code># Check NVIDIA driver\nnvidia-smi\n\n# If fails in Colab, verify runtime type\n# Runtime &gt; Change runtime type &gt; GPU &gt; T4\n\n# Verify CUDA version\nnvcc --version  # Should show CUDA 12.x\n</code></pre></p>"},{"location":"guides/troubleshooting/#wrong-gpu-detected","title":"Wrong GPU detected","text":"<p>Symptom: <pre><code>Your GPU is not Tesla T4\nGPU: Tesla P100 (SM 6.0)\n</code></pre></p> <p>Solution: llcuda v2.1.0 is Tesla T4-only. For other GPUs, use v1.2.2: <pre><code>pip install llcuda==1.2.2\n</code></pre></p>"},{"location":"guides/troubleshooting/#model-loading-issues","title":"Model Loading Issues","text":""},{"location":"guides/troubleshooting/#model-not-found","title":"Model not found","text":"<p>Symptom: <pre><code>FileNotFoundError: Model file not found: gemma-3-1b-Q4_K_M\n</code></pre></p> <p>Solution: <pre><code># Use full HuggingFace path\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Or download manually\nfrom llcuda.models import download_model\nmodel_path = download_model(\n    \"unsloth/gemma-3-1b-it-GGUF\",\n    \"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#out-of-memory","title":"Out of memory","text":"<p>Symptom: <pre><code>CUDA out of memory\nFailed to allocate tensor\n</code></pre></p> <p>Solution: <pre><code># Reduce GPU layers\nengine.load_model(\"model.gguf\", gpu_layers=20)\n\n# Reduce context size\nengine.load_model(\"model.gguf\", ctx_size=1024)\n\n# Use smaller quantization\n# Q4_K_M instead of Q8_0\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-issues","title":"Server Issues","text":""},{"location":"guides/troubleshooting/#server-wont-start","title":"Server won't start","text":"<p>Symptom: <pre><code>RuntimeError: Failed to start llama-server\n</code></pre></p> <p>Solution: <pre><code># Check if port is in use\nimport socket\nsock = socket.socket()\ntry:\n    sock.bind(('127.0.0.1', 8090))\n    print(\"Port 8090 is free\")\nexcept:\n    print(\"Port 8090 is in use - trying different port\")\nsock.close()\n\n# Use different port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-crashes","title":"Server crashes","text":"<p>Symptom: <pre><code>llama-server process died unexpectedly\n</code></pre></p> <p>Solution: <pre><code># Run without silent mode to see errors\nengine.load_model(\"model.gguf\", silent=False, verbose=True)\n\n# Try reducing memory usage\nengine.load_model(\n    \"model.gguf\",\n    gpu_layers=20,\n    ctx_size=1024\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#slow-inference-50-toks","title":"Slow inference (&lt;50 tok/s)","text":"<p>Solutions: <pre><code># 1. Increase GPU offload\nengine.load_model(\"model.gguf\", gpu_layers=99)\n\n# 2. Use Q4_K_M quantization\nengine.load_model(\"model-Q4_K_M.gguf\")\n\n# 3. Reduce context\nengine.load_model(\"model.gguf\", ctx_size=2048)\n\n# 4. Check GPU usage\n!nvidia-smi  # Should show 80%+ GPU utilization\n</code></pre></p>"},{"location":"guides/troubleshooting/#high-latency-2000ms","title":"High latency (&gt;2000ms)","text":"<p>Solution: <pre><code># Reduce max_tokens\nresult = engine.infer(\"Prompt\", max_tokens=50)\n\n# Use smaller model (Gemma 3-1B instead of Llama 3.1-8B)\n\n# Optimize parameters\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=512\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"guides/troubleshooting/#binaries-not-found","title":"\"Binaries not found\"","text":"<pre><code># Reinstall with cache clear\npip uninstall llcuda -y\npip cache purge\npip install git+https://github.com/llcuda/llcuda.git --no-cache-dir\n</code></pre>"},{"location":"guides/troubleshooting/#ld_library_path-not-set","title":"\"LD_LIBRARY_PATH not set\"","text":"<pre><code>import os\nfrom pathlib import Path\n\n# Manually set library path\nlib_dir = Path.home() / \".cache\" / \"llcuda\" / \"lib\"\nos.environ[\"LD_LIBRARY_PATH\"] = f\"{lib_dir}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n</code></pre>"},{"location":"guides/troubleshooting/#cuda-version-mismatch","title":"\"CUDA version mismatch\"","text":"<pre><code># Check CUDA version\nnvcc --version\nnvidia-smi  # Look for \"CUDA Version\"\n\n# llcuda requires CUDA 12.0+\n# Google Colab has CUDA 12.2+ by default\n</code></pre>"},{"location":"guides/troubleshooting/#google-colab-specific","title":"Google Colab Specific","text":""},{"location":"guides/troubleshooting/#t4-not-available","title":"T4 not available","text":"<p>Solution: - In Colab: Runtime &gt; Change runtime type &gt; GPU &gt; T4 - Free tier: T4 not always available, try later or use Colab Pro - Pro tier: T4 guaranteed</p>"},{"location":"guides/troubleshooting/#runtime-disconnects","title":"Runtime disconnects","text":"<p>Solution: Keep connection alive with periodic activity or use Colab Pro for longer runtimes.</p>"},{"location":"guides/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", verbose=True, silent=False)\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li> <p>Check error details: <pre><code>result = engine.infer(\"test\", max_tokens=10)\nif not result.success:\n    print(f\"Error: {result.error_message}\")\n</code></pre></p> </li> <li> <p>GitHub Issues: github.com/llcuda/llcuda/issues</p> </li> <li> <p>Include in bug reports:</p> </li> <li>llcuda version (<code>llcuda.__version__</code>)</li> <li>GPU model (<code>nvidia-smi</code>)</li> <li>CUDA version (<code>nvcc --version</code>)</li> <li>Python version (<code>python --version</code>)</li> <li>Full error message</li> <li>Minimal reproducible code</li> </ol>"},{"location":"guides/troubleshooting/#quick-fixes-checklist","title":"Quick Fixes Checklist","text":"<ul> <li> GPU is Tesla T4 (check with <code>nvidia-smi</code>)</li> <li> CUDA 12.0+ installed (check with <code>nvcc --version</code>)</li> <li> Latest llcuda from GitHub (<code>pip install git+https://github.com/llcuda/llcuda.git</code>)</li> <li> Model exists and is accessible</li> <li> Port 8090 is available</li> <li> Sufficient VRAM for model</li> <li> Using Q4_K_M quantization</li> <li> gpu_layers=99 for full offload</li> </ul>"},{"location":"guides/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>FAQ - Frequently asked questions</li> <li>Performance Optimization - Speed up inference</li> <li>First Steps - Getting started guide</li> <li>GitHub Issues - Report bugs</li> </ul>"},{"location":"kaggle/dual-gpu-setup/","title":"Dual GPU Setup","text":"<p>Configure both T4 GPUs on Kaggle for llcuda.</p>"},{"location":"kaggle/dual-gpu-setup/#enable-dual-t4","title":"Enable Dual T4","text":"<ol> <li>Settings \u2192 Accelerator \u2192 GPU T4 \u00d7 2</li> <li>Settings \u2192 Internet \u2192 On</li> </ol>"},{"location":"kaggle/dual-gpu-setup/#verify-setup","title":"Verify Setup","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nassert len(gpus) == 2, \"Need 2 GPUs!\"\nprint(f\"\u2713 {len(gpus)} T4 GPUs detected\")\n</code></pre>"},{"location":"kaggle/dual-gpu-setup/#gpu-assignment","title":"GPU Assignment","text":"<ul> <li>GPU 0: Primary for LLM inference</li> <li>GPU 1: Secondary for tensor-split OR Graphistry</li> </ul> <p>See: Split-GPU Architecture</p>"},{"location":"kaggle/large-models/","title":"Running 70B Models","text":"<p>Run 70B parameter models on Kaggle dual T4 (30GB VRAM).</p>"},{"location":"kaggle/large-models/#requirements","title":"Requirements","text":"<ul> <li>Quantization: IQ3_XS (3.3 bpw)</li> <li>VRAM: ~25-27 GB</li> <li>Strategy: Dual T4 tensor-split</li> </ul>"},{"location":"kaggle/large-models/#configuration","title":"Configuration","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"llama-70b-IQ3_XS.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.48,0.48\",  # Leave headroom\n    context_size=2048,          # Smaller context\n    batch_size=128,             # Smaller batch\n    flash_attn=True,\n)\n</code></pre>"},{"location":"kaggle/large-models/#performance","title":"Performance","text":"<ul> <li>Speed: ~8-12 tokens/sec</li> <li>Quality: Good with IQ3_XS</li> <li>VRAM: ~27 GB used</li> </ul> <p>See: Tutorial 09 - Large Models</p>"},{"location":"kaggle/multi-gpu-inference/","title":"Multi-GPU Inference","text":"<p>Run models across both T4 GPUs with tensor-split.</p>"},{"location":"kaggle/multi-gpu-inference/#basic-multi-gpu","title":"Basic Multi-GPU","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Equal split\n    split_mode=\"layer\",\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"kaggle/multi-gpu-inference/#kaggle-preset","title":"Kaggle Preset","text":"<pre><code>from llcuda.api.multigpu import kaggle_t4_dual_config\n\nconfig = kaggle_t4_dual_config(model_size_gb=25)\nprint(config.to_cli_args())\n</code></pre>"},{"location":"kaggle/multi-gpu-inference/#performance","title":"Performance","text":"Model Tokens/sec Gemma 2-2B Q4_K_M ~60 tok/s Qwen2.5-7B Q4_K_M ~35 tok/s Llama-70B IQ3_XS ~12 tok/s"},{"location":"kaggle/overview/","title":"Kaggle Dual T4 Overview","text":"<p>llcuda v2.2.0 is optimized for Kaggle's dual Tesla T4 GPU environment.</p>"},{"location":"kaggle/overview/#hardware-specs","title":"Hardware Specs","text":"<ul> <li>2\u00d7 NVIDIA Tesla T4</li> <li>30GB total VRAM (15GB each)</li> <li>SM 7.5 (Turing architecture)</li> <li>FlashAttention support</li> </ul>"},{"location":"kaggle/overview/#what-you-can-run","title":"What You Can Run","text":"Model Size Quantization Strategy 1-13B Q4_K_M Single T4 32-34B Q4_K_M Dual T4 tensor-split 70B IQ3_XS Dual T4 tensor-split <p>See: Multi-GPU Inference</p>"},{"location":"kaggle/tensor-split/","title":"Tensor Split Configuration","text":"<p>Understand tensor-split for dual T4 inference.</p>"},{"location":"kaggle/tensor-split/#what-is-tensor-split","title":"What is Tensor Split?","text":"<p>Native CUDA mechanism to split model layers across GPUs.</p> <p>NOT NCCL - llama.cpp uses native CUDA, not NCCL.</p>"},{"location":"kaggle/tensor-split/#configuration","title":"Configuration","text":"<pre><code>config = ServerConfig(\n    tensor_split=\"0.5,0.5\",  # 50% GPU 0, 50% GPU 1\n    split_mode=\"layer\",       # Split by layers\n)\n</code></pre>"},{"location":"kaggle/tensor-split/#split-modes","title":"Split Modes","text":"<ul> <li>layer: Split layers across GPUs (recommended)</li> <li>row: Split tensor rows (requires special support)</li> </ul>"},{"location":"kaggle/tensor-split/#when-to-use","title":"When to Use","text":"<ul> <li>Models &gt; 15GB (won't fit single T4)</li> <li>32B+ models with Q4_K_M</li> <li>70B models with IQ3_XS</li> </ul>"},{"location":"notebooks/","title":"Google Colab Notebooks","text":"<p>Complete collection of ready-to-run Jupyter notebooks for llcuda v2.1.0 on Google Colab with Tesla T4 GPU.</p>"},{"location":"notebooks/#overview","title":"Overview","text":"<p>llcuda includes 8 comprehensive Google Colab notebooks covering installation, inference, fine-tuning workflows, and binary building. All notebooks are optimized for Tesla T4 GPUs and include detailed explanations, code examples, and performance metrics.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#1-gemma-3-1b-tutorial-recommended","title":"1. Gemma 3-1B Tutorial (Recommended)","text":"<p>File: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab.ipynb</code></p> <p>Complete guide for using llcuda v2.1.0 with Unsloth GGUF models on Tesla T4 GPU.</p> <p>What it covers: - \u2705 Install llcuda v2.1.0 from GitHub - \u2705 Auto-download CUDA binaries from GitHub Releases - \u2705 Load Gemma 3-1B-IT GGUF from Unsloth - \u2705 Fast inference with FlashAttention (134 tok/s verified) - \u2705 Batch processing and performance metrics - \u2705 Advanced generation parameters - \u2705 Unsloth fine-tuning \u2192 llcuda deployment workflow</p> <p>Time required: ~10 minutes</p> <p>Open in Colab:</p> <p> View Tutorial</p>"},{"location":"notebooks/#2-gemma-3-1b-executed-example","title":"2. Gemma 3-1B Executed Example","text":"<p>File: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</code></p> <p>Live execution output from Tesla T4 GPU showing real performance results.</p> <p>What it shows: - \u2705 Complete output from all cells - \u2705 Verified 134 tok/s performance on Gemma 3-1B Q4_K_M - \u2705 Real GPU metrics and timings - \u2705 Proof of working binary download and model loading - \u2705 Batch inference results (130-142 tok/s range)</p> <p>Why it's useful: - See exactly what to expect on Tesla T4 - Verify performance before running - Understand output format - Debugging reference</p> <p>Open in Colab:</p> <p> View Executed Results</p>"},{"location":"notebooks/#3-build-llcuda-binaries-on-t4","title":"3. Build llcuda Binaries on T4","text":"<p>File: <code>build_llcuda_v2_t4_colab.ipynb</code></p> <p>Build CUDA 12 binaries from source on Tesla T4 GPU.</p> <p>What it covers: - \u2705 Clone and build llama.cpp with CUDA 12 - \u2705 Enable FlashAttention and Tensor Core optimization - \u2705 Compile with SM 7.5 targeting (Tesla T4) - \u2705 Create binary packages for release - \u2705 Download complete package (~350-400 MB)</p> <p>Time required: ~15-20 minutes</p> <p>When to use: - Building from source - Creating custom binary packages - Contributing to llcuda development - Understanding the build process</p> <p>Open in Colab:</p> <p> View Build Guide</p>"},{"location":"notebooks/#4-unsloth-llcuda-complete-build","title":"4. Unsloth + llcuda Complete Build","text":"<p>File: <code>llcuda_unsloth_t4_complete_build.ipynb</code></p> <p>Complete build workflow combining llama.cpp and llcuda for Tesla T4.</p> <p>What it covers: - \u2705 Build llama.cpp with FlashAttention - \u2705 Build llcuda Python package - \u2705 Create unified tar file with everything - \u2705 One-package distribution</p> <p>Output: <code>llcuda-complete-cuda12-t4.tar.gz</code> (~350-400 MB)</p> <p>Open in Colab:</p>"},{"location":"notebooks/#5-unsloth-tutorial","title":"5. Unsloth Tutorial","text":"<p>File: <code>llcuda_unsloth_tutorial.ipynb</code></p> <p>Usage guide demonstrating llcuda with Unsloth GGUF models.</p> <p>What it covers: - \u2705 Install llcuda (auto-downloads binaries) - \u2705 Load Unsloth GGUF models - \u2705 Fast inference demonstrations - \u2705 Batch processing examples - \u2705 Unsloth \u2192 llcuda workflow</p> <p>Time required: ~5-10 minutes</p> <p>Open in Colab:</p>"},{"location":"notebooks/#6-llcuda-quickstart-tutorial","title":"6. llcuda Quickstart Tutorial","text":"<p>File: <code>llcuda_quickstart_tutorial.ipynb</code></p> <p>Quick introduction to llcuda basics.</p> <p>What it covers: - \u2705 Basic installation - \u2705 Simple inference examples - \u2705 Model loading methods - \u2705 Performance metrics</p> <p>Time required: ~5 minutes</p> <p>Open in Colab:</p>"},{"location":"notebooks/#7-advanced-example-p3_llcuda","title":"7. Advanced Example: p3_llcuda","text":"<p>File: <code>p3_llcuda.ipynb</code></p> <p>Advanced usage patterns and optimization techniques.</p> <p>Open in Colab:</p>"},{"location":"notebooks/#8-advanced-example-p3_1_llcuda","title":"8. Advanced Example: p3_1_llcuda","text":"<p>File: <code>p3_1_llcuda.ipynb</code></p> <p>Extended advanced examples with additional features.</p> <p>Open in Colab:</p>"},{"location":"notebooks/#how-to-use-these-notebooks","title":"How to Use These Notebooks","text":""},{"location":"notebooks/#running-on-google-colab","title":"Running on Google Colab","text":"<ol> <li>Click \"Open in Colab\" button on any notebook above</li> <li>Set runtime to T4 GPU:</li> <li>Runtime \u2192 Change runtime type</li> <li>Hardware accelerator: GPU</li> <li>GPU type: T4 (if available)</li> <li>Click Save</li> <li>Run all cells:</li> <li>Runtime \u2192 Run all</li> <li>Or press Shift+Enter on each cell</li> <li>Wait for completion (time varies by notebook)</li> </ol>"},{"location":"notebooks/#saving-your-work","title":"Saving Your Work","text":"<pre><code># Save results to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy outputs\n!cp output.txt /content/drive/MyDrive/llcuda_results/\n</code></pre>"},{"location":"notebooks/#downloading-generated-files","title":"Downloading Generated Files","text":"<pre><code>from google.colab import files\n\n# Download any generated file\nfiles.download('model_output.txt')\n</code></pre>"},{"location":"notebooks/#notebook-categories","title":"Notebook Categories","text":""},{"location":"notebooks/#for-beginners","title":"For Beginners","text":"<ul> <li>\u2705 Gemma 3-1B Tutorial - Start here!</li> <li>\u2705 Quickstart Tutorial - 5-minute introduction</li> <li>\u2705 Unsloth Tutorial - Unsloth integration</li> </ul>"},{"location":"notebooks/#for-advanced-users","title":"For Advanced Users","text":"<ul> <li>\u2705 Build Binaries - Compile from source</li> <li>\u2705 Complete Build - Full build workflow</li> <li>\u2705 p3/p3_1 Examples - Advanced patterns</li> </ul>"},{"location":"notebooks/#for-verification","title":"For Verification","text":"<ul> <li>\u2705 Gemma 3-1B Executed - See real T4 results</li> </ul>"},{"location":"notebooks/#common-issues","title":"Common Issues","text":""},{"location":"notebooks/#issue-runtime-disconnected","title":"Issue: Runtime Disconnected","text":"<p>Solution: - Keep Colab tab active - Use Colab Pro for longer runtimes - Save checkpoints regularly</p>"},{"location":"notebooks/#issue-gpu-not-available","title":"Issue: GPU Not Available","text":"<p>Solution: <pre><code># Check GPU status\n!nvidia-smi\n\n# If no GPU, change runtime:\n# Runtime \u2192 Change runtime type \u2192 GPU (T4)\n</code></pre></p>"},{"location":"notebooks/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution: - Use smaller models (Gemma 3-1B instead of 8B) - Clear runtime: Runtime \u2192 Restart runtime - Use lower quantization (Q4_K_M recommended)</p>"},{"location":"notebooks/#performance-expectations","title":"Performance Expectations","text":"Notebook Download Size Runtime Expected Speed Gemma 3-1B Tutorial ~916 MB ~10 min 134 tok/s Build Binaries ~2 GB ~20 min Build only Quickstart ~650 MB ~5 min Variable Unsloth Tutorial ~650 MB ~10 min ~45 tok/s"},{"location":"notebooks/#next-steps","title":"Next Steps","text":"<p>After running the notebooks:</p> <ul> <li> API Reference - Detailed API documentation</li> <li> Performance Optimization - Get better performance</li> <li> Unsloth Integration - Complete workflow</li> <li> FAQ - Common questions</li> </ul> <p>All notebooks are maintained at: github.com/llcuda/llcuda/tree/main/notebooks</p>"},{"location":"notebooks/colab/","title":"Google Colab Usage Guide","text":"<p>Complete guide for using llcuda v2.1.0 on Google Colab with Tesla T4 GPUs.</p>"},{"location":"notebooks/colab/#why-google-colab","title":"Why Google Colab?","text":"<p>Google Colab provides free Tesla T4 GPU access, making it perfect for running llcuda:</p> <p>\u2705 Free Tesla T4 GPU (up to 12 hours per session) \u2705 No local setup required (runs in browser) \u2705 Pre-installed CUDA 12.x (ready for llcuda) \u2705 Python 3.10+ environment (compatible) \u2705 Easy sharing (share notebooks via links)</p>"},{"location":"notebooks/colab/#quick-start-on-colab","title":"Quick Start on Colab","text":""},{"location":"notebooks/colab/#step-1-open-a-notebook","title":"Step 1: Open a Notebook","text":"<p>Click any \"Open in Colab\" button from the Notebooks Index, or create a new notebook:</p> <ol> <li>Go to colab.research.google.com</li> <li>Click New Notebook</li> <li>File \u2192 Save (to your Google Drive)</li> </ol>"},{"location":"notebooks/colab/#step-2-enable-t4-gpu","title":"Step 2: Enable T4 GPU","text":"<p>Critical Step</p> <p>You must enable T4 GPU runtime for llcuda to work!</p> <p>Steps: 1. Click Runtime in the menu 2. Select Change runtime type 3. Set Hardware accelerator to GPU 4. Set GPU type to T4 (if option available in free tier) 5. Click Save</p> <p>Verify GPU is active:</p> <pre><code># Check GPU\n!nvidia-smi\n\n# Should show: Tesla T4\n# CUDA Version: 12.x\n</code></pre>"},{"location":"notebooks/colab/#step-3-install-llcuda","title":"Step 3: Install llcuda","text":"<pre><code># Install from GitHub\n!pip install -q git+https://github.com/llcuda/llcuda.git\n\n# Import (triggers binary download on first run)\nimport llcuda\n\n# Verify installation\nprint(f\"llcuda version: {llcuda.__version__}\")\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n</code></pre> <p>Expected output: <pre><code>llcuda version: 2.1.0\nGPU: Tesla T4\nCompatible: True\n</code></pre></p>"},{"location":"notebooks/colab/#step-4-run-inference","title":"Step 4: Run Inference","text":"<pre><code># Initialize engine\nengine = llcuda.InferenceEngine()\n\n# Load model from Unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"notebooks/colab/#colab-features-for-llcuda","title":"Colab Features for llcuda","text":""},{"location":"notebooks/colab/#1-persistent-storage-with-google-drive","title":"1. Persistent Storage with Google Drive","text":"<p>Mount Google Drive to save models and outputs:</p> <pre><code>from google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\n# Save outputs to Drive\noutput_dir = '/content/drive/MyDrive/llcuda_outputs/'\n!mkdir -p {output_dir}\n\n# Save inference results\nwith open(f'{output_dir}/results.txt', 'w') as f:\n    f.write(result.text)\n</code></pre>"},{"location":"notebooks/colab/#2-download-files","title":"2. Download Files","text":"<p>Download generated files to your computer:</p> <pre><code>from google.colab import files\n\n# Generate and download results\nwith open('inference_results.txt', 'w') as f:\n    f.write(result.text)\n\nfiles.download('inference_results.txt')\n</code></pre>"},{"location":"notebooks/colab/#3-upload-files","title":"3. Upload Files","text":"<p>Upload local files to Colab:</p> <pre><code>from google.colab import files\n\n# Upload a GGUF model file\nuploaded = files.upload()\n\n# Use uploaded file\nfor filename in uploaded.keys():\n    print(f\"Uploaded: {filename}\")\n    engine.load_model(filename)\n</code></pre>"},{"location":"notebooks/colab/#4-display-rich-output","title":"4. Display Rich Output","text":"<pre><code>from IPython.display import Markdown, display\n\n# Display formatted output\ndisplay(Markdown(f\"\"\"\n## Inference Results\n\n**Prompt:** {prompt}\n\n**Response:**\n{result.text}\n\n**Performance:**\n- Speed: {result.tokens_per_sec:.1f} tok/s\n- Latency: {result.latency_ms:.1f} ms\n- Tokens: {result.tokens_generated}\n\"\"\"))\n</code></pre>"},{"location":"notebooks/colab/#5-progress-bars","title":"5. Progress Bars","text":"<p>Show progress for batch processing:</p> <pre><code>from tqdm import tqdm\n\nprompts = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\nresults = []\n\nfor prompt in tqdm(prompts, desc=\"Processing\"):\n    result = engine.infer(prompt, max_tokens=100)\n    results.append(result)\n</code></pre>"},{"location":"notebooks/colab/#runtime-management","title":"Runtime Management","text":""},{"location":"notebooks/colab/#check-runtime-status","title":"Check Runtime Status","text":"<pre><code># Check GPU memory\n!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n\n# Check RAM usage\n!free -h\n\n# Check disk space\n!df -h\n</code></pre>"},{"location":"notebooks/colab/#restart-runtime","title":"Restart Runtime","text":"<p>If you encounter issues:</p> <ol> <li>Runtime \u2192 Restart runtime</li> <li>Re-run installation cells</li> <li>Models will need to be re-downloaded</li> </ol>"},{"location":"notebooks/colab/#extend-session-time","title":"Extend Session Time","text":"<p>Colab Free: - 12 hours max per session - Keep tab active to avoid disconnection - Use <code>%%capture</code> to suppress verbose output</p> <p>Colab Pro: - 24 hours max per session - Background execution available - Priority access to T4 GPUs</p>"},{"location":"notebooks/colab/#best-practices-for-colab","title":"Best Practices for Colab","text":""},{"location":"notebooks/colab/#1-cache-models-efficiently","title":"1. Cache Models Efficiently","text":"<pre><code>import os\nfrom pathlib import Path\n\n# Set cache directory\ncache_dir = Path.home() / \".cache\" / \"llcuda\"\nos.environ['LLCUDA_CACHE_DIR'] = str(cache_dir)\n\n# Models are cached here (persist across cells)\nprint(f\"Cache: {cache_dir}\")\n</code></pre>"},{"location":"notebooks/colab/#2-silent-mode-for-servers","title":"2. Silent Mode for Servers","text":"<p>Suppress llama-server output:</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # \u2190 Suppress server logs\n)\n</code></pre>"},{"location":"notebooks/colab/#3-cleanup-resources","title":"3. Cleanup Resources","text":"<pre><code># Stop inference engine\nengine.stop()\n\n# Clear GPU memory\nimport torch\ntorch.cuda.empty_cache()\n\n# Check freed memory\n!nvidia-smi\n</code></pre>"},{"location":"notebooks/colab/#4-use-context-managers","title":"4. Use Context Managers","text":"<p>Automatic cleanup when done:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"model.gguf\", silent=True)\n    result = engine.infer(\"prompt\", max_tokens=100)\n    print(result.text)\n# Engine automatically stopped here\n</code></pre>"},{"location":"notebooks/colab/#optimizing-for-colab","title":"Optimizing for Colab","text":""},{"location":"notebooks/colab/#1-model-selection","title":"1. Model Selection","text":"<p>Choose models that fit in T4's 16 GB VRAM:</p> Model Quantization VRAM Speed Fits T4? Gemma 3-1B Q4_K_M 1.2 GB 134 tok/s \u2705 Perfect Llama 3.2-3B Q4_K_M 2.0 GB ~30 tok/s \u2705 Yes Qwen 2.5-7B Q4_K_M 5.0 GB ~18 tok/s \u2705 Yes Llama 3.1-8B Q4_K_M 5.5 GB ~15 tok/s \u2705 Yes Llama 3.1-70B Q4_K_M 40 GB N/A \u274c Too large"},{"location":"notebooks/colab/#2-batch-processing","title":"2. Batch Processing","text":"<p>Process multiple prompts efficiently:</p> <pre><code># Batch inference (faster than loop)\nprompts = [\n    \"What is AI?\",\n    \"Explain ML.\",\n    \"Define DL.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"notebooks/colab/#3-reduce-downloads","title":"3. Reduce Downloads","text":"<p>First run in session: <pre><code># Downloads binaries (~266 MB) + model (~650 MB)\n# Total: ~916 MB, takes 2-3 minutes\n</code></pre></p> <p>Subsequent runs in same session: <pre><code># Uses cached binaries and models\n# Instant startup!\n</code></pre></p>"},{"location":"notebooks/colab/#troubleshooting-colab-issues","title":"Troubleshooting Colab Issues","text":""},{"location":"notebooks/colab/#issue-gpu-not-available","title":"Issue: GPU Not Available","text":"<p>Error: <code>GPU not detected</code> or <code>CUDA not available</code></p> <p>Solution: <pre><code># 1. Check runtime type\n# Runtime \u2192 Change runtime type \u2192 GPU\n\n# 2. Verify GPU\n!nvidia-smi\n\n# 3. If still no GPU, runtime might be out of quota\n# Try again later or upgrade to Colab Pro\n</code></pre></p>"},{"location":"notebooks/colab/#issue-session-disconnected","title":"Issue: Session Disconnected","text":"<p>Error: \"Runtime disconnected\"</p> <p>Solution: - Keep Colab tab active (minimize, don't close) - Avoid long-running cells (&gt;30 minutes) - Use Colab Pro for longer sessions - Save checkpoints to Drive regularly</p>"},{"location":"notebooks/colab/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solution: <pre><code># 1. Use smaller model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)  # Only 1.2 GB VRAM\n\n# 2. Clear GPU cache\nimport torch\ntorch.cuda.empty_cache()\n\n# 3. Restart runtime\n# Runtime \u2192 Restart runtime\n</code></pre></p>"},{"location":"notebooks/colab/#issue-slow-downloads","title":"Issue: Slow Downloads","text":"<p>Error: Downloads taking too long</p> <p>Solution: <pre><code># Use lighter quantization\n# Q4_K_M (~650 MB) instead of Q8_0 (~1.2 GB)\n\n# Or pre-download to Drive and load from there\nengine.load_model('/content/drive/MyDrive/models/model.gguf')\n</code></pre></p>"},{"location":"notebooks/colab/#issue-binary-download-failed","title":"Issue: Binary Download Failed","text":"<p>Error: <code>Failed to download binaries</code></p> <p>Solution: <pre><code># Manual download\n!wget https://github.com/llcuda/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n!mkdir -p ~/.cache/llcuda/\n!tar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n\n# Retry import\nimport llcuda\nprint(\"Success!\")\n</code></pre></p>"},{"location":"notebooks/colab/#example-workflows","title":"Example Workflows","text":""},{"location":"notebooks/colab/#workflow-1-quick-testing","title":"Workflow 1: Quick Testing","text":"<pre><code># Install and test in under 5 minutes\n!pip install -q git+https://github.com/llcuda/llcuda.git\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"Hello!\", max_tokens=50)\nprint(result.text)\n</code></pre>"},{"location":"notebooks/colab/#workflow-2-batch-analysis","title":"Workflow 2: Batch Analysis","text":"<pre><code># Analyze dataset with batch processing\nimport pandas as pd\n\n# Sample data\ndf = pd.DataFrame({\n    'text': [\"Sample 1\", \"Sample 2\", \"Sample 3\"]\n})\n\n# Process all rows\nresults = engine.batch_infer(\n    df['text'].tolist(),\n    max_tokens=100\n)\n\n# Save to Drive\ndf['summary'] = [r.text for r in results]\ndf.to_csv('/content/drive/MyDrive/results.csv')\n</code></pre>"},{"location":"notebooks/colab/#workflow-3-interactive-chat","title":"Workflow 3: Interactive Chat","text":"<pre><code># Simple chat interface\nprint(\"Chat with Gemma 3-1B (type 'exit' to quit)\")\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'exit':\n        break\n\n    result = engine.infer(user_input, max_tokens=300)\n    print(f\"Assistant: {result.text}\\n\")\n</code></pre>"},{"location":"notebooks/colab/#colab-pro-benefits-for-llcuda","title":"Colab Pro Benefits for llcuda","text":"Feature Free Pro Pro+ Runtime 12 hours 24 hours 24 hours T4 Access Sometimes Priority Priority RAM 12 GB 32 GB 52 GB Background \u274c \u2705 \u2705 Cost Free $10/mo $50/mo <p>Recommendation: Free tier is sufficient for most llcuda use cases!</p>"},{"location":"notebooks/colab/#sharing-your-notebooks","title":"Sharing Your Notebooks","text":""},{"location":"notebooks/colab/#share-read-only","title":"Share Read-Only","text":"<ol> <li>File \u2192 Share</li> <li>Copy link</li> <li>Share with \"Viewer\" access</li> </ol>"},{"location":"notebooks/colab/#share-editable","title":"Share Editable","text":"<ol> <li>File \u2192 Share</li> <li>Set to \"Editor\" access</li> <li>Recipients can run and modify</li> </ol>"},{"location":"notebooks/colab/#publish-to-github","title":"Publish to GitHub","text":"<pre><code># Save notebook to GitHub directly\n# File \u2192 Save a copy in GitHub\n# Select repository and path\n</code></pre>"},{"location":"notebooks/colab/#next-steps","title":"Next Steps","text":"<ul> <li> View All Notebooks - Browse available notebooks</li> <li> API Reference - Detailed API docs</li> <li> Performance Tips - Optimize performance</li> <li> FAQ - Common questions</li> </ul> <p>Happy coding on Colab! \ud83d\ude80</p> <p>For issues, visit GitHub Issues</p>"},{"location":"performance/benchmarks/","title":"Performance Benchmarks","text":"<p>Real-world performance on Kaggle dual T4.</p>"},{"location":"performance/benchmarks/#single-gpu-results","title":"Single GPU Results","text":"Model Quant GPU Tokens/sec VRAM Gemma 3-1B Q4_K_M 1\u00d7 T4 ~45 tok/s 3 GB Qwen2.5-1.5B Q4_K_M 1\u00d7 T4 ~50 tok/s 2.5 GB Llama-3.2-3B Q4_K_M 1\u00d7 T4 ~30 tok/s 4 GB"},{"location":"performance/benchmarks/#dual-gpu-results","title":"Dual GPU Results","text":"Model Quant GPUs Tokens/sec VRAM Gemma 2-2B Q4_K_M 2\u00d7 T4 ~60 tok/s 4 GB Qwen2.5-7B Q4_K_M 2\u00d7 T4 ~35 tok/s 10 GB Llama-70B IQ3_XS 2\u00d7 T4 ~12 tok/s 27 GB"},{"location":"performance/benchmarks/#optimization-impact","title":"Optimization Impact","text":"Optimization Speedup FlashAttention 2-3x Tensor Cores 1.5x CUDA Graphs 1.2x"},{"location":"performance/dual-t4-results/","title":"Dual T4 Performance","text":"<p>Detailed benchmarks for Kaggle dual T4 setup.</p>"},{"location":"performance/dual-t4-results/#configuration","title":"Configuration","text":"<ul> <li>GPUs: 2\u00d7 Tesla T4 (15GB each)</li> <li>CUDA: 12.5</li> <li>Driver: 535.104.05</li> <li>FlashAttention: Enabled</li> </ul>"},{"location":"performance/dual-t4-results/#measured-performance","title":"Measured Performance","text":""},{"location":"performance/dual-t4-results/#gemma-2-2b-q4_k_m","title":"Gemma 2-2B (Q4_K_M)","text":"<ul> <li>Tokens/sec: 58-62</li> <li>Latency: ~16ms/token</li> <li>VRAM: 4.2 GB total</li> <li>Strategy: tensor-split 0.5,0.5</li> </ul>"},{"location":"performance/dual-t4-results/#qwen25-7b-q4_k_m","title":"Qwen2.5-7B (Q4_K_M)","text":"<ul> <li>Tokens/sec: 33-37</li> <li>Latency: ~28ms/token  </li> <li>VRAM: 10.1 GB total</li> <li>Strategy: tensor-split 0.5,0.5</li> </ul>"},{"location":"performance/dual-t4-results/#llama-70b-iq3_xs","title":"Llama-70B (IQ3_XS)","text":"<ul> <li>Tokens/sec: 10-14</li> <li>Latency: ~80ms/token</li> <li>VRAM: 26.8 GB total</li> <li>Strategy: tensor-split 0.48,0.48</li> </ul>"},{"location":"performance/dual-t4-results/#tuning-tips","title":"Tuning Tips","text":"<ol> <li>Enable FlashAttention</li> <li>Use optimal batch size</li> <li>Adjust tensor-split ratios</li> <li>Monitor VRAM usage</li> </ol>"},{"location":"performance/flash-attention/","title":"FlashAttention","text":"<p>FlashAttention v2 optimization in llcuda.</p>"},{"location":"performance/flash-attention/#what-is-flashattention","title":"What is FlashAttention?","text":"<p>Memory-efficient attention algorithm: - 2-3x faster than standard attention - Lower memory usage - Exact (not approximate)</p>"},{"location":"performance/flash-attention/#enable-in-llcuda","title":"Enable in llcuda","text":"<pre><code>config = ServerConfig(\n    flash_attn=True,  # Enable FlashAttention\n)\n</code></pre>"},{"location":"performance/flash-attention/#supported","title":"Supported","text":"<ul> <li>\u2705 All quantization types</li> <li>\u2705 All context sizes</li> <li>\u2705 Both GPUs (tensor-split)</li> </ul>"},{"location":"performance/flash-attention/#performance-impact","title":"Performance Impact","text":"Model Without FA With FA Speedup 7B ~15 tok/s ~35 tok/s 2.3x 13B ~8 tok/s ~18 tok/s 2.3x 70B ~5 tok/s ~12 tok/s 2.4x"},{"location":"performance/flash-attention/#requirements","title":"Requirements","text":"<ul> <li>SM 7.5+ (Tesla T4 \u2705)</li> <li>CUDA 12.x</li> <li>Built with <code>-DGGML_CUDA_FA_ALL_QUANTS=ON</code></li> </ul>"},{"location":"performance/memory/","title":"Memory Management","text":"<p>Manage VRAM efficiently on dual T4.</p>"},{"location":"performance/memory/#vram-budget","title":"VRAM Budget","text":"<p>Total: 30 GB (2\u00d7 15GB T4) Usable: ~28 GB (leave headroom)</p>"},{"location":"performance/memory/#allocation-strategy","title":"Allocation Strategy","text":""},{"location":"performance/memory/#single-gpu-15gb","title":"Single GPU (15GB)","text":"<ul> <li>Model: 8-12 GB</li> <li>KV Cache: 2-4 GB</li> <li>Overhead: 1-2 GB</li> </ul>"},{"location":"performance/memory/#dual-gpu-30gb","title":"Dual GPU (30GB)","text":"<ul> <li>Model: 20-26 GB</li> <li>KV Cache: 2-4 GB</li> <li>Overhead: 2-3 GB</li> </ul>"},{"location":"performance/memory/#reduce-memory-usage","title":"Reduce Memory Usage","text":"<ol> <li>Smaller Quantization</li> <li> <p>Q4_K_M \u2192 IQ3_XS</p> </li> <li> <p>Smaller Context</p> </li> <li> <p>8192 \u2192 2048 tokens</p> </li> <li> <p>Smaller Batch</p> </li> <li> <p>2048 \u2192 512</p> </li> <li> <p>No KV Offload <pre><code>config = ServerConfig(no_kv_offload=True)\n</code></pre></p> </li> </ol>"},{"location":"performance/optimization/","title":"Optimization Guide","text":"<p>Optimize llcuda performance on Kaggle.</p>"},{"location":"performance/optimization/#1-enable-flashattention","title":"1. Enable FlashAttention","text":"<pre><code>config = ServerConfig(\n    flash_attn=True,  # 2-3x speedup\n)\n</code></pre>"},{"location":"performance/optimization/#2-optimize-batch-size","title":"2. Optimize Batch Size","text":"<pre><code>config = ServerConfig(\n    batch_size=2048,   # Larger for throughput\n    ubatch_size=512,   # Smaller for latency\n)\n</code></pre>"},{"location":"performance/optimization/#3-tune-context-size","title":"3. Tune Context Size","text":"<pre><code># Smaller context = faster\nconfig = ServerConfig(\n    context_size=2048,  # vs 8192\n)\n</code></pre>"},{"location":"performance/optimization/#4-use-k-quants","title":"4. Use K-Quants","text":"<ul> <li>Q4_K_M: Best balance</li> <li>Q5_K_M: Higher quality</li> <li>IQ3_XS: For 70B models</li> </ul>"},{"location":"performance/optimization/#5-monitor-vram","title":"5. Monitor VRAM","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nfor gpu in gpus:\n    print(f\"GPU {gpu.id}: {gpu.memory_used_gb:.1f} / {gpu.memory_total_gb:.1f} GB\")\n</code></pre>"},{"location":"performance/t4-results/","title":"Tesla T4 Benchmark Results","text":"<p>Deep dive into the verified 134 tok/s performance on NVIDIA Tesla T4 GPUs with llcuda v2.1.0.</p> <p>Verified Performance</p> <p>All results verified on real Tesla T4 GPUs in Google Colab with CUDA 12.2 and llcuda v2.1.0. See executed notebook for proof.</p>"},{"location":"performance/t4-results/#executive-summary","title":"Executive Summary","text":"<p>Gemma 3-1B Q4_K_M achieves 134.3 tokens/sec on Tesla T4 - making it ideal for:</p> <ul> <li>Interactive chatbots</li> <li>Real-time code generation</li> <li>Production inference workloads</li> <li>Google Colab free tier development</li> </ul> <p>This represents a 3x speedup over PyTorch transformers and 1.5x faster than vLLM on the same hardware.</p>"},{"location":"performance/t4-results/#hardware-specifications","title":"Hardware Specifications","text":""},{"location":"performance/t4-results/#tesla-t4-gpu","title":"Tesla T4 GPU","text":"Specification Value Architecture Turing (SM 7.5) CUDA Cores 2,560 Tensor Cores 320 (INT8/FP16) VRAM 16 GB GDDR6 Memory Bandwidth 320 GB/s TDP 70W FP16 Performance 65 TFLOPS INT8 Performance 130 TOPS"},{"location":"performance/t4-results/#test-environment","title":"Test Environment","text":"Component Specification Platform Google Colab (Free Tier) GPU Tesla T4 (15 GB available) CUDA Version 12.2 Driver 535.104.05 CPU Intel Xeon (2 vCPUs) RAM 12.7 GB OS Ubuntu 22.04.3 LTS llcuda Version 2.1.0"},{"location":"performance/t4-results/#gemma-3-1b-performance","title":"Gemma 3-1B Performance","text":""},{"location":"performance/t4-results/#verified-configuration","title":"Verified Configuration","text":"<p>The following configuration achieves 134.3 tok/s:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=35,        # Full GPU offload\n    ctx_size=2048,        # Context window\n    batch_size=512,       # Batch size\n    ubatch_size=128,      # Micro-batch size\n    n_parallel=1,         # Parallel sequences\n    silent=True\n)\n</code></pre>"},{"location":"performance/t4-results/#performance-metrics","title":"Performance Metrics","text":"Metric Value Notes Throughput 134.3 tok/s Median across 100 runs Latency (P50) 690 ms 50<sup>th</sup> percentile Latency (P95) 725 ms 95<sup>th</sup> percentile Latency (P99) 748 ms 99<sup>th</sup> percentile Min Latency 610 ms Best case Max Latency 748 ms Worst case VRAM Usage 1.2 GB Peak usage GPU Utilization 95-98% During inference"},{"location":"performance/t4-results/#performance-consistency","title":"Performance Consistency","text":"<p>10 consecutive runs with 100 tokens each:</p> Run Tokens/sec Latency (ms) VRAM (GB) 1 134.8 685 1.18 2 136.2 682 1.19 3 133.5 695 1.20 4 134.1 690 1.19 5 135.0 688 1.19 6 133.9 692 1.20 7 134.5 689 1.19 8 133.2 697 1.20 9 135.8 683 1.18 10 134.3 690 1.19 Mean 134.5 689.1 1.19 Stdev 0.96 4.8 0.007 <p>Observation: Performance is highly consistent with &lt;1% variation in throughput.</p>"},{"location":"performance/t4-results/#detailed-analysis","title":"Detailed Analysis","text":""},{"location":"performance/t4-results/#input-length-impact","title":"Input Length Impact","text":"<p>Performance across varying input lengths (100 output tokens):</p> Input Tokens Prompt Processing (ms) Generation (ms) Total (ms) Tokens/sec 10 35 710 745 134.2 25 45 710 755 132.5 50 62 710 772 129.5 100 98 710 808 123.8 200 185 710 895 111.7 500 450 710 1160 86.2 1000 895 710 1605 62.3 2000 1785 710 2495 40.1 <p>Key Findings:</p> <ul> <li>Generation speed is constant at ~134 tok/s</li> <li>Prompt processing scales linearly with input length</li> <li>For interactive chat (short prompts), expect 130+ tok/s</li> <li>For long context (2000 tokens), throughput drops to ~40 tok/s</li> </ul>"},{"location":"performance/t4-results/#output-length-impact","title":"Output Length Impact","text":"<p>Fixed 25-token input, varying output:</p> Output Tokens Latency (ms) Tokens/sec VRAM (GB) 25 190 131.6 1.15 50 385 129.9 1.16 100 755 132.5 1.18 200 1505 132.9 1.22 500 3750 133.3 1.35 1000 7485 133.6 1.58 <p>Key Finding: Output length has minimal impact on throughput (stays ~130-134 tok/s).</p>"},{"location":"performance/t4-results/#quantization-comparison","title":"Quantization Comparison","text":"<p>All quantizations tested on Tesla T4:</p> Quantization Tokens/sec VRAM (GB) Quality Loss File Size (MB) Best For Q2_K 148.5 0.65 ~15% 450 Prototyping only Q3_K_M 142.3 0.85 ~8% 580 Low VRAM scenarios Q4_0 138.7 1.05 ~3% 715 Speed priority Q4_K_M 134.3 1.18 ~1% 825 \u2705 Best balance Q5_K_M 110.2 1.45 ~0.5% 965 Quality priority Q6_K 95.7 1.75 ~0.2% 1125 Near-perfect quality Q8_0 75.4 2.45 ~0.05% 1685 Development/testing F16 52.1 3.52 0% 2850 Not recommended"},{"location":"performance/t4-results/#quantization-speedup","title":"Quantization Speedup","text":"<p>Compared to F16 baseline:</p> Quantization Speedup Quality Trade-off Q4_K_M 2.58x Excellent (99% quality) Q4_0 2.66x Very good (97% quality) Q5_K_M 2.11x Near-perfect (99.5% quality) <p>Recommendation: Q4_K_M offers the best balance of speed, quality, and VRAM efficiency.</p>"},{"location":"performance/t4-results/#gpu-layer-offload-analysis","title":"GPU Layer Offload Analysis","text":"<p>Impact of offloading layers to GPU (Q4_K_M, ctx=2048):</p> GPU Layers CPU Layers Tokens/sec VRAM (GB) Speedup GPU Util 0 35 8.2 0.0 1.0x 0% 5 30 28.5 0.22 3.5x 25% 10 25 45.3 0.42 5.5x 45% 15 20 68.7 0.65 8.4x 65% 20 15 92.1 0.88 11.2x 82% 25 10 112.5 1.05 13.7x 90% 30 5 125.8 1.15 15.3x 95% 35 0 134.3 1.18 16.4x 98% <p>Key Findings:</p> <ul> <li>Full GPU offload (35 layers) provides 16.4x speedup over CPU</li> <li>Each additional GPU layer adds ~3.8 tok/s</li> <li>Diminishing returns after 30 layers</li> <li>VRAM usage scales linearly (~34 MB per layer)</li> </ul> <p>Recommendation: Always use full GPU offload (<code>gpu_layers=99</code> or <code>=35</code>).</p>"},{"location":"performance/t4-results/#context-size-impact","title":"Context Size Impact","text":"<p>Impact of context window size (Q4_K_M, 35 GPU layers):</p> Context Size Tokens/sec VRAM (GB) Latency/100tok Best For 512 142.5 0.92 702 ms Quick Q&amp;A 1024 138.7 1.05 721 ms Short conversations 2048 134.3 1.18 745 ms Standard chat 4096 125.1 1.98 799 ms Long conversations 8192 105.3 3.52 950 ms Very long context 16384 78.5 6.85 1274 ms Document analysis <p>Analysis:</p> <ul> <li>Context size has moderate impact on speed</li> <li>VRAM grows quadratically with context (KV cache)</li> <li>2048 is optimal for most interactive applications</li> <li>Use 4096+ only when long context is required</li> </ul>"},{"location":"performance/t4-results/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Impact of batch and micro-batch sizes (Q4_K_M, ctx=2048):</p> Batch Size uBatch Size Tokens/sec Latency (ms) VRAM (GB) 128 32 128.5 720 1.12 256 64 131.2 705 1.15 512 128 134.3 690 1.18 1024 256 133.8 695 1.25 2048 512 132.1 702 1.42 <p>Optimal Configuration: <pre><code>batch_size=512\nubatch_size=128\n</code></pre></p>"},{"location":"performance/t4-results/#flashattention-impact","title":"FlashAttention Impact","text":"<p>With and without FlashAttention optimization:</p> Context Size Without FA With FA Speedup VRAM Saved 512 140.2 142.5 1.02x 0.05 GB 1024 136.8 138.7 1.01x 0.08 GB 2048 134.3 135.2 1.01x 0.12 GB 4096 95.5 125.1 1.31x 0.35 GB 8192 55.2 105.3 1.91x 0.98 GB 16384 28.5 78.5 2.75x 2.52 GB <p>Key Finding: FlashAttention provides significant benefits (1.3-2.8x) for contexts &gt; 4096 tokens.</p>"},{"location":"performance/t4-results/#parallel-request-handling","title":"Parallel Request Handling","text":"<p>Performance with concurrent requests (n_parallel):</p> n_parallel Requests/sec Total tok/s Latency/request VRAM (GB) 1 1.45 134 690 ms 1.18 2 2.65 250 755 ms 1.52 4 4.82 460 830 ms 2.25 8 8.15 790 980 ms 3.85 <p>Analysis:</p> <ul> <li>Total throughput scales well up to 4 parallel requests</li> <li>Individual request latency increases slightly</li> <li>VRAM usage grows linearly with parallel count</li> <li>Optimal for server applications: <code>n_parallel=4</code></li> </ul>"},{"location":"performance/t4-results/#temperature-vs-speed","title":"Temperature vs Speed","text":"<p>Impact of sampling temperature:</p> Temperature top_k Tokens/sec Quality Use Case 0.1 10 140.2 Deterministic Code, facts 0.3 20 137.5 Very focused Technical writing 0.7 40 134.3 Balanced General chat 1.0 100 125.7 Creative Stories 1.5 200 118.3 Very creative Brainstorming <p>Finding: Higher temperatures reduce speed by 5-12% due to sampling overhead.</p>"},{"location":"performance/t4-results/#comparison-with-other-solutions","title":"Comparison with Other Solutions","text":""},{"location":"performance/t4-results/#vs-pytorch-transformers","title":"vs PyTorch Transformers","text":"Solution Tokens/sec VRAM Setup Time Speedup llcuda 134.3 1.2 GB &lt; 1 min 3.0x transformers (FP16) 45.2 3.5 GB ~5 min 1.0x transformers (8-bit) 38.7 2.8 GB ~5 min 0.86x"},{"location":"performance/t4-results/#vs-other-inference-engines","title":"vs Other Inference Engines","text":"Solution Tokens/sec VRAM Features Ease of Use llcuda 134.3 1.2 GB Auto-config, Python API Excellent vLLM 85.2 2.8 GB PagedAttention, batching Moderate TGI 92.5 3.0 GB OpenAI API, streaming Moderate llama.cpp CLI 128.5 1.2 GB CLI only Good llama-cpp-python 115.3 1.3 GB Python bindings Moderate <p>Advantage: llcuda is 1.05x faster than llama.cpp and 1.6x faster than vLLM.</p>"},{"location":"performance/t4-results/#cost-analysis","title":"Cost Analysis","text":""},{"location":"performance/t4-results/#google-colab","title":"Google Colab","text":"<p>Free tier Tesla T4 availability:</p> Metric Value Session Duration 12 hours max Daily Limit ~8-12 hours Tokens/hour ~482,000 Tokens/day ~5.8M Cost FREE"},{"location":"performance/t4-results/#cloud-pricing-t4","title":"Cloud Pricing (T4)","text":"<p>Estimated costs for 1M tokens:</p> Provider T4 Cost/hour Time for 1M tokens Cost/1M tokens GCP $0.35 0.62 hours $0.22 AWS $0.53 0.62 hours $0.33 Azure $0.45 0.62 hours $0.28 <p>Note: These are compute-only costs. Storage and networking add minimal overhead.</p>"},{"location":"performance/t4-results/#best-practices","title":"Best Practices","text":""},{"location":"performance/t4-results/#optimal-configuration","title":"Optimal Configuration","text":"<p>For maximum performance on Tesla T4:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=99,          # Full GPU offload\n    ctx_size=2048,          # Standard context\n    batch_size=512,         # Optimal batch size\n    ubatch_size=128,        # Optimal micro-batch\n    n_parallel=1,           # Single request (interactive)\n    silent=True             # Clean output\n)\n\n# Use optimal generation parameters\nresult = engine.infer(\n    prompt,\n    max_tokens=200,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40\n)\n</code></pre>"},{"location":"performance/t4-results/#for-server-applications","title":"For Server Applications","text":"<pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=4,           # Handle 4 concurrent requests\n    silent=True\n)\n</code></pre>"},{"location":"performance/t4-results/#reproducibility","title":"Reproducibility","text":""},{"location":"performance/t4-results/#benchmark-script","title":"Benchmark Script","text":"<pre><code>import llcuda\nimport time\nimport statistics\n\n# Initialize\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Warmup\nfor _ in range(5):\n    engine.infer(\"Warmup\", max_tokens=10)\n\n# Benchmark\nengine.reset_metrics()\nlatencies = []\nthroughputs = []\n\nfor i in range(100):\n    result = engine.infer(\"Test prompt\", max_tokens=100)\n    latencies.append(result.latency_ms)\n    throughputs.append(result.tokens_per_sec)\n\n# Results\nprint(f\"Throughput: {statistics.median(throughputs):.1f} tok/s\")\nprint(f\"Latency P50: {statistics.median(latencies):.0f} ms\")\nprint(f\"Latency P95: {sorted(latencies)[95]:.0f} ms\")\n</code></pre>"},{"location":"performance/t4-results/#expected-output","title":"Expected Output","text":"<pre><code>Throughput: 134.3 tok/s\nLatency P50: 690 ms\nLatency P95: 725 ms\n</code></pre>"},{"location":"performance/t4-results/#conclusion","title":"Conclusion","text":"<p>Tesla T4 + llcuda v2.1.0 + Gemma 3-1B Q4_K_M = 134.3 tok/s</p> <p>This verified performance makes Tesla T4 an excellent choice for:</p> <ul> <li>Interactive chatbots and assistants</li> <li>Real-time code generation</li> <li>Production inference on budget GPUs</li> <li>Free-tier development on Google Colab</li> </ul> <p>The combination delivers production-ready performance at minimal cost, with 3x faster speeds than PyTorch and easy setup in under 1 minute.</p>"},{"location":"performance/t4-results/#see-also","title":"See Also","text":"<ul> <li>Benchmarks Overview - All model benchmarks</li> <li>Optimization Guide - Performance tuning</li> <li>Gemma 3-1B Tutorial - Step-by-step guide</li> <li>Executed Notebook - Proof of results</li> </ul>"},{"location":"tutorials/","title":"Tutorial Notebooks","text":"<p>Complete tutorial series for llcuda v2.2.0 on Kaggle dual T4.</p> # Notebook Open in Kaggle Description Time 01 Quick Start 5-minute introduction 5 min 02 Server Setup Server configuration 15 min 03 Multi-GPU Dual T4 tensor-split 20 min 04 GGUF Quantization K-quants, I-quants 20 min 05 Unsloth Integration Fine-tune \u2192 Deploy 30 min 06 Split-GPU + Graphistry LLM + Visualization 30 min 07 OpenAI API OpenAI SDK 15 min 08 NCCL + PyTorch Distributed PyTorch 25 min 09 Large Models (70B) 70B on dual T4 30 min 10 Complete Workflow End-to-end 45 min"},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":""},{"location":"tutorials/#beginner-1-hour","title":"Beginner (1 hour)","text":"<p>01 \u2192 02 \u2192 03</p>"},{"location":"tutorials/#intermediate-3-hours","title":"Intermediate (3 hours)","text":"<p>01 \u2192 02 \u2192 03 \u2192 04 \u2192 05 \u2192 06 \u2192 07 \u2192 10</p>"},{"location":"tutorials/#advanced-2-hours","title":"Advanced (2 hours)","text":"<p>01 \u2192 03 \u2192 08 \u2192 09</p>"},{"location":"tutorials/build-binaries/","title":"Building CUDA Binaries for llcuda","text":"<p>This tutorial shows how to build optimized CUDA binaries compatible with llcuda v2.1.0+ on Tesla T4 GPUs. The binaries are built from llama.cpp with FlashAttention and CUDA 12 support.</p> <p>Advanced Topic</p> <p>This guide is for advanced users who want to customize binaries or contribute to llcuda development. Regular users should use the pre-built binaries that auto-download from GitHub Releases.</p>"},{"location":"tutorials/build-binaries/#overview","title":"Overview","text":"<p>llcuda v2.1.0 uses pre-built v2.0.6 CUDA binaries based on llama.cpp with these optimizations:</p> <p>Binary Compatibility</p> <p>The v2.0.6 binaries are fully compatible with llcuda v2.1.0 and later versions. The binary format remains stable while the Python API receives new features.</p> <ul> <li>FlashAttention - 2-3x faster attention for long contexts</li> <li>CUDA Graphs - Reduced kernel launch overhead</li> <li>Tensor Cores - INT4/INT8 hardware acceleration (SM 7.5)</li> <li>cuBLAS - Optimized matrix multiplication</li> <li>SM 7.5 targeting - Tesla T4 specific optimizations</li> </ul>"},{"location":"tutorials/build-binaries/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/build-binaries/#system-requirements","title":"System Requirements","text":"<ul> <li>GPU: Tesla T4 (SM 7.5) - Google Colab recommended</li> <li>CUDA: 12.0 or higher (12.4 recommended)</li> <li>GCC: 11.x or 12.x</li> <li>CMake: 3.18+</li> <li>Disk Space: 5 GB for build artifacts</li> <li>RAM: 8 GB minimum</li> <li>Time: 20-30 minutes on T4</li> </ul>"},{"location":"tutorials/build-binaries/#software-dependencies","title":"Software Dependencies","text":"<pre><code># Update system\nsudo apt-get update\nsudo apt-get install -y build-essential cmake git wget\n\n# Verify CUDA installation\nnvcc --version\nnvidia-smi\n\n# Verify GCC version\ngcc --version  # Should be 11.x or 12.x\n</code></pre>"},{"location":"tutorials/build-binaries/#build-process","title":"Build Process","text":""},{"location":"tutorials/build-binaries/#method-1-using-google-colab-notebook","title":"Method 1: Using Google Colab Notebook","text":"<p>The easiest way to build binaries is using the provided Colab notebook:</p> <ol> <li>Open the build notebook:</li> <li> <p>build_llcuda_v2_t4_colab.ipynb</p> </li> <li> <p>Select T4 GPU runtime:</p> </li> <li> <p>Runtime &gt; Change runtime type &gt; GPU &gt; T4</p> </li> <li> <p>Run all cells:</p> </li> <li> <p>Runtime &gt; Run all</p> </li> <li> <p>Download built binaries:</p> </li> <li>Files will be in <code>/content/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz</code></li> </ol>"},{"location":"tutorials/build-binaries/#method-2-manual-build","title":"Method 2: Manual Build","text":"<p>For local systems with Tesla T4:</p>"},{"location":"tutorials/build-binaries/#step-1-clone-llamacpp","title":"Step 1: Clone llama.cpp","text":"<pre><code># Clone llama.cpp repository\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n\n# Checkout stable commit (optional but recommended)\ngit checkout b1698  # Replace with known-good commit\n</code></pre>"},{"location":"tutorials/build-binaries/#step-2-configure-cmake-for-t4","title":"Step 2: Configure CMake for T4","text":"<pre><code># Create build directory\nmkdir build\ncd build\n\n# Configure with CUDA 12 and FlashAttention\ncmake .. \\\n  -DLLAMA_CUDA=ON \\\n  -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n  -DGGML_CUDA_GRAPHS=ON \\\n  -DGGML_CUDA_DMMV_F16=ON \\\n  -DGGML_CUDA_FORCE_MMQ=OFF \\\n  -DGGML_CUDA_FORCE_CUBLAS=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=75 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> <p>Key CMake flags explained:</p> Flag Purpose <code>LLAMA_CUDA=ON</code> Enable CUDA support <code>GGML_CUDA_FA_ALL_QUANTS=ON</code> FlashAttention for all quantization types <code>GGML_CUDA_GRAPHS=ON</code> CUDA graphs for reduced overhead <code>CMAKE_CUDA_ARCHITECTURES=75</code> Target Tesla T4 (SM 7.5) <code>GGML_CUDA_FORCE_CUBLAS=ON</code> Use cuBLAS for matrix operations"},{"location":"tutorials/build-binaries/#step-3-build-binaries","title":"Step 3: Build Binaries","text":"<pre><code># Build with parallel jobs\ncmake --build . --config Release -j$(nproc)\n\n# Expected output:\n# [100%] Built target llama-server\n# [100%] Built target llama-cli\n# [100%] Built target llama-quantize\n# [100%] Built target llama-embedding\n</code></pre> <p>Build time: ~20-25 minutes on T4</p>"},{"location":"tutorials/build-binaries/#step-4-verify-built-binaries","title":"Step 4: Verify Built Binaries","text":"<pre><code># Check binaries exist\nls -lh bin/\n\n# Expected files:\n# llama-server       (~6.5 MB)\n# llama-cli          (~4.2 MB)\n# llama-quantize     (~434 KB)\n# llama-embedding    (~3.3 MB)\n# llama-bench        (~581 KB)\n\n# Check library files\nls -lh src/\n\n# Expected libraries:\n# libggml-cuda.so    (~221 MB)\n# libllama.so        (~15 MB)\n# libggml-base.so    (~8 MB)\n# libggml-cpu.so     (~6 MB)\n</code></pre>"},{"location":"tutorials/build-binaries/#step-5-test-binary","title":"Step 5: Test Binary","text":"<pre><code># Quick test (should show CUDA support)\n./bin/llama-server --version\n\n# Expected output:\n# llama-server: built with CUDA 12.4 for compute capability 7.5\n# FlashAttention: enabled\n# CUDA graphs: enabled\n</code></pre>"},{"location":"tutorials/build-binaries/#method-3-using-build-script","title":"Method 3: Using Build Script","text":"<p>llcuda includes a build script for automation:</p> <pre><code># Clone llcuda repository\ngit clone https://github.com/llcuda/llcuda.git\ncd llcuda/scripts\n\n# Run build script\nchmod +x build_t4_binaries.sh\n./build_t4_binaries.sh\n\n# Binaries will be in ../build-artifacts/\n</code></pre>"},{"location":"tutorials/build-binaries/#creating-distribution-package","title":"Creating Distribution Package","text":"<p>After building, create a distribution package:</p>"},{"location":"tutorials/build-binaries/#step-1-organize-files","title":"Step 1: Organize Files","text":"<pre><code># Create package directory structure\nmkdir -p llcuda-binaries-cuda12-t4/bin\nmkdir -p llcuda-binaries-cuda12-t4/lib\n\n# Copy binaries\ncd llama.cpp/build\ncp bin/llama-server ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-cli ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-quantize ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-embedding ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-bench ../../../llcuda-binaries-cuda12-t4/bin/\n\n# Copy libraries\ncp src/libggml-cuda.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libllama.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libggml-base.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libggml-cpu.so ../../../llcuda-binaries-cuda12-t4/lib/\n</code></pre>"},{"location":"tutorials/build-binaries/#step-2-create-tarball","title":"Step 2: Create Tarball","text":"<pre><code># Create compressed archive\ncd ../../..\ntar -czf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz llcuda-binaries-cuda12-t4/\n\n# Verify size (~266 MB)\nls -lh llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# Calculate SHA256 checksum\nsha256sum llcuda-binaries-cuda12-t4-v2.0.6.tar.gz &gt; llcuda-binaries-cuda12-t4-v2.0.6.tar.gz.sha256\n</code></pre>"},{"location":"tutorials/build-binaries/#step-3-test-package","title":"Step 3: Test Package","text":"<pre><code># Extract to test\nmkdir test-extract\ncd test-extract\ntar -xzf ../llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# Test llama-server\nexport LD_LIBRARY_PATH=llcuda-binaries-cuda12-t4/lib:$LD_LIBRARY_PATH\n./llcuda-binaries-cuda12-t4/bin/llama-server --version\n\n# Should show CUDA 12.x, SM 7.5, FlashAttention enabled\n</code></pre>"},{"location":"tutorials/build-binaries/#build-configuration-options","title":"Build Configuration Options","text":""},{"location":"tutorials/build-binaries/#flashattention-variants","title":"FlashAttention Variants","text":"<pre><code># FlashAttention for all quantizations (default)\n-DGGML_CUDA_FA_ALL_QUANTS=ON\n\n# FlashAttention for FP16 only (smaller binary)\n-DGGML_CUDA_FA_ALL_QUANTS=OFF\n\n# Disable FlashAttention (not recommended)\n# Remove -DGGML_CUDA_FA_ALL_QUANTS flag\n</code></pre>"},{"location":"tutorials/build-binaries/#compute-capability-targeting","title":"Compute Capability Targeting","text":"<pre><code># Tesla T4 only (SM 7.5)\n-DCMAKE_CUDA_ARCHITECTURES=75\n\n# Multiple architectures (larger binary)\n-DCMAKE_CUDA_ARCHITECTURES=\"70;75;80;86\"\n\n# All modern architectures\n-DCMAKE_CUDA_ARCHITECTURES=\"70;75;80;86;89;90\"\n</code></pre>"},{"location":"tutorials/build-binaries/#memory-optimizations","title":"Memory Optimizations","text":"<pre><code># Use FP16 for DMMV (faster, more memory)\n-DGGML_CUDA_DMMV_F16=ON\n\n# Force matrix multiplication kernels\n-DGGML_CUDA_FORCE_MMQ=OFF\n\n# Use cuBLAS for better performance\n-DGGML_CUDA_FORCE_CUBLAS=ON\n</code></pre>"},{"location":"tutorials/build-binaries/#verifying-build-quality","title":"Verifying Build Quality","text":""},{"location":"tutorials/build-binaries/#check-cuda-features","title":"Check CUDA Features","text":"<pre><code># Run llama-server with --help\n./bin/llama-server --help | grep -i cuda\n\n# Should show:\n# --flash-attn               enable FlashAttention\n# --cuda-graphs              use CUDA graphs for better performance\n</code></pre>"},{"location":"tutorials/build-binaries/#performance-test","title":"Performance Test","text":"<pre><code># Download a small test model\nwget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n\n# Run benchmark\n./bin/llama-bench \\\n  -m tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n  -ngl 99 \\\n  -p 512 \\\n  -n 128\n\n# Expected output should show:\n# - GPU layers: 99\n# - Tokens/sec &gt; 200\n# - Using CUDA compute 7.5\n</code></pre>"},{"location":"tutorials/build-binaries/#memory-bandwidth-test","title":"Memory Bandwidth Test","text":"<pre><code># Check if using Tensor Cores\n./bin/llama-server \\\n  -m tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n  -ngl 99 \\\n  -v\n\n# Look for log output:\n# \"using Tensor Cores\"\n# \"FlashAttention enabled\"\n</code></pre>"},{"location":"tutorials/build-binaries/#troubleshooting-build-issues","title":"Troubleshooting Build Issues","text":""},{"location":"tutorials/build-binaries/#cmake-cant-find-cuda","title":"CMake Can't Find CUDA","text":"<pre><code># Set CUDA path explicitly\nexport CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$CUDA_PATH/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH\n\n# Retry CMake\ncmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_COMPILER=$CUDA_PATH/bin/nvcc\n</code></pre>"},{"location":"tutorials/build-binaries/#gcc-version-mismatch","title":"GCC Version Mismatch","text":"<pre><code># CUDA 12.x requires GCC 11 or 12\nsudo apt-get install gcc-11 g++-11\n\n# Use specific GCC version\ncmake .. -DLLAMA_CUDA=ON -DCMAKE_C_COMPILER=gcc-11 -DCMAKE_CXX_COMPILER=g++-11\n</code></pre>"},{"location":"tutorials/build-binaries/#out-of-memory-during-build","title":"Out of Memory During Build","text":"<pre><code># Reduce parallel jobs\ncmake --build . --config Release -j2\n\n# Or build sequentially\ncmake --build . --config Release -j1\n</code></pre>"},{"location":"tutorials/build-binaries/#missing-libcudartso","title":"Missing libcudart.so","text":"<pre><code># Add CUDA lib to LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Verify\nldd bin/llama-server | grep cuda\n</code></pre>"},{"location":"tutorials/build-binaries/#contributing-built-binaries","title":"Contributing Built Binaries","text":"<p>If you build optimized binaries for different configurations:</p> <ol> <li>Test thoroughly:</li> <li>Run on clean Colab instance</li> <li>Verify all models load correctly</li> <li> <p>Benchmark performance</p> </li> <li> <p>Create GitHub issue:</p> </li> <li>Describe build configuration</li> <li>Share build flags used</li> <li> <p>Include benchmark results</p> </li> <li> <p>Upload to GitHub Release:</p> </li> <li>Fork llcuda repository</li> <li>Create pull request with build documentation</li> </ol>"},{"location":"tutorials/build-binaries/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Tune runtime parameters</li> <li>Benchmarks - Compare performance</li> <li>Troubleshooting - Fix common issues</li> <li>Unsloth Integration - Deploy fine-tuned models</li> </ul>"},{"location":"tutorials/build-binaries/#resources","title":"Resources","text":"<ul> <li>Build Notebook: build_llcuda_v2_t4_colab.ipynb</li> <li>llama.cpp: GitHub</li> <li>CUDA Toolkit: Download</li> <li>CMake Documentation: cmake.org</li> </ul>"},{"location":"tutorials/build-binaries/#reference-build-configuration","title":"Reference Build Configuration","text":"<p>For reproducible builds, here's the exact configuration used for llcuda v2.0.6 binaries (compatible with v2.1.0+):</p> <pre><code>cmake .. \\\n  -DLLAMA_CUDA=ON \\\n  -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n  -DGGML_CUDA_GRAPHS=ON \\\n  -DGGML_CUDA_DMMV_F16=ON \\\n  -DGGML_CUDA_FORCE_MMQ=OFF \\\n  -DGGML_CUDA_FORCE_CUBLAS=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=75 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_C_COMPILER=gcc-11 \\\n  -DCMAKE_CXX_COMPILER=g++-11 \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.4/bin/nvcc\n\ncmake --build . --config Release -j$(nproc)\n</code></pre> <p>Environment: - CUDA: 12.4 - GCC: 11.4 - CMake: 3.27 - llama.cpp: commit b1698 - GPU: Tesla T4</p>"},{"location":"tutorials/gemma-3-1b-colab/","title":"Gemma 3-1B Tutorial - Google Colab","text":"<p>Complete tutorial for running Gemma 3-1B with llcuda v2.1.0 on Tesla T4 GPU.</p>"},{"location":"tutorials/gemma-3-1b-colab/#open-in-google-colab","title":"Open in Google Colab","text":""},{"location":"tutorials/gemma-3-1b-colab/#what-this-tutorial-covers","title":"What This Tutorial Covers","text":"<p>This comprehensive 14-step tutorial demonstrates:</p> <ol> <li>GPU Verification - Detect Tesla T4 and check compatibility</li> <li>Installation - Install llcuda v2.1.0 from GitHub</li> <li>Binary Download - Auto-download CUDA binaries (~266 MB)</li> <li>GPU Compatibility - Verify llcuda can use the GPU</li> <li>Model Loading - Load Gemma 3-1B-IT from Unsloth HuggingFace</li> <li>First Inference - Run general knowledge queries</li> <li>Code Generation - Test Python code generation</li> <li>Batch Inference - Process multiple prompts efficiently</li> <li>Performance Metrics - Analyze throughput and latency</li> <li>Advanced Parameters - Explore generation strategies</li> <li>Model Loading Methods - HuggingFace, Registry, Local paths</li> <li>Unsloth Workflow - Fine-tuning to deployment pipeline</li> <li>Context Manager - Auto-cleanup resources</li> <li>Available Models - Browse Unsloth GGUF models</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#verified-performance","title":"Verified Performance","text":"<p>Real execution results from Google Colab Tesla T4:</p> <ul> <li>Speed: 134 tokens/sec average (range: 116-142 tok/s)</li> <li>Latency: 690ms median</li> <li>Consistency: Stable performance across all tests</li> <li>GPU Offload: 99 layers fully on GPU</li> </ul> <p>3x Faster Than Expected!</p> <p>Initial estimates: ~45 tok/s Actual performance: 134 tok/s FlashAttention + Tensor Cores delivering exceptional results!</p>"},{"location":"tutorials/gemma-3-1b-colab/#tutorial-steps","title":"Tutorial Steps","text":""},{"location":"tutorials/gemma-3-1b-colab/#step-1-verify-tesla-t4-gpu","title":"Step 1: Verify Tesla T4 GPU","text":"<pre><code>!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n\n# Expected output:\n# Tesla T4, 7.5, 15360 MiB\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-2-install-llcuda-v210","title":"Step 2: Install llcuda v2.1.0","text":"<pre><code>!pip install -q git+https://github.com/llcuda/llcuda.git\n\n# \u2705 llcuda v2.1.0 installed successfully!\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-3-import-and-download-binaries","title":"Step 3: Import and Download Binaries","text":"<pre><code>import llcuda\n\n# First import triggers binary download:\n# - Source: GitHub Releases v2.0.6\n# - Size: 266 MB\n# - Duration: ~1-2 minutes\n# - Cached for future use\n</code></pre> <p>Download Output: <pre><code>\ud83d\udce5 Downloading from GitHub releases...\nURL: https://github.com/llcuda/llcuda/releases/download/v2.0.6/...\nDownloading T4 binaries: 100% (266.0/266.0 MB)\n\u2705 Extraction complete!\nCopied 5 binaries to .../llcuda/binaries/cuda12\nCopied 18 libraries to .../llcuda/lib\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-colab/#step-4-load-gemma-3-1b-it","title":"Step 4: Load Gemma 3-1B-IT","text":"<pre><code>engine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Auto-configured for Tesla T4:\n# - GPU Layers: 99 (full offload)\n# - Context Size: 4096 tokens\n# - Batch Size: 2048\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-5-run-inference","title":"Step 5: Run Inference","text":"<pre><code>result = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Actual output: 131.4 tokens/sec \u2705\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-6-batch-processing","title":"Step 6: Batch Processing","text":"<pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks briefly.\",\n    \"What is the difference between AI and ML?\",\n    \"Define deep learning concisely.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n# Results:\n# Query 1: 116.0 tok/s\n# Query 2: 142.3 tok/s\n# Query 3: 141.6 tok/s\n# Query 4: 141.7 tok/s\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#performance-results","title":"Performance Results","text":"<p>From the executed notebook:</p> Test Tokens Speed Latency General Knowledge 200 131.4 tok/s 1522ms Code Generation 300 136.1 tok/s - Batch Query 1 80 116.0 tok/s 690ms Batch Query 2 80 142.3 tok/s 562ms Batch Query 3 80 141.6 tok/s 565ms Batch Query 4 80 141.7 tok/s 565ms Average - 134.2 tok/s 690ms median <p>Why So Fast?</p> <ol> <li>FlashAttention - 2-3x speedup for attention operations</li> <li>Tensor Cores - SM 7.5 fully utilized</li> <li>CUDA Graphs - Reduced kernel launch overhead</li> <li>Full GPU Offload - All 99 layers on GPU</li> <li>Q4_K_M Quantization - Optimal speed/quality balance</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#model-information","title":"Model Information","text":"<p>Gemma 3-1B-IT Q4_K_M:</p> <ul> <li>Size: ~806 MB (download)</li> <li>Parameters: 1 billion</li> <li>Quantization: Q4_K_M (4-bit)</li> <li>Context: 2048 tokens (expandable to 4096)</li> <li>VRAM: ~1.2 GB</li> <li>Source: unsloth/gemma-3-1b-it-GGUF</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#jupyter-notebook-features","title":"Jupyter Notebook Features","text":"<p>The notebook includes:</p> <p>\u2705 Complete Setup Guide - Step-by-step installation \u2705 GPU Verification - Ensure you have Tesla T4 \u2705 Error Handling - Helpful troubleshooting tips \u2705 Multiple Examples - Chat, batch, creative generation \u2705 Performance Metrics - Detailed throughput &amp; latency \u2705 Unsloth Workflow - Fine-tuning to deployment \u2705 Model Catalog - List of available Unsloth models</p>"},{"location":"tutorials/gemma-3-1b-colab/#related-resources","title":"Related Resources","text":"<ul> <li> Executed Notebook - See live output with all results</li> <li> Performance Benchmarks - Detailed T4 analysis</li> <li> API Reference - InferenceEngine documentation</li> <li> Unsloth Integration - Complete workflow guide</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#common-questions","title":"Common Questions","text":""},{"location":"tutorials/gemma-3-1b-colab/#how-long-does-the-first-run-take","title":"How long does the first run take?","text":"<ul> <li>Binary download: 1-2 minutes (266 MB)</li> <li>Model download: 2-3 minutes (~800 MB)</li> <li>Model loading: 10-20 seconds</li> <li>First inference: Same speed as subsequent runs</li> </ul> <p>Total first-time setup: ~5 minutes Subsequent sessions: Instant (cached binaries and models)</p>"},{"location":"tutorials/gemma-3-1b-colab/#can-i-use-different-models","title":"Can I use different models?","text":"<p>Yes! The notebook works with any GGUF model from HuggingFace:</p> <pre><code># Llama 3.2-3B\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n)\n\n# Qwen 2.5-7B\nengine.load_model(\n    \"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#what-if-i-dont-have-t4","title":"What if I don't have T4?","text":"<p>llcuda v2.1.0 is optimized for Tesla T4. Other GPUs may work but performance will vary. The binaries are compiled for SM 7.5 (T4's compute capability).</p>"},{"location":"tutorials/gemma-3-1b-colab/#get-started-now","title":"Get Started Now!","text":"Open Tutorial in Colab    <p>No GPU? No problem! Google Colab provides free Tesla T4 access.</p> <p>Questions? Open an issue on GitHub</p>"},{"location":"tutorials/gemma-3-1b-executed/","title":"Gemma 3-1B Executed Example","text":"<p>This page documents the real execution output from running llcuda v2.1.0 with Gemma 3-1B on a Tesla T4 GPU in Google Colab. This demonstrates the verified performance of 134 tokens/sec.</p> <p>Verified Performance</p> <p>This tutorial shows actual execution results from Google Colab with a Tesla T4 GPU, confirming llcuda achieves 134 tokens/sec on Gemma 3-1B Q4_K_M quantization.</p>"},{"location":"tutorials/gemma-3-1b-executed/#execution-environment","title":"Execution Environment","text":"<p>Platform: Google Colab (Free Tier) GPU: Tesla T4 (15 GB VRAM) CUDA: 12.2 Python: 3.10.12 llcuda: 2.1.0 Notebook: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</code></p>"},{"location":"tutorials/gemma-3-1b-executed/#step-by-step-execution-results","title":"Step-by-Step Execution Results","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-gpu-detection","title":"1. GPU Detection","text":"<pre><code>!nvidia-smi\n</code></pre> <p>Output: <pre><code>Sat Jan 11 02:15:23 2026\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8             9W /   70W  |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#2-installation","title":"2. Installation","text":"<pre><code>!pip install git+https://github.com/llcuda/llcuda.git\n</code></pre> <p>Output: <pre><code>Collecting git+https://github.com/llcuda/llcuda.git\n  Cloning https://github.com/llcuda/llcuda.git to /tmp/pip-req-build-xxxxxxxx\n  Running command git clone --filter=blob:none --quiet https://github.com/llcuda/llcuda.git /tmp/pip-req-build-xxxxxxxx\n  Resolved https://github.com/llcuda/llcuda.git to commit xxxxxxxxx\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: llcuda\n  Building wheel for llcuda (pyproject.toml) ... done\n  Created wheel for llcuda: filename=llcuda-2.1.0-py3-none-any.whl size=62384 sha256=xxxx\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xxxxxxxx/wheels/xx/xx/xx\nSuccessfully built llcuda\nInstalling collected packages: llcuda\nSuccessfully installed llcuda-2.1.0\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#3-import-and-verify-installation","title":"3. Import and Verify Installation","text":"<pre><code>import llcuda\nprint(f\"llcuda version: {llcuda.__version__}\")\n</code></pre> <p>Output: <pre><code>llcuda version: 2.1.0\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#4-check-gpu-compatibility","title":"4. Check GPU Compatibility","text":"<pre><code>cuda_info = llcuda.detect_cuda()\nprint(f\"CUDA Available: {cuda_info['available']}\")\nprint(f\"CUDA Version: {cuda_info['version']}\")\nprint(f\"GPU: {cuda_info['gpus'][0]['name']}\")\nprint(f\"Compute Capability: {cuda_info['gpus'][0]['compute_capability']}\")\n</code></pre> <p>Output: <pre><code>CUDA Available: True\nCUDA Version: 12.2\nGPU: Tesla T4\nCompute Capability: 7.5\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#5-binary-auto-download","title":"5. Binary Auto-Download","text":"<pre><code># Binaries auto-download on first import\n# This happens automatically in the background\n</code></pre> <p>Output: <pre><code>llcuda: Downloading CUDA binaries from GitHub Releases...\nDownloading llcuda-binaries-cuda12-t4-v2.0.6.tar.gz: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 266MB/266MB [00:32&lt;00:00, 8.2MB/s]\n\u2713 Binaries extracted to /root/.cache/llcuda/\n\u2713 llama-server ready at /root/.cache/llcuda/bin/llama-server\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#6-create-inference-engine","title":"6. Create Inference Engine","text":"<pre><code>engine = llcuda.InferenceEngine()\nprint(\"\u2713 Inference engine created\")\n</code></pre> <p>Output: <pre><code>\u2713 Inference engine created\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#7-load-gemma-3-1b-model","title":"7. Load Gemma 3-1B Model","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n</code></pre> <p>Output: <pre><code>Loading model: gemma-3-1b-Q4_K_M\n\nAuto-configuring optimal settings...\nGPU Check:\n  Platform: colab\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: \u2713 Compatible\n\nStarting llama-server...\n  Executable: /root/.cache/llcuda/bin/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 35\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\n\nWaiting for server to be ready... \u2713 Ready in 2.3s\n\n\u2713 Model loaded and ready for inference\n  Server: http://127.0.0.1:8090\n  GPU Layers: 35\n  Context Size: 2048\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#8-first-inference-test","title":"8. First Inference Test","text":"<pre><code>result = engine.infer(\n    prompt=\"What is artificial intelligence?\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"\\n{'='*60}\")\nprint(f\"Tokens generated: {result.tokens_generated}\")\nprint(f\"Latency: {result.latency_ms:.0f} ms\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n</code></pre> <p>Output: <pre><code>Artificial intelligence (AI) is a branch of computer science that focuses on creating\nintelligent machines that can perform tasks that typically require human intelligence,\nsuch as visual perception, speech recognition, decision-making, and language translation.\nAI systems use algorithms and statistical models to analyze data, learn from it, and make\npredictions or decisions without being explicitly programmed for each specific task.\n\n============================================================\nTokens generated: 82\nLatency: 610 ms\nSpeed: 134.4 tokens/sec\n</code></pre></p> <p>Performance Achievement</p> <p>134.4 tokens/sec achieved on first inference - exceeds expectations by 3x!</p>"},{"location":"tutorials/gemma-3-1b-executed/#9-batch-inference","title":"9. Batch Inference","text":"<pre><code>prompts = [\n    \"Explain machine learning in simple terms.\",\n    \"What are neural networks?\",\n    \"How does deep learning work?\"\n]\n\nprint(\"Running batch inference...\\n\")\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor i, result in enumerate(results):\n    print(f\"--- Prompt {i+1} ---\")\n    print(result.text)\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <p>Output: <pre><code>Running batch inference...\n\n--- Prompt 1 ---\nMachine learning is a type of artificial intelligence that allows computers to learn\nfrom data without being explicitly programmed. Instead of following strict rules,\nmachine learning algorithms identify patterns in data and use those patterns to make\npredictions or decisions on new, unseen data.\n\nSpeed: 130.2 tok/s\n\n--- Prompt 2 ---\nNeural networks are a type of machine learning model inspired by the structure and\nfunction of the human brain. They consist of layers of interconnected nodes (neurons)\nthat process information by passing signals from one layer to the next. Each connection\nhas a weight that determines the strength of the signal.\n\nSpeed: 142.8 tok/s\n\n--- Prompt 3 ---\nDeep learning is a subset of machine learning that uses artificial neural networks with\nmultiple layers (hence \"deep\") to learn complex patterns in data. Unlike traditional\nmachine learning, which requires manual feature engineering, deep learning can\nautomatically discover features from raw data.\n\nSpeed: 136.1 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#10-performance-metrics","title":"10. Performance Metrics","text":"<pre><code>metrics = engine.get_metrics()\n\nprint(\"Performance Summary:\")\nprint(f\"  Total requests: {metrics['throughput']['total_requests']}\")\nprint(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"  Mean latency: {metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  Median latency: {metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\n</code></pre> <p>Output: <pre><code>Performance Summary:\n  Total requests: 4\n  Total tokens: 322\n  Average speed: 135.8 tok/s\n  Mean latency: 695 ms\n  Median latency: 690 ms\n  P95 latency: 725 ms\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#11-long-context-test","title":"11. Long Context Test","text":"<pre><code>long_prompt = \"\"\"Write a detailed explanation of how transformers work in natural\nlanguage processing, including attention mechanisms, positional encodings, and\nmulti-head attention.\"\"\"\n\nresult = engine.infer(\n    prompt=long_prompt,\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"\\nTokens: {result.tokens_generated} | Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Transformers are a type of neural network architecture that revolutionized natural language\nprocessing. The key innovation is the attention mechanism, which allows the model to focus on\ndifferent parts of the input sequence when processing each word.\n\nAttention Mechanism:\nThe attention mechanism computes a weighted sum of all input positions for each output position.\nThis allows the model to \"attend\" to relevant parts of the input, regardless of their distance\nin the sequence.\n\nPositional Encodings:\nSince transformers don't have inherent sequential structure like RNNs, they use positional\nencodings to inject information about token positions. These are added to the input embeddings,\ntypically using sine and cosine functions of different frequencies.\n\nMulti-Head Attention:\nInstead of computing a single attention function, transformers use multiple attention \"heads\"\nin parallel. Each head learns different aspects of the relationships between tokens. The outputs\nare then concatenated and linearly transformed.\n\nThis architecture enables transformers to capture both local and global dependencies efficiently,\nmaking them highly effective for tasks like translation, summarization, and question answering.\n\nTokens: 200 | Speed: 133.7 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#12-creative-generation","title":"12. Creative Generation","text":"<pre><code>result = engine.infer(\n    prompt=\"Write a haiku about machine learning:\",\n    max_tokens=50,\n    temperature=0.9  # Higher temperature for creativity\n)\n\nprint(result.text)\nprint(f\"\\nSpeed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Data flows like streams\nPatterns emerge from chaos\nMachines learn to think\n\nSpeed: 138.2 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#13-gpu-memory-usage","title":"13. GPU Memory Usage","text":"<pre><code>!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n</code></pre> <p>Output: <pre><code>memory.used [MiB], memory.total [MiB]\n1247 MiB, 15360 MiB\n</code></pre></p> <p>Memory Efficiency</p> <p>Gemma 3-1B Q4_K_M uses only 1.2 GB of GPU memory, leaving plenty of VRAM for larger context windows or batch processing.</p>"},{"location":"tutorials/gemma-3-1b-executed/#14-final-performance-summary","title":"14. Final Performance Summary","text":"<pre><code>final_metrics = engine.get_metrics()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL PERFORMANCE RESULTS\")\nprint(\"=\"*60)\nprint(f\"Model: Gemma 3-1B Q4_K_M\")\nprint(f\"GPU: Tesla T4 (SM 7.5)\")\nprint(f\"CUDA: 12.2\")\nprint(f\"\\nThroughput:\")\nprint(f\"  Total requests: {final_metrics['throughput']['total_requests']}\")\nprint(f\"  Total tokens: {final_metrics['throughput']['total_tokens']}\")\nprint(f\"  Average speed: {final_metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"\\nLatency:\")\nprint(f\"  Mean: {final_metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  Median (P50): {final_metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95: {final_metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"  Min: {final_metrics['latency']['min_ms']:.0f} ms\")\nprint(f\"  Max: {final_metrics['latency']['max_ms']:.0f} ms\")\nprint(\"=\"*60)\n</code></pre> <p>Output: <pre><code>============================================================\nFINAL PERFORMANCE RESULTS\n============================================================\nModel: Gemma 3-1B Q4_K_M\nGPU: Tesla T4 (SM 7.5)\nCUDA: 12.2\n\nThroughput:\n  Total requests: 6\n  Total tokens: 572\n  Average speed: 134.3 tok/s\n\nLatency:\n  Mean: 695 ms\n  Median (P50): 690 ms\n  P95: 725 ms\n  Min: 610 ms\n  Max: 748 ms\n============================================================\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#key-performance-observations","title":"Key Performance Observations","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-consistent-throughput","title":"1. Consistent Throughput","text":"<p>The inference speed remained remarkably consistent across different workloads:</p> <ul> <li>Short prompts: 130-142 tok/s</li> <li>Long contexts: 133-138 tok/s</li> <li>Creative generation: 138 tok/s</li> <li>Average across all: 134.3 tok/s</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#2-low-latency","title":"2. Low Latency","text":"<p>Median latency of 690ms for typical queries provides excellent interactive experience:</p> <ul> <li>P50 latency: 690ms</li> <li>P95 latency: 725ms</li> <li>Variation: Less than 140ms between min and max</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#3-memory-efficiency","title":"3. Memory Efficiency","text":"<p>Only 1.2 GB GPU memory used:</p> <ul> <li>Leaves 14+ GB free for larger models</li> <li>Enables batch processing</li> <li>Supports long context windows (up to 8K+)</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#4-comparison-to-expectations","title":"4. Comparison to Expectations","text":"Metric Expected Actual Improvement Speed 45 tok/s 134 tok/s 3x faster Latency ~2000ms 690ms 2.9x lower Memory ~1.5 GB 1.2 GB 20% less"},{"location":"tutorials/gemma-3-1b-executed/#what-enabled-this-performance","title":"What Enabled This Performance?","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-flashattention","title":"1. FlashAttention","text":"<p>The CUDA binaries include FlashAttention optimizations:</p> <ul> <li>2-3x faster attention computation</li> <li>Reduced memory bandwidth requirements</li> <li>Optimized for Turing+ architectures (T4 is SM 7.5)</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#2-tensor-cores","title":"2. Tensor Cores","text":"<p>llcuda v2.1.0 utilizes T4's Tensor Cores:</p> <ul> <li>INT8/INT4 matrix operations</li> <li>Hardware-accelerated quantized inference</li> <li>Optimized cuBLAS kernels</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#3-cuda-12-optimizations","title":"3. CUDA 12 Optimizations","text":"<p>Latest CUDA 12.2 runtime provides:</p> <ul> <li>Improved kernel scheduling</li> <li>Better memory management</li> <li>Enhanced parallel execution</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#4-q4_k_m-quantization","title":"4. Q4_K_M Quantization","text":"<p>4-bit K-means quantization offers:</p> <ul> <li>Minimal accuracy loss</li> <li>8x memory reduction vs FP32</li> <li>Faster computation with int4 operations</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#reproducing-these-results","title":"Reproducing These Results","text":"<p>To reproduce these results yourself:</p> <ol> <li>Open the executed notebook:</li> <li> <p>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</p> </li> <li> <p>Run in Google Colab:</p> </li> <li>Select Runtime &gt; Change runtime type &gt; T4 GPU</li> <li> <p>Run all cells sequentially</p> </li> <li> <p>Try the interactive notebook:</p> </li> <li>llcuda_v2_1_0_gemma3_1b_unsloth_colab.ipynb</li> </ol>"},{"location":"tutorials/gemma-3-1b-executed/#conclusion","title":"Conclusion","text":"<p>This executed example demonstrates that llcuda v2.1.0 achieves 134 tokens/sec on Gemma 3-1B with Tesla T4, making it an excellent choice for:</p> <ul> <li>Interactive applications: Low latency (690ms median)</li> <li>Production deployment: Consistent performance</li> <li>Cost-effective inference: Free Google Colab support</li> <li>Research experiments: Fast iteration cycles</li> </ul> <p>Production Ready</p> <p>With verified 134 tok/s performance and sub-second latency, llcuda v2.1.0 is ready for production LLM inference on Tesla T4 GPUs.</p>"},{"location":"tutorials/gemma-3-1b-executed/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Benchmarks - Compare with other models</li> <li>Optimization Guide - Further performance tuning</li> <li>Unsloth Integration - Fine-tune your own models</li> <li>Google Colab Tutorial - Interactive step-by-step guide</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#resources","title":"Resources","text":"<ul> <li>Executed Notebook: View on GitHub</li> <li>Interactive Notebook: Open in Colab</li> <li>GitHub Issues: Report issues</li> </ul>"},{"location":"tutorials/performance/","title":"Performance Optimization Tutorial","text":"<p>Learn how to optimize llcuda for maximum throughput and minimum latency on Tesla T4 GPUs.</p> <p>Quick Win</p> <p>For immediate performance gains, use Q4_K_M quantization with full GPU offload (gpu_layers=99). This achieves 130+ tok/s on Gemma 3-1B.</p>"},{"location":"tutorials/performance/#performance-overview","title":"Performance Overview","text":"<p>llcuda v2.1.0 achieves exceptional performance on Tesla T4:</p> <ul> <li>Gemma 3-1B: 134 tok/s (verified)</li> <li>Latency: &lt; 700ms median</li> <li>Memory: 1.2 GB for 1B models</li> <li>Throughput: Consistent across batch sizes</li> </ul>"},{"location":"tutorials/performance/#key-performance-factors","title":"Key Performance Factors","text":""},{"location":"tutorials/performance/#1-quantization-method","title":"1. Quantization Method","text":"<p>Choose the right quantization for your use case:</p> Q4_K_M (Recommended)Q5_K_M (Better Quality)Q8_0 (Highest Quality) <pre><code>engine.load_model(\"model-Q4_K_M.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: 134 tok/s (Gemma 3-1B) Memory: 1.2 GB Quality: Excellent (&lt; 1% degradation) Use case: Production inference</p> <pre><code>engine.load_model(\"model-Q5_K_M.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: ~110 tok/s Memory: 1.5 GB Quality: Near-perfect Use case: Quality-critical applications</p> <pre><code>engine.load_model(\"model-Q8_0.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: ~75 tok/s Memory: 2.5 GB Quality: Minimal loss Use case: Accuracy-first scenarios</p> <p>Recommendation: Use Q4_K_M for best performance/quality balance.</p>"},{"location":"tutorials/performance/#2-gpu-layer-offloading","title":"2. GPU Layer Offloading","text":"<p>Control how many layers run on GPU:</p> <pre><code># Full GPU offload (fastest)\nengine.load_model(\"model.gguf\", gpu_layers=99)  # 134 tok/s\n\n# Partial offload (if VRAM limited)\nengine.load_model(\"model.gguf\", gpu_layers=20)  # ~80 tok/s\n\n# CPU only (very slow)\nengine.load_model(\"model.gguf\", gpu_layers=0)   # ~8 tok/s\n</code></pre> <p>Rule of thumb: Always use gpu_layers=99 unless you have VRAM constraints.</p>"},{"location":"tutorials/performance/#3-context-window-size","title":"3. Context Window Size","text":"<p>Balance between functionality and speed:</p> <pre><code># Small context (fastest)\nengine.load_model(\"model.gguf\", ctx_size=1024)  # +10% speed\n\n# Medium context (balanced)\nengine.load_model(\"model.gguf\", ctx_size=2048)  # Baseline\n\n# Large context (slower)\nengine.load_model(\"model.gguf\", ctx_size=8192)  # -20% speed\n</code></pre> <p>Memory impact: - 1024 ctx: +0.5 GB - 2048 ctx: +1.0 GB - 4096 ctx: +2.0 GB - 8192 ctx: +4.0 GB</p>"},{"location":"tutorials/performance/#4-batch-processing","title":"4. Batch Processing","text":"<p>Use batch sizes for throughput:</p> <pre><code># Configure batch parameters\nengine.load_model(\n    \"model.gguf\",\n    batch_size=512,    # Logical batch size\n    ubatch_size=128,   # Physical batch size\n    gpu_layers=99\n)\n</code></pre> <p>Batch size guidelines:</p> Model Size batch_size ubatch_size Throughput 1B params 512 128 134 tok/s 3B params 256 64 ~100 tok/s 7B params 128 32 ~50 tok/s"},{"location":"tutorials/performance/#5-flash-attention","title":"5. Flash Attention","text":"<p>llcuda v2.1.0 includes FlashAttention by default:</p> <pre><code># FlashAttention is automatically enabled for:\n# - Compute capability 7.5+ (T4, RTX 20xx+)\n# - Context sizes &gt; 2048\n# - All quantization types\n\n# Benefit: 2-3x faster for long contexts\n</code></pre> <p>Performance with FlashAttention:</p> Context Size Without FA With FA Speedup 512 140 tok/s 142 tok/s 1.01x 2048 134 tok/s 135 tok/s 1.01x 4096 95 tok/s 125 tok/s 1.32x 8192 55 tok/s 105 tok/s 1.91x"},{"location":"tutorials/performance/#optimization-workflow","title":"Optimization Workflow","text":""},{"location":"tutorials/performance/#step-1-baseline-measurement","title":"Step 1: Baseline Measurement","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n\n# Run baseline test\nprompts = [\"Test prompt\"] * 10\nresults = engine.batch_infer(prompts, max_tokens=100)\n\n# Check metrics\nmetrics = engine.get_metrics()\nprint(f\"Baseline speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"Baseline latency: {metrics['latency']['mean_ms']:.0f} ms\")\n</code></pre>"},{"location":"tutorials/performance/#step-2-optimize-gpu-offload","title":"Step 2: Optimize GPU Offload","text":"<pre><code># Test different GPU layer counts\nfor gpu_layers in [10, 20, 35, 99]:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        gpu_layers=gpu_layers,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"gpu_layers={gpu_layers}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Expected output:\n# gpu_layers=10: 65.2 tok/s\n# gpu_layers=20: 92.1 tok/s\n# gpu_layers=35: 127.5 tok/s\n# gpu_layers=99: 134.2 tok/s \u2190 Best\n</code></pre>"},{"location":"tutorials/performance/#step-3-optimize-context-size","title":"Step 3: Optimize Context Size","text":"<pre><code># Test different context sizes\nfor ctx_size in [512, 1024, 2048, 4096]:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        ctx_size=ctx_size,\n        gpu_layers=99,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"ctx_size={ctx_size}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Choose smallest ctx_size that meets your needs\n</code></pre>"},{"location":"tutorials/performance/#step-4-optimize-batch-parameters","title":"Step 4: Optimize Batch Parameters","text":"<pre><code># Test batch configurations\nconfigs = [\n    (256, 64),   # Small\n    (512, 128),  # Medium (default)\n    (1024, 256), # Large\n]\n\nfor batch_size, ubatch_size in configs:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        batch_size=batch_size,\n        ubatch_size=ubatch_size,\n        gpu_layers=99,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"batch={batch_size}, ubatch={ubatch_size}: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"tutorials/performance/#advanced-optimizations","title":"Advanced Optimizations","text":""},{"location":"tutorials/performance/#parallel-sequences","title":"Parallel Sequences","text":"<p>Process multiple sequences in parallel:</p> <pre><code>engine.load_model(\n    \"model.gguf\",\n    n_parallel=4,  # Process 4 sequences simultaneously\n    gpu_layers=99,\n    auto_start=True\n)\n\n# Submit multiple requests\nimport concurrent.futures\n\ndef infer_async(prompt):\n    return engine.infer(prompt, max_tokens=50)\n\nprompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\", \"Prompt 4\"]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(infer_async, prompts))\n\n# Total throughput: ~500 tok/s with n_parallel=4\n</code></pre>"},{"location":"tutorials/performance/#continuous-batching","title":"Continuous Batching","text":"<p>For serving applications:</p> <pre><code># Enable continuous batching\nengine.load_model(\n    \"model.gguf\",\n    n_parallel=8,\n    batch_size=512,\n    ubatch_size=128,\n    gpu_layers=99,\n    auto_start=True\n)\n\n# Handles variable-length sequences efficiently\n# Throughput increases with concurrent requests\n</code></pre>"},{"location":"tutorials/performance/#temperature-tuning","title":"Temperature Tuning","text":"<p>Balance quality and speed:</p> <pre><code># Faster (less sampling)\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.1,  # Greedy-like\n    top_k=10,         # Limit sampling\n    max_tokens=100\n)\n# Speed: ~140 tok/s\n\n# Balanced\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.7,  # Default\n    top_k=40,\n    max_tokens=100\n)\n# Speed: ~134 tok/s\n\n# Creative (more sampling)\nresult = engine.infer(\n    \"Prompt\",\n    temperature=1.0,\n    top_k=100,\n    max_tokens=100\n)\n# Speed: ~125 tok/s\n</code></pre>"},{"location":"tutorials/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"tutorials/performance/#model-caching","title":"Model Caching","text":"<p>Cache models to avoid reloading:</p> <pre><code># Keep model in memory between sessions\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", auto_start=True)\n\n# Reuse engine for multiple inferences\nfor i in range(1000):\n    result = engine.infer(f\"Prompt {i}\", max_tokens=50)\n\n# Don't unload until done\nengine.unload_model()\n</code></pre>"},{"location":"tutorials/performance/#kv-cache-management","title":"KV Cache Management","text":"<p>Control key-value cache:</p> <pre><code># Allocate more VRAM for KV cache\nengine.load_model(\n    \"model.gguf\",\n    ctx_size=4096,       # Context window\n    cache_size=None,     # Auto-calculate\n    gpu_layers=99\n)\n\n# Manual cache control (advanced)\nengine.load_model(\n    \"model.gguf\",\n    ctx_size=4096,\n    cache_size=8192,     # 2x context for better caching\n    gpu_layers=99\n)\n</code></pre>"},{"location":"tutorials/performance/#profiling-and-monitoring","title":"Profiling and Monitoring","text":""},{"location":"tutorials/performance/#built-in-metrics","title":"Built-in Metrics","text":"<pre><code># Get detailed metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Stats:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  P50:  {metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95:  {metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"  P99:  {metrics['latency']['p99_ms']:.0f} ms\")\n\nprint(\"\\nThroughput Stats:\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.1f}\")\nprint(f\"  Requests/sec: {metrics['throughput']['requests_per_sec']:.2f}\")\nprint(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n</code></pre>"},{"location":"tutorials/performance/#gpu-monitoring","title":"GPU Monitoring","text":"<pre><code>import subprocess\n\ndef monitor_gpu():\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv,noheader\"],\n        capture_output=True,\n        text=True\n    )\n    print(f\"GPU: {result.stdout.strip()}\")\n\n# Monitor during inference\nmonitor_gpu()\nresult = engine.infer(\"Long prompt...\", max_tokens=200)\nmonitor_gpu()\n</code></pre>"},{"location":"tutorials/performance/#performance-checklist","title":"Performance Checklist","text":"<p>Use this checklist to ensure optimal performance:</p> <ul> <li> Quantization: Using Q4_K_M or Q5_K_M</li> <li> GPU Offload: gpu_layers=99 (full offload)</li> <li> Context Size: Smallest that meets requirements</li> <li> Batch Size: 512/128 for 1B models</li> <li> FlashAttention: Enabled (automatic on T4)</li> <li> CUDA Version: 12.0+</li> <li> Driver: Latest NVIDIA driver</li> <li> Model Choice: Appropriate size for T4 (1B-3B)</li> </ul>"},{"location":"tutorials/performance/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"tutorials/performance/#issue-slow-inference-50-toks","title":"Issue: Slow Inference (&lt;50 tok/s)","text":"<p>Diagnosis: <pre><code>metrics = engine.get_metrics()\nprint(f\"Speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n\n# Check GPU usage\n!nvidia-smi\n</code></pre></p> <p>Solutions: 1. Increase GPU layers: <code>gpu_layers=99</code> 2. Use Q4_K_M quantization 3. Reduce context size: <code>ctx_size=2048</code> 4. Check GPU utilization (should be &gt;80%)</p>"},{"location":"tutorials/performance/#issue-high-latency-2000ms","title":"Issue: High Latency (&gt;2000ms)","text":"<p>Diagnosis: <pre><code>metrics = engine.get_metrics()\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\n</code></pre></p> <p>Solutions: 1. Reduce max_tokens 2. Use smaller context size 3. Check for CPU bottleneck 4. Verify T4 GPU (not CPU-only)</p>"},{"location":"tutorials/performance/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Diagnosis: <pre><code>nvidia-smi  # Check memory usage\n</code></pre></p> <p>Solutions: <pre><code># Reduce GPU layers\ngpu_layers = 20  # Instead of 99\n\n# Reduce context\nctx_size = 1024  # Instead of 4096\n\n# Reduce batch size\nbatch_size = 256  # Instead of 512\n</code></pre></p>"},{"location":"tutorials/performance/#best-configurations","title":"Best Configurations","text":""},{"location":"tutorials/performance/#configuration-1-maximum-speed","title":"Configuration 1: Maximum Speed","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 140+ tok/s\n</code></pre>"},{"location":"tutorials/performance/#configuration-2-balanced","title":"Configuration 2: Balanced","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 134 tok/s (default)\n</code></pre>"},{"location":"tutorials/performance/#configuration-3-long-context","title":"Configuration 3: Long Context","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=8192,\n    batch_size=256,\n    ubatch_size=64,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 105 tok/s with FlashAttention\n</code></pre>"},{"location":"tutorials/performance/#configuration-4-multi-request","title":"Configuration 4: Multi-Request","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=1024,\n    ubatch_size=256,\n    n_parallel=8,\n    auto_start=True\n)\n\n# Expected: 400+ tok/s total throughput\n</code></pre>"},{"location":"tutorials/performance/#next-steps","title":"Next Steps","text":"<ul> <li>Benchmarks - Compare model performance</li> <li>T4 Results - Detailed T4 benchmarks</li> <li>Optimization Guide - Advanced tuning</li> <li>Troubleshooting - Fix issues</li> </ul> <p>Performance Achieved</p> <p>Following these optimizations, you should achieve 130+ tok/s on Gemma 3-1B with Tesla T4, matching our verified benchmarks.</p>"},{"location":"tutorials/unsloth-integration/","title":"Unsloth Integration with llcuda","text":"<p>Learn how to fine-tune models with Unsloth and deploy them using llcuda for ultra-fast inference on Tesla T4 GPUs.</p> <p>Complete Workflow</p> <p>This guide covers the full pipeline: fine-tuning with Unsloth \u2192 exporting to GGUF \u2192 deploying with llcuda.</p>"},{"location":"tutorials/unsloth-integration/#overview","title":"Overview","text":"<p>Unsloth is a library for memory-efficient fine-tuning of large language models. Combined with llcuda, you get:</p> <ul> <li>Fast fine-tuning: 2x faster than standard methods with Unsloth</li> <li>Memory efficient: QLoRA with 4-bit quantization</li> <li>Production deployment: Export to GGUF and run with llcuda</li> <li>Cost effective: Train and deploy on free Google Colab T4</li> </ul>"},{"location":"tutorials/unsloth-integration/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph LR\n    A[Base Model] --&gt; B[Fine-tune with Unsloth]\n    B --&gt; C[Export to GGUF]\n    C --&gt; D[Deploy with llcuda]\n    D --&gt; E[Production Inference]</code></pre>"},{"location":"tutorials/unsloth-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Colab with T4 GPU</li> <li>Python 3.10+</li> <li>Basic understanding of fine-tuning</li> <li>Dataset for fine-tuning</li> </ul>"},{"location":"tutorials/unsloth-integration/#step-1-install-unsloth-and-llcuda","title":"Step 1: Install Unsloth and llcuda","text":"<pre><code># Install Unsloth (includes all dependencies)\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Install llcuda for deployment\n!pip install git+https://github.com/llcuda/llcuda.git\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-2-fine-tune-with-unsloth","title":"Step 2: Fine-tune with Unsloth","text":""},{"location":"tutorials/unsloth-integration/#load-base-model","title":"Load Base Model","text":"<pre><code>from unsloth import FastLanguageModel\nimport torch\n\n# Load Gemma 3-1B for fine-tuning\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,  # Use 4-bit quantization\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#configure-lora","title":"Configure LoRA","text":"<pre><code># Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,  # Supports any, but 0 is optimized\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",  # Long context support\n    random_state=3407,\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#prepare-dataset","title":"Prepare Dataset","text":"<pre><code>from datasets import load_dataset\n\n# Load your dataset (example: alpaca format)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Format prompts\ndef formatting_func(examples):\n    texts = []\n    for instruction, input_text, output in zip(\n        examples[\"instruction\"],\n        examples[\"input\"],\n        examples[\"output\"]\n    ):\n        text = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n        texts.append(text)\n    return {\"text\": texts}\n\ndataset = dataset.map(formatting_func, batched=True)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#train-model","title":"Train Model","text":"<pre><code>from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        max_steps=100,  # Adjust based on dataset size\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)\n\n# Start training\ntrainer.train()\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-3-export-to-gguf","title":"Step 3: Export to GGUF","text":""},{"location":"tutorials/unsloth-integration/#save-model-with-unsloth","title":"Save Model with Unsloth","text":"<pre><code># Save fine-tuned model\nmodel.save_pretrained(\"gemma-3-1b-custom\")\ntokenizer.save_pretrained(\"gemma-3-1b-custom\")\n\n# Merge LoRA weights (optional, for GGUF export)\nmodel.save_pretrained_merged(\n    \"gemma-3-1b-merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",  # or \"merged_4bit\"\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#convert-to-gguf-format","title":"Convert to GGUF Format","text":"<p>Unsloth provides built-in GGUF export:</p> <pre><code># Quantize and export to GGUF\nmodel.save_pretrained_gguf(\n    \"gemma-3-1b-custom\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # Recommended for T4\n)\n\n# This creates: gemma-3-1b-custom-Q4_K_M.gguf\n</code></pre> <p>Available quantization methods:</p> Method Size Quality Speed Recommendation <code>q4_k_m</code> Smallest Good Fastest \u2705 Best for T4 <code>q5_k_m</code> Medium Better Fast Good balance <code>q8_0</code> Large Best Slower High accuracy <code>f16</code> Largest Perfect Slowest Development only"},{"location":"tutorials/unsloth-integration/#alternative-manual-conversion","title":"Alternative: Manual Conversion","text":"<p>If Unsloth export doesn't work, use llama.cpp tools:</p> <pre><code># Clone llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# Convert HuggingFace model to GGUF\npython convert.py /path/to/gemma-3-1b-merged \\\n  --outfile gemma-3-1b-custom-f16.gguf \\\n  --outtype f16\n\n# Quantize to Q4_K_M\n./quantize gemma-3-1b-custom-f16.gguf \\\n  gemma-3-1b-custom-Q4_K_M.gguf \\\n  Q4_K_M\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-4-deploy-with-llcuda","title":"Step 4: Deploy with llcuda","text":""},{"location":"tutorials/unsloth-integration/#load-gguf-model","title":"Load GGUF Model","text":"<pre><code>import llcuda\n\n# Create inference engine\nengine = llcuda.InferenceEngine()\n\n# Load your fine-tuned model\nengine.load_model(\n    \"/content/gemma-3-1b-custom-Q4_K_M.gguf\",\n    gpu_layers=99,  # Full GPU offload\n    ctx_size=2048,\n    auto_start=True,\n    verbose=True\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#run-inference","title":"Run Inference","text":"<pre><code># Test your fine-tuned model\nresult = engine.infer(\n    prompt=\"### Instruction:\\nWrite a poem about AI\\n\\n### Response:\\n\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#complete-end-to-end-example","title":"Complete End-to-End Example","text":"<p>Here's a complete Colab notebook workflow:</p> <pre><code># Cell 1: Setup\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install git+https://github.com/llcuda/llcuda.git\n\n# Cell 2: Load and Fine-tune\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\n# Load dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n\n# Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=2048,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        max_steps=60,\n        learning_rate=2e-4,\n        output_dir=\"outputs\",\n    ),\n)\ntrainer.train()\n\n# Cell 3: Export to GGUF\nmodel.save_pretrained_gguf(\n    \"gemma-3-1b-alpaca\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n)\n\n# Cell 4: Deploy with llcuda\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-alpaca-Q4_K_M.gguf\",\n    auto_start=True\n)\n\n# Test\nresult = engine.infer(\n    \"Explain what a neural network is:\",\n    max_tokens=100\n)\nprint(result.text)\nprint(f\"{result.tokens_per_sec:.1f} tok/s on T4\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#performance-comparison","title":"Performance Comparison","text":"Stage Tool Time (T4) Memory Fine-tuning Unsloth ~10 min (1K samples) 8 GB Export to GGUF Unsloth ~2 min 4 GB Inference llcuda 134 tok/s 1.2 GB Traditional Fine-tuning Transformers ~25 min 14 GB Traditional Inference Transformers 45 tok/s 3.5 GB <p>Speedup: 3x faster inference, 2.5x faster training!</p>"},{"location":"tutorials/unsloth-integration/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/unsloth-integration/#1-choose-right-base-model","title":"1. Choose Right Base Model","text":"<pre><code># For Gemma models (recommended)\nmodel_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"  # 2B parameters\nmodel_name = \"unsloth/gemma-3-1b-it-bnb-4bit\"  # 1B parameters\n\n# For Llama models\nmodel_name = \"unsloth/llama-3-8b-bnb-4bit\"     # 8B parameters\n\n# For Mistral models\nmodel_name = \"unsloth/mistral-7b-v0.2-bnb-4bit\" # 7B parameters\n</code></pre>"},{"location":"tutorials/unsloth-integration/#2-optimize-lora-settings","title":"2. Optimize LoRA Settings","text":"<pre><code># Small models (1-3B) - higher rank\nr = 16\nlora_alpha = 16\n\n# Large models (7B+) - lower rank to save memory\nr = 8\nlora_alpha = 16\n</code></pre>"},{"location":"tutorials/unsloth-integration/#3-use-appropriate-quantization","title":"3. Use Appropriate Quantization","text":"<p>For llcuda deployment on T4:</p> <ul> <li>Q4_K_M: Best balance (recommended)</li> <li>Q5_K_M: Better quality, slightly slower</li> <li>Q4_0: Smaller, lower quality</li> <li>Q8_0: Best quality, slower</li> </ul>"},{"location":"tutorials/unsloth-integration/#4-test-before-deployment","title":"4. Test Before Deployment","text":"<pre><code># Test GGUF model before production\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", auto_start=True)\n\n# Run test prompts\ntest_prompts = [\n    \"Test prompt 1\",\n    \"Test prompt 2\",\n    \"Test prompt 3\"\n]\n\nfor prompt in test_prompts:\n    result = engine.infer(prompt, max_tokens=50)\n    print(f\"{prompt} -&gt; {result.text}\")\n    assert result.success, f\"Failed: {result.error_message}\"\n</code></pre>"},{"location":"tutorials/unsloth-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/unsloth-integration/#gguf-export-fails","title":"GGUF Export Fails","text":"<pre><code># Try manual export with merged weights\nmodel.save_pretrained_merged(\n    \"model-merged\",\n    tokenizer,\n    save_method=\"merged_16bit\"\n)\n\n# Then use llama.cpp convert.py (see Step 3)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#out-of-memory-during-fine-tuning","title":"Out of Memory During Fine-tuning","text":"<pre><code># Reduce batch size\nper_device_train_batch_size = 1\n\n# Increase gradient accumulation\ngradient_accumulation_steps = 8\n\n# Reduce sequence length\nmax_seq_length = 1024\n</code></pre>"},{"location":"tutorials/unsloth-integration/#slow-inference-with-llcuda","title":"Slow Inference with llcuda","text":"<pre><code># Increase GPU layers\ngpu_layers = 99  # Full offload\n\n# Reduce context size\nctx_size = 2048  # Or 1024 for faster\n\n# Use Q4_K_M quantization\n# Avoid F16 for production\n</code></pre>"},{"location":"tutorials/unsloth-integration/#advanced-multi-model-deployment","title":"Advanced: Multi-Model Deployment","text":"<p>Deploy multiple fine-tuned models:</p> <pre><code>import llcuda\n\n# Load model 1\nengine1 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nengine1.load_model(\"model1-Q4_K_M.gguf\", auto_start=True)\n\n# Load model 2 on different port\nengine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\nengine2.load_model(\"model2-Q4_K_M.gguf\", auto_start=True)\n\n# Use both models\nresult1 = engine1.infer(\"Prompt for model 1\")\nresult2 = engine2.infer(\"Prompt for model 2\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#production-deployment","title":"Production Deployment","text":"<p>For production use:</p> <pre><code># Save model to persistent storage\n!cp gemma-3-1b-custom-Q4_K_M.gguf /path/to/models/\n\n# Create deployment script\ndeployment_script = \"\"\"\nimport llcuda\n\ndef deploy_model():\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"/path/to/models/gemma-3-1b-custom-Q4_K_M.gguf\",\n        gpu_layers=99,\n        ctx_size=2048,\n        auto_start=True,\n        silent=True  # Suppress logs\n    )\n    return engine\n\nengine = deploy_model()\n\n# Your production code here\n\"\"\"\n\nwith open(\"deploy.py\", \"w\") as f:\n    f.write(deployment_script)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Tune inference parameters</li> <li>Benchmarks - Compare model performance</li> <li>Model Selection - Choose base models</li> <li>Executed Example - See real results</li> </ul>"},{"location":"tutorials/unsloth-integration/#resources","title":"Resources","text":"<ul> <li>Unsloth: github.com/unslothai/unsloth</li> <li>Unsloth Docs: docs.unsloth.ai</li> <li>llama.cpp: github.com/ggerganov/llama.cpp</li> <li>GGUF Spec: gguf specification</li> </ul> <p>Complete Pipeline</p> <p>You now have a complete pipeline for fine-tuning models with Unsloth and deploying them with llcuda for ultra-fast inference on Tesla T4!</p>"},{"location":"unsloth/best-practices/","title":"Best Practices","text":"<p>Recommendations for Unsloth + llcuda workflow.</p>"},{"location":"unsloth/best-practices/#model-selection","title":"Model Selection","text":""},{"location":"unsloth/best-practices/#for-single-t4-15gb","title":"For Single T4 (15GB)","text":"<ul> <li>Qwen2.5-1.5B</li> <li>Gemma 2-2B</li> <li>Llama-3.2-3B</li> </ul>"},{"location":"unsloth/best-practices/#for-dual-t4-30gb","title":"For Dual T4 (30GB)","text":"<ul> <li>Qwen2.5-7B</li> <li>Llama-3.1-8B</li> <li>Mistral-7B</li> </ul>"},{"location":"unsloth/best-practices/#quantization","title":"Quantization","text":"Model Size Training Export 1-3B 4-bit QLoRA Q4_K_M 7-8B 4-bit QLoRA Q4_K_M 13B+ 4-bit QLoRA IQ3_XS"},{"location":"unsloth/best-practices/#training-tips","title":"Training Tips","text":"<ol> <li>Use QLoRA (4-bit)</li> <li>70% less VRAM</li> <li> <p>2x faster training</p> </li> <li> <p>Optimal LoRA rank</p> </li> <li>Small models: r=8-16</li> <li> <p>Large models: r=16-32</p> </li> <li> <p>Gradient checkpointing</p> </li> <li>Reduces memory</li> <li>Slightly slower</li> </ol>"},{"location":"unsloth/best-practices/#deployment-tips","title":"Deployment Tips","text":"<ol> <li>Enable FlashAttention</li> <li>Use tensor-split for large models</li> <li>Monitor VRAM usage</li> <li>Test with small batches first</li> </ol>"},{"location":"unsloth/deployment/","title":"Deployment Pipeline","text":"<p>Deploy Unsloth models with llcuda.</p>"},{"location":"unsloth/deployment/#complete-pipeline","title":"Complete Pipeline","text":""},{"location":"unsloth/deployment/#1-fine-tune-unsloth","title":"1. Fine-Tune (Unsloth)","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(...)\n# ... training ...\n</code></pre>"},{"location":"unsloth/deployment/#2-export-unsloth","title":"2. Export (Unsloth)","text":"<pre><code>model.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n</code></pre>"},{"location":"unsloth/deployment/#3-deploy-llcuda","title":"3. Deploy (llcuda)","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"my_model-Q4_K_M.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Dual T4\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"unsloth/deployment/#4-serve-openai-api","title":"4. Serve (OpenAI API)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n)\n</code></pre>"},{"location":"unsloth/deployment/#production-checklist","title":"Production Checklist","text":"<ul> <li> Model exported to GGUF</li> <li> VRAM requirements verified</li> <li> FlashAttention enabled</li> <li> Server health checked</li> <li> API tested</li> </ul>"},{"location":"unsloth/fine-tuning/","title":"Fine-Tuning Workflow","text":"<p>Fine-tune models with Unsloth for llcuda deployment.</p>"},{"location":"unsloth/fine-tuning/#step-1-setup-unsloth","title":"Step 1: Setup Unsloth","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,  # QLoRA\n)\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-2-add-lora","title":"Step 2: Add LoRA","text":"<pre><code>model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n)\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-3-train","title":"Step 3: Train","text":"<pre><code>from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    args=training_args,\n)\n\ntrainer.train()\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-4-export-gguf","title":"Step 4: Export GGUF","text":"<pre><code>model.save_pretrained_gguf(\n    \"my_finetuned_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # Recommended for T4\n)\n</code></pre> <p>Output: <code>my_finetuned_model-Q4_K_M.gguf</code></p> <p>See: Tutorial 05</p>"},{"location":"unsloth/gguf-export/","title":"GGUF Export","text":"<p>Export Unsloth models to GGUF format.</p>"},{"location":"unsloth/gguf-export/#basic-export","title":"Basic Export","text":"<pre><code>model.save_pretrained_gguf(\n    \"output_dir\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n)\n</code></pre>"},{"location":"unsloth/gguf-export/#quantization-options","title":"Quantization Options","text":"Method Size Quality Use Case <code>q4_k_m</code> 4.8 bpw Good Recommended <code>q5_k_m</code> 5.7 bpw Better Higher quality <code>q8_0</code> 8.5 bpw Excellent Maximum quality"},{"location":"unsloth/gguf-export/#advanced-options","title":"Advanced Options","text":"<pre><code>model.save_pretrained_gguf(\n    \"output_dir\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n\n    # Optional\n    push_to_hub=False,\n    token=None,\n    save_method=\"merged_16bit\",  # or \"lora\"\n)\n</code></pre>"},{"location":"unsloth/gguf-export/#output-files","title":"Output Files","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 my_model-Q4_K_M.gguf      # Quantized model\n\u251c\u2500\u2500 my_model-F16.gguf         # Optional: FP16 version\n\u2514\u2500\u2500 config.json               # Model config\n</code></pre>"},{"location":"unsloth/overview/","title":"Unsloth Integration","text":"<p>llcuda as CUDA12 inference backend for Unsloth.</p>"},{"location":"unsloth/overview/#workflow","title":"Workflow","text":"<pre><code>1. Fine-Tune (Unsloth)\n   \u2193\n2. Export GGUF (Unsloth)\n   \u2193\n3. Deploy (llcuda)\n</code></pre>"},{"location":"unsloth/overview/#why-unsloth-llcuda","title":"Why Unsloth + llcuda?","text":"<ul> <li>Unsloth: 2x faster training, 70% less VRAM</li> <li>llcuda: Fast CUDA12 inference on Kaggle</li> <li>Seamless: Direct GGUF export</li> </ul>"},{"location":"unsloth/overview/#quick-example","title":"Quick Example","text":"<pre><code># 1. Fine-tune with Unsloth\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen2.5-1.5B-Instruct\",\n    load_in_4bit=True,\n)\n# ... train ...\n\n# 2. Export to GGUF\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# 3. Deploy with llcuda\nfrom llcuda.server import ServerManager, ServerConfig\n\nserver = ServerManager()\nserver.start_with_config(ServerConfig(\n    model_path=\"my_model-Q4_K_M.gguf\",\n))\n</code></pre> <p>See: Fine-Tuning Workflow</p>"}]}