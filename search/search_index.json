{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llcuda v2.1.0: Tesla T4 CUDA Inference","text":"<p>Fast LLM inference on Tesla T4 GPUs with complete Unsloth integration and advanced CUDA optimization. Built exclusively for Google Colab and Tesla T4 hardware with GitHub-only distribution.</p>"},{"location":"#whats-new-in-v210","title":"What's New in v2.1.0?","text":"<p>Four New Powerful API Modules</p> <p>v2.1.0 introduces seamless Unsloth fine-tuning to deployment workflow:</p> <ul> <li>Quantization API: 29 formats including NF4, Q4_K_M, Q5_K_M, Q8_0, F16</li> <li>Unsloth Integration: Direct loading and GGUF export with LoRA merging</li> <li>CUDA Optimization: CUDA Graphs (20-40% latency reduction), Triton kernels</li> <li>Advanced Inference: FlashAttention v2, KV-cache optimization, batch processing</li> </ul> <p>Explore v2.1+ APIs \u2192</p>"},{"location":"#why-llcuda-v210","title":"Why llcuda v2.1.0?","text":"Tesla T4 OptimizedGitHub-Only DistributionGoogle Colab ReadyUnsloth Integration <p>Built specifically for Tesla T4 (SM 7.5) with:</p> <ul> <li>\u2705 FlashAttention support (2-3x faster)</li> <li>\u2705 Tensor Core optimization</li> <li>\u2705 CUDA Graphs for reduced overhead</li> <li>\u2705 134 tokens/sec verified on Gemma 3-1B</li> </ul> <p>No PyPI dependency:</p> <pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <ul> <li>Binaries auto-download from GitHub Releases (266 MB)</li> <li>One-time setup, cached for future use</li> <li>Direct from source, always up-to-date</li> </ul> <p>Perfect for cloud notebooks:</p> <ul> <li>\u2705 Tesla T4 Free tier supported</li> <li>\u2705 One-line install</li> <li>\u2705 Instant inference</li> <li>\u2705 Verified 134 tok/s performance</li> </ul> <p>Seamless workflow:</p> <ul> <li>Fine-tune with Unsloth (2x faster training)</li> <li>Export to GGUF format</li> <li>Deploy with llcuda (fast inference)</li> <li>Production-ready pipeline</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Try llcuda on Google Colab right now!</p>"},{"location":"#60-second-setup","title":"60-Second Setup","text":"<pre><code># Install from GitHub\npip install git+https://github.com/waqasm86/llcuda.git\n\n# Run inference\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n# Expected output: ~134 tokens/sec on Tesla T4\n</code></pre> <p>First Run Downloads</p> <p>CUDA binaries (266 MB) download automatically from GitHub Releases v2.0.6 on first import. These v2.0.6 binaries are fully compatible with llcuda v2.1.0+. Subsequent runs use cached binaries - instant startup!</p>"},{"location":"#verified-performance","title":"Verified Performance","text":"<p>Real Google Colab Tesla T4 results with proven 3x faster performance:</p> Model Quantization Speed Latency VRAM Status Gemma 3-1B Q4_K_M 134 tok/s 690ms 1.2 GB \u2705 Verified Llama 3.2-3B Q4_K_M ~30 tok/s - 2.0 GB Estimated Qwen 2.5-7B Q4_K_M ~18 tok/s - 5.0 GB Estimated Llama 3.1-8B Q4_K_M ~15 tok/s - 5.5 GB Estimated <p>Performance Highlights</p> <ul> <li>3x faster than expected (134 vs 45 tok/s initial estimate)</li> <li>Consistent 130-142 tok/s range across batch inference</li> <li>Full GPU offload (99 layers on T4)</li> <li>FlashAttention + Tensor Cores delivering exceptional results</li> </ul> <p> See Executed Notebook</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Auto-Download</p> <p>Fetch CUDA binaries and GGUF models automatically</p> <ul> <li>GitHub Releases integration</li> <li>HuggingFace model support</li> <li>Smart caching system</li> </ul> </li> <li> <p> Optimized for T4</p> <p>Built specifically for Tesla T4 GPUs</p> <ul> <li>SM 7.5 targeting</li> <li>FlashAttention enabled</li> <li>Tensor Core support</li> </ul> </li> <li> <p> Easy API</p> <p>PyTorch-style inference interface</p> <ul> <li>Single-line model loading</li> <li>Batch processing</li> <li>Streaming support</li> </ul> </li> <li> <p> Production Ready</p> <p>Reliable and well-tested</p> <ul> <li>Comprehensive error handling</li> <li>Silent mode for servers</li> <li>MIT licensed</li> </ul> </li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"Interactive ChatBatch ProcessingGoogle ColabUnsloth Workflow <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == \"exit\":\n        break\n\n    result = engine.infer(user_input, max_tokens=400)\n    print(f\"Assistant: {result.text}\")\n</code></pre> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nprompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks briefly.\",\n    \"Define deep learning concisely.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <pre><code>import llcuda\n\n# Verify GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n\n# Load model\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain artificial intelligence\",\n    max_tokens=300\n)\nprint(result.text)\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <pre><code># Step 1: Fine-tune with Unsloth\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-3-1b-it\",\n    max_seq_length=2048,\n    load_in_4bit=True\n)\n\n# Train your model...\n\n# Step 2: Export to GGUF\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# Step 3: Deploy with llcuda\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"my_model/unsloth.Q4_K_M.gguf\")\n\nresult = engine.infer(\"Your prompt\", max_tokens=200)\nprint(result.text)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start Guide</p> <p>Get started in 5 minutes with step-by-step instructions</p> </li> <li> <p> Installation</p> <p>Detailed installation for Google Colab and local systems</p> </li> <li> <p> Google Colab Tutorial</p> <p>Complete walkthrough with Tesla T4 GPU examples</p> </li> <li> <p> API Reference</p> <p>Full API documentation and advanced usage</p> </li> <li> <p> Performance Benchmarks</p> <p>Detailed benchmarks and optimization tips</p> </li> <li> <p> Jupyter Notebooks</p> <p>Ready-to-run Colab notebooks with examples</p> </li> </ul>"},{"location":"#whats-new-in-v210_1","title":"What's New in v2.1.0","text":"<ul> <li>Four New API Modules - Quantization, Unsloth Integration, CUDA Optimization, Advanced Inference</li> <li>Seamless Unsloth Integration - Direct loading and GGUF export with LoRA merging</li> <li>29 Quantization Formats - Including NF4, Q4_K_M, Q5_K_M, Q8_0, F16</li> <li>CUDA Graphs - 20-40% latency reduction with optimized kernel execution</li> <li>Triton Kernels - Custom GPU kernels for enhanced performance</li> <li>FlashAttention v2 - 2-3x faster attention for long contexts</li> <li>Verified Performance - Real Tesla T4 results: 134 tok/s on Gemma 3-1B</li> <li>Binary Compatibility - Uses stable v2.0.6 CUDA binaries (same SHA256)</li> </ul> <p> Read Changelog</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Repository: github.com/waqasm86/llcuda</li> <li>GitHub Releases: Releases &amp; Downloads</li> <li>Bug Reports: GitHub Issues</li> <li>Email: waqasm86@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - Free for commercial and personal use.</p>    Built with \u2764\ufe0f by Waqas Muhammad | Powered by llama.cpp | Optimized for Unsloth"},{"location":"api/device/","title":"GPU Device Management","text":"<p>Complete API for GPU device detection, compatibility checking, and CUDA management in llcuda v2.1.0.</p>"},{"location":"api/device/#overview","title":"Overview","text":"<p>llcuda provides comprehensive GPU device management functions to help you:</p> <ul> <li>Detect CUDA-capable GPUs</li> <li>Check compatibility with llcuda binaries</li> <li>Get device properties and VRAM information</li> <li>Configure optimal inference settings</li> <li>Handle multi-GPU environments</li> </ul>"},{"location":"api/device/#core-functions","title":"Core Functions","text":""},{"location":"api/device/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if your GPU is compatible with llcuda binaries.</p> <p>Signature: <pre><code>def check_gpu_compatibility(min_compute_cap: float = 5.0) -&gt; Dict[str, Any]\n</code></pre></p> <p>Parameters: - <code>min_compute_cap</code> (float, optional): Minimum compute capability required. Default: 5.0</p> <p>Returns: <pre><code>{\n    'compatible': bool,              # Whether GPU is compatible\n    'compute_capability': float,     # GPU compute capability (e.g., 7.5)\n    'gpu_name': str,                 # GPU name (e.g., \"Tesla T4\")\n    'reason': str,                   # Explanation if not compatible\n    'platform': str                  # Detected platform (local/colab/kaggle)\n}\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\n\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\n    print(f\"   Compute Capability: {compat['compute_capability']}\")\n    print(f\"   Platform: {compat['platform']}\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} is not compatible\")\n    print(f\"   Reason: {compat['reason']}\")\n</code></pre></p> <p>Output on Tesla T4: <pre><code>\u2705 Tesla T4 is compatible!\n   Compute Capability: 7.5\n   Platform: colab\n</code></pre></p>"},{"location":"api/device/#detect_cuda","title":"<code>detect_cuda()</code>","text":"<p>Detect CUDA installation and get detailed GPU information.</p> <p>Signature: <pre><code>def detect_cuda() -&gt; Dict[str, Any]\n</code></pre></p> <p>Returns: <pre><code>{\n    'available': bool,     # Whether CUDA is available\n    'version': str,        # CUDA version (e.g., \"12.2\")\n    'gpus': [              # List of GPU information\n        {\n            'name': str,                  # GPU name\n            'memory': str,                # Total VRAM (e.g., \"15360 MiB\")\n            'driver_version': str,        # NVIDIA driver version\n            'compute_capability': str     # Compute capability\n        }\n    ]\n}\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available']:\n    print(f\"CUDA Version: {cuda_info['version']}\")\n    print(f\"Number of GPUs: {len(cuda_info['gpus'])}\")\n\n    for i, gpu in enumerate(cuda_info['gpus']):\n        print(f\"\\nGPU {i}:\")\n        print(f\"  Name: {gpu['name']}\")\n        print(f\"  VRAM: {gpu['memory']}\")\n        print(f\"  Driver: {gpu['driver_version']}\")\n        print(f\"  Compute Capability: {gpu['compute_capability']}\")\nelse:\n    print(\"CUDA is not available\")\n</code></pre></p> <p>Output: <pre><code>CUDA Version: 12.2\nNumber of GPUs: 1\n\nGPU 0:\n  Name: Tesla T4\n  VRAM: 15360 MiB\n  Driver: 535.104.05\n  Compute Capability: 7.5\n</code></pre></p>"},{"location":"api/device/#get_cuda_device_info","title":"<code>get_cuda_device_info()</code>","text":"<p>Get simplified CUDA device information.</p> <p>Signature: <pre><code>def get_cuda_device_info() -&gt; Optional[Dict[str, Any]]\n</code></pre></p> <p>Returns: <pre><code>{\n    'cuda_version': str,   # CUDA version\n    'gpus': list          # List of GPU dictionaries\n}\n# Returns None if CUDA is not available\n</code></pre></p> <p>Example: <pre><code>import llcuda\n\ndevice_info = llcuda.get_cuda_device_info()\n\nif device_info:\n    print(f\"CUDA: {device_info['cuda_version']}\")\n    print(f\"GPUs detected: {len(device_info['gpus'])}\")\nelse:\n    print(\"No CUDA devices found\")\n</code></pre></p>"},{"location":"api/device/#check_cuda_available","title":"<code>check_cuda_available()</code>","text":"<p>Quick check if CUDA is available.</p> <p>Signature: <pre><code>def check_cuda_available() -&gt; bool\n</code></pre></p> <p>Returns: - <code>True</code> if CUDA is available, <code>False</code> otherwise</p> <p>Example: <pre><code>import llcuda\n\nif llcuda.check_cuda_available():\n    print(\"\u2705 CUDA is available\")\n    # Proceed with GPU inference\nelse:\n    print(\"\u274c CUDA not available - CPU mode only\")\n</code></pre></p>"},{"location":"api/device/#supported-gpus","title":"Supported GPUs","text":"<p>llcuda binaries support NVIDIA GPUs with compute capability 5.0 and higher:</p>"},{"location":"api/device/#architecture-support","title":"Architecture Support","text":"Architecture Compute Cap Examples Status Maxwell 5.0 - 5.3 GTX 900, Tesla M40 \u2705 Supported Pascal 6.0 - 6.2 GTX 10xx, Tesla P100 \u2705 Supported Volta 7.0 Tesla V100 \u2705 Supported Turing 7.5 RTX 20xx, Tesla T4, GTX 16xx \u2705 Verified Ampere 8.0 - 8.6 RTX 30xx, A100 \u2705 Supported Ada Lovelace 8.9 RTX 40xx \u2705 Supported Hopper 9.0 H100 \u2705 Supported"},{"location":"api/device/#popular-gpus","title":"Popular GPUs","text":"GPU Model VRAM Compute Cap Recommended Model Size Tesla T4 15 GB 7.5 Up to 7B (Q4_K_M) RTX 3060 12 GB 8.6 Up to 7B (Q4_K_M) RTX 3070 8 GB 8.6 Up to 3B (Q4_K_M) RTX 3080 10 GB 8.6 Up to 7B (Q4_K_M) RTX 3090 24 GB 8.6 Up to 13B (Q4_K_M) RTX 4070 12 GB 8.9 Up to 7B (Q4_K_M) RTX 4090 24 GB 8.9 Up to 13B (Q4_K_M) A100 40 GB 8.0 Up to 30B (Q4_K_M) A100 80 GB 8.0 Up to 70B (Q4_K_M)"},{"location":"api/device/#vram-management","title":"VRAM Management","text":""},{"location":"api/device/#get-available-vram","title":"Get Available VRAM","text":"<pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available'] and cuda_info['gpus']:\n    gpu = cuda_info['gpus'][0]\n    vram_str = gpu['memory']\n\n    # Parse VRAM\n    if 'GiB' in vram_str:\n        vram_gb = float(vram_str.split()[0])\n    elif 'MiB' in vram_str:\n        vram_mb = float(vram_str.split()[0])\n        vram_gb = vram_mb / 1024\n\n    print(f\"Available VRAM: {vram_gb:.1f} GB\")\n</code></pre>"},{"location":"api/device/#vram-recommendations","title":"VRAM Recommendations","text":"<p>Get recommended settings based on available VRAM:</p> <pre><code>from llcuda.utils import get_recommended_gpu_layers\n\n# For a 1.2 GB model with 15 GB VRAM\ngpu_layers = get_recommended_gpu_layers(\n    model_size_gb=1.2,\n    vram_gb=15.0\n)\n\nprint(f\"Recommended GPU layers: {gpu_layers}\")\n# Output: 99 (full GPU offload)\n</code></pre> <p>VRAM to GPU Layers Mapping:</p> Available VRAM Model Size Recommended Layers &gt;= 1.2x model Any 99 (full offload) &gt;= 0.8x model Any 40 (most layers) &gt;= 0.6x model Any 30 (many layers) &gt;= 0.4x model Any 20 (some layers) &gt;= 0.2x model Any 10 (few layers) &lt; 0.2x model Any 0 (CPU only)"},{"location":"api/device/#auto-configuration","title":"Auto-Configuration","text":""},{"location":"api/device/#auto_configure_for_model","title":"<code>auto_configure_for_model()</code>","text":"<p>Automatically configure optimal settings for your GPU and model.</p> <p>Signature: <pre><code>from llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\ndef auto_configure_for_model(\n    model_path: Path,\n    vram_gb: Optional[float] = None\n) -&gt; Dict[str, Any]\n</code></pre></p> <p>Parameters: - <code>model_path</code> (Path): Path to GGUF model file - <code>vram_gb</code> (float, optional): VRAM in GB (auto-detected if not provided)</p> <p>Returns: <pre><code>{\n    'gpu_layers': int,      # Number of layers to offload\n    'ctx_size': int,        # Context window size\n    'batch_size': int,      # Batch size for processing\n    'ubatch_size': int,     # Micro-batch size\n    'n_parallel': int       # Parallel sequences\n}\n</code></pre></p> <p>Example: <pre><code>from llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Auto-configure for model\nmodel_path = Path(\"/path/to/model.gguf\")\nsettings = auto_configure_for_model(model_path)\n\nprint(\"Recommended settings:\")\nprint(f\"  GPU Layers: {settings['gpu_layers']}\")\nprint(f\"  Context Size: {settings['ctx_size']}\")\nprint(f\"  Batch Size: {settings['batch_size']}\")\nprint(f\"  Micro-batch Size: {settings['ubatch_size']}\")\n\n# Use settings with InferenceEngine\nengine.load_model(\n    str(model_path),\n    gpu_layers=settings['gpu_layers'],\n    ctx_size=settings['ctx_size'],\n    batch_size=settings['batch_size'],\n    ubatch_size=settings['ubatch_size']\n)\n</code></pre></p> <p>Output on Tesla T4 (15 GB): <pre><code>\u2713 Auto-configured for 15.0 GB VRAM\n  GPU Layers: 99\n  Context Size: 4096\n  Batch Size: 2048\n  Micro-batch Size: 512\n</code></pre></p>"},{"location":"api/device/#platform-detection","title":"Platform Detection","text":"<p>llcuda automatically detects the execution environment:</p> <pre><code>import llcuda\n\ncompat = llcuda.check_gpu_compatibility()\nplatform = compat['platform']\n\nif platform == 'colab':\n    print(\"Running on Google Colab\")\n    print(\"Expected GPU: Tesla T4\")\nelif platform == 'kaggle':\n    print(\"Running on Kaggle\")\n    print(\"Expected GPU: Tesla P100 or T4\")\nelse:\n    print(\"Running on local machine\")\n</code></pre> <p>Detected Platforms:</p> <ul> <li><code>colab</code> - Google Colab</li> <li><code>kaggle</code> - Kaggle Notebooks</li> <li><code>local</code> - Local machine or other cloud</li> </ul>"},{"location":"api/device/#multi-gpu-support","title":"Multi-GPU Support","text":""},{"location":"api/device/#selecting-specific-gpu","title":"Selecting Specific GPU","text":"<pre><code>import os\n\n# Use GPU 0 only\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Use GPU 1 only\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Use GPUs 0 and 2\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n\n# Then import llcuda\nimport llcuda\n</code></pre>"},{"location":"api/device/#checking-multiple-gpus","title":"Checking Multiple GPUs","text":"<pre><code>import llcuda\n\ncuda_info = llcuda.detect_cuda()\n\nif cuda_info['available']:\n    num_gpus = len(cuda_info['gpus'])\n    print(f\"Detected {num_gpus} GPU(s)\")\n\n    for i, gpu in enumerate(cuda_info['gpus']):\n        print(f\"\\nGPU {i}: {gpu['name']}\")\n        print(f\"  VRAM: {gpu['memory']}\")\n        print(f\"  Compute: {gpu['compute_capability']}\")\n</code></pre>"},{"location":"api/device/#environment-setup","title":"Environment Setup","text":""},{"location":"api/device/#setup_environment","title":"<code>setup_environment()</code>","text":"<p>Automatically configure environment variables for optimal performance.</p> <pre><code>from llcuda.utils import setup_environment\n\n# Setup environment\nenv_vars = setup_environment()\n\nprint(\"Environment configured:\")\nfor key, value in env_vars.items():\n    print(f\"  {key}: {value}\")\n</code></pre> <p>Configured Variables:</p> <ul> <li><code>LD_LIBRARY_PATH</code> - Shared library path (Linux)</li> <li><code>CUDA_VISIBLE_DEVICES</code> - Visible GPUs</li> <li><code>LLAMA_CPP_DIR</code> - llama.cpp installation directory</li> </ul>"},{"location":"api/device/#system-information","title":"System Information","text":""},{"location":"api/device/#print_system_info","title":"<code>print_system_info()</code>","text":"<p>Print comprehensive system information for debugging.</p> <pre><code>from llcuda.utils import print_system_info\n\nprint_system_info()\n</code></pre> <p>Output: <pre><code>============================================================\nllcuda System Information\n============================================================\n\nPython:\n  Version: 3.10.12 (main, Nov 20 2023, 15:14:05)\n  Executable: /usr/bin/python3\n\nOperating System:\n  System: Linux\n  Release: 5.15.0-91-generic\n  Machine: x86_64\n\nCUDA:\n  Available: True\n  Version: 12.2\n  GPUs: 1\n    GPU 0: Tesla T4\n      Memory: 15360 MiB\n      Driver: 535.104.05\n      Compute: 7.5\n\nGGUF Models Found: 3\n  - gemma-3-1b-it-Q4_K_M.gguf (872.5 MB)\n  - llama-3.2-3b-Q4_K_M.gguf (1856.2 MB)\n  - qwen2.5-7b-Q4_K_M.gguf (4368.7 MB)\n\n============================================================\n</code></pre></p>"},{"location":"api/device/#common-patterns","title":"Common Patterns","text":""},{"location":"api/device/#complete-gpu-verification","title":"Complete GPU Verification","text":"<pre><code>import llcuda\n\ndef verify_gpu_setup():\n    \"\"\"Verify GPU setup before running inference.\"\"\"\n\n    # Check CUDA availability\n    if not llcuda.check_cuda_available():\n        print(\"\u274c CUDA not available\")\n        return False\n\n    # Get detailed info\n    cuda_info = llcuda.detect_cuda()\n    print(f\"\u2705 CUDA {cuda_info['version']} detected\")\n\n    # Check compatibility\n    compat = llcuda.check_gpu_compatibility()\n\n    if not compat['compatible']:\n        print(f\"\u274c {compat['gpu_name']} is not compatible\")\n        print(f\"   {compat['reason']}\")\n        return False\n\n    print(f\"\u2705 {compat['gpu_name']} is compatible\")\n    print(f\"   Compute Capability: {compat['compute_capability']}\")\n    print(f\"   Platform: {compat['platform']}\")\n\n    # Get VRAM info\n    gpu = cuda_info['gpus'][0]\n    print(f\"   VRAM: {gpu['memory']}\")\n\n    return True\n\n# Use it\nif verify_gpu_setup():\n    print(\"\\n\ud83d\ude80 Ready for inference!\")\n    # Proceed with model loading...\nelse:\n    print(\"\\n\u26a0\ufe0f GPU setup incomplete\")\n</code></pre>"},{"location":"api/device/#auto-configure-and-load","title":"Auto-Configure and Load","text":"<pre><code>import llcuda\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nif not compat['compatible']:\n    raise RuntimeError(f\"GPU not compatible: {compat['reason']}\")\n\n# Auto-configure\nmodel_path = Path(\"model.gguf\")\nsettings = auto_configure_for_model(model_path)\n\n# Create engine with optimal settings\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    str(model_path),\n    **settings,  # Use all auto-configured settings\n    silent=True\n)\n\nprint(\"\u2705 Model loaded with optimal settings!\")\n</code></pre>"},{"location":"api/device/#error-handling","title":"Error Handling","text":""},{"location":"api/device/#handle-no-gpu","title":"Handle No GPU","text":"<pre><code>import llcuda\n\ntry:\n    compat = llcuda.check_gpu_compatibility()\n\n    if not compat['compatible']:\n        raise RuntimeError(compat['reason'])\n\n    # Proceed with GPU inference\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"model.gguf\", gpu_layers=99)\n\nexcept RuntimeError as e:\n    print(f\"GPU Error: {e}\")\n    print(\"Falling back to CPU mode...\")\n\n    # Load with CPU\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"model.gguf\", gpu_layers=0)\n</code></pre>"},{"location":"api/device/#handle-insufficient-vram","title":"Handle Insufficient VRAM","text":"<pre><code>import llcuda\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\nmodel_path = Path(\"large-model.gguf\")\n\ntry:\n    # Try auto-configuration\n    settings = auto_configure_for_model(model_path)\n\n    if settings['gpu_layers'] == 0:\n        print(\"\u26a0\ufe0f Insufficient VRAM for GPU offload\")\n        print(\"   Using CPU mode\")\n    elif settings['gpu_layers'] &lt; 99:\n        print(f\"\u26a0\ufe0f Partial GPU offload: {settings['gpu_layers']} layers\")\n        print(\"   Consider using a smaller model for better performance\")\n\n    # Load with recommended settings\n    engine = llcuda.InferenceEngine()\n    engine.load_model(str(model_path), **settings)\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(\"Try a smaller model or more aggressive quantization\")\n</code></pre>"},{"location":"api/device/#see-also","title":"See Also","text":"<ul> <li>API Overview - Complete API reference</li> <li>InferenceEngine - Inference API</li> <li>Models &amp; GGUF - Model management</li> <li>Performance Guide - Optimization techniques</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"api/examples/","title":"Code Examples","text":"<p>Complete, production-ready code examples for common llcuda use cases.</p>"},{"location":"api/examples/#quick-reference","title":"Quick Reference","text":"Example Use Case Complexity Basic Inference Single question-answer Beginner Chat Application Interactive conversation Beginner Batch Processing Process multiple prompts Beginner Streaming Inference Real-time token generation Intermediate Custom Parameters Fine-tune generation Intermediate Context Manager Auto-cleanup resources Intermediate Error Handling Production-ready code Advanced Benchmarking Measure performance Advanced"},{"location":"api/examples/#basic-inference","title":"Basic Inference","text":"<p>Simple question-answer inference.</p> <pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\n# Print results\nprint(f\"Response: {result.text}\")\nprint(f\"\\nPerformance:\")\nprint(f\"  Speed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"  Latency: {result.latency_ms:.0f}ms\")\nprint(f\"  Tokens: {result.tokens_generated}\")\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Response: Quantum computing uses quantum mechanics principles...\n\nPerformance:\n  Speed: 134.2 tokens/sec\n  Latency: 690ms\n  Tokens: 93\n</code></pre></p>"},{"location":"api/examples/#chat-application","title":"Chat Application","text":"<p>Interactive chat with conversation loop.</p> <pre><code>import llcuda\n\ndef chat_application():\n    \"\"\"Interactive chat application with Gemma 3-1B.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n\n    print(\"Loading Gemma 3-1B model...\")\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    print(\"\\n\ud83e\udd16 Chat with Gemma 3-1B\")\n    print(\"Type 'exit' to quit, 'clear' to reset metrics\\n\")\n\n    while True:\n        # Get user input\n        user_input = input(\"You: \").strip()\n\n        # Handle commands\n        if user_input.lower() == 'exit':\n            print(\"\\nGoodbye!\")\n            break\n\n        if user_input.lower() == 'clear':\n            engine.reset_metrics()\n            print(\"\u2705 Metrics reset\\n\")\n            continue\n\n        if not user_input:\n            continue\n\n        # Generate response\n        result = engine.infer(\n            user_input,\n            max_tokens=300,\n            temperature=0.7\n        )\n\n        # Display response\n        print(f\"\\n\ud83e\udd16 AI: {result.text}\")\n        print(f\"   ({result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms)\\n\")\n\n    # Show final metrics\n    metrics = engine.get_metrics()\n    print(\"\\n\ud83d\udcca Session Statistics:\")\n    print(f\"  Total requests: {metrics['throughput']['total_requests']}\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Run the chat app\nif __name__ == \"__main__\":\n    chat_application()\n</code></pre> <p>Sample Interaction: <pre><code>You: What is machine learning?\n\n\ud83e\udd16 AI: Machine learning is a subset of artificial intelligence that enables\n   computers to learn from data without explicit programming...\n   (134.5 tok/s, 685ms)\n\nYou: Give me an example\n\n\ud83e\udd16 AI: A common example is email spam filtering. The system learns to\n   identify spam by analyzing thousands of emails...\n   (136.2 tok/s, 702ms)\n\nYou: exit\n\n\ud83d\udcca Session Statistics:\n  Total requests: 2\n  Total tokens: 184\n  Avg speed: 135.2 tok/s\n  Avg latency: 694ms\n</code></pre></p>"},{"location":"api/examples/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently.</p> <pre><code>import llcuda\nimport time\n\ndef batch_processing_example():\n    \"\"\"Process multiple prompts with performance tracking.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define prompts\n    prompts = [\n        \"What is artificial intelligence?\",\n        \"Explain neural networks briefly.\",\n        \"What is deep learning?\",\n        \"Define machine learning.\",\n        \"What are transformers in AI?\",\n        \"Explain backpropagation.\",\n        \"What is gradient descent?\",\n        \"Define overfitting in ML.\"\n    ]\n\n    print(f\"Processing {len(prompts)} prompts...\\n\")\n\n    # Reset metrics\n    engine.reset_metrics()\n\n    # Process batch\n    start_time = time.time()\n    results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n    total_time = time.time() - start_time\n\n    # Display results\n    for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n        print(f\"{i}. Q: {prompt}\")\n        print(f\"   A: {result.text[:100]}...\")\n        print(f\"   Performance: {result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms\\n\")\n\n    # Show aggregate metrics\n    metrics = engine.get_metrics()\n    print(\"\ud83d\udcca Batch Processing Summary:\")\n    print(f\"  Prompts processed: {len(prompts)}\")\n    print(f\"  Total time: {total_time:.2f}s\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg throughput: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n    print(f\"  P95 latency: {metrics['latency']['p95_ms']:.0f}ms\")\n    print(f\"  Requests/sec: {len(prompts) / total_time:.2f}\")\n\n# Run batch processing\nif __name__ == \"__main__\":\n    batch_processing_example()\n</code></pre> <p>Expected Output: <pre><code>Processing 8 prompts...\n\n1. Q: What is artificial intelligence?\n   A: Artificial intelligence (AI) is the simulation of human intelligence...\n   Performance: 134.8 tok/s, 685ms\n\n2. Q: Explain neural networks briefly.\n   A: Neural networks are computational models inspired by the human brain...\n   Performance: 135.2 tok/s, 692ms\n\n[...]\n\n\ud83d\udcca Batch Processing Summary:\n  Prompts processed: 8\n  Total time: 5.52s\n  Total tokens: 592\n  Avg throughput: 134.5 tok/s\n  Avg latency: 690ms\n  P95 latency: 725ms\n  Requests/sec: 1.45\n</code></pre></p>"},{"location":"api/examples/#streaming-inference","title":"Streaming Inference","text":"<p>Stream tokens as they're generated (simulation).</p> <pre><code>import llcuda\nimport time\n\ndef streaming_inference_example():\n    \"\"\"Demonstrate streaming inference with callback.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define callback for streaming\n    def stream_callback(chunk):\n        \"\"\"Print each chunk as it arrives.\"\"\"\n        print(chunk, end='', flush=True)\n\n    prompt = \"Write a short story about a robot learning to paint\"\n\n    print(\"\ud83e\udd16 Generating story (streaming):\\n\")\n    print(\"AI: \", end='', flush=True)\n\n    # Stream inference\n    result = engine.infer_stream(\n        prompt,\n        callback=stream_callback,\n        max_tokens=200,\n        temperature=0.8\n    )\n\n    # Show metrics\n    print(f\"\\n\\n\ud83d\udcca Performance:\")\n    print(f\"  Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"  Latency: {result.latency_ms:.0f}ms\")\n    print(f\"  Tokens: {result.tokens_generated}\")\n\n# Run streaming example\nif __name__ == \"__main__\":\n    streaming_inference_example()\n</code></pre> <p>Note: Current implementation simulates streaming. True token-by-token streaming will be available in a future release.</p>"},{"location":"api/examples/#custom-generation-parameters","title":"Custom Generation Parameters","text":"<p>Fine-tune generation with custom parameters.</p> <pre><code>import llcuda\n\ndef custom_parameters_example():\n    \"\"\"Demonstrate different generation strategies.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    prompt = \"Once upon a time in a futuristic city\"\n\n    # Strategy 1: Deterministic (low temperature)\n    print(\"1\ufe0f\u20e3 Deterministic Generation (temp=0.1):\")\n    result1 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.1,\n        top_p=0.9,\n        top_k=10\n    )\n    print(f\"{result1.text}\\n\")\n\n    # Strategy 2: Balanced (default)\n    print(\"2\ufe0f\u20e3 Balanced Generation (temp=0.7):\")\n    result2 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.7,\n        top_p=0.9,\n        top_k=40\n    )\n    print(f\"{result2.text}\\n\")\n\n    # Strategy 3: Creative (high temperature)\n    print(\"3\ufe0f\u20e3 Creative Generation (temp=1.2):\")\n    result3 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.2,\n        top_p=0.95,\n        top_k=100\n    )\n    print(f\"{result3.text}\\n\")\n\n    # Strategy 4: Very creative (high temp + nucleus sampling)\n    print(\"4\ufe0f\u20e3 Very Creative (temp=1.5, top_p=0.95):\")\n    result4 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.5,\n        top_p=0.95,\n        top_k=200\n    )\n    print(f\"{result4.text}\\n\")\n\n    # Compare performance\n    print(\"\ud83d\udcca Performance Comparison:\")\n    for i, result in enumerate([result1, result2, result3, result4], 1):\n        print(f\"  Strategy {i}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Run custom parameters example\nif __name__ == \"__main__\":\n    custom_parameters_example()\n</code></pre> <p>Parameter Guide:</p> Parameter Range Effect Use Case <code>temperature</code> 0.1 - 0.3 Deterministic, focused Code, facts <code>temperature</code> 0.6 - 0.8 Balanced creativity General chat <code>temperature</code> 1.0 - 1.5 Very creative Stories, brainstorming <code>top_p</code> 0.9 - 0.95 Nucleus sampling Quality control <code>top_k</code> 10 - 200 Diversity limit Token variety"},{"location":"api/examples/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Automatic resource cleanup.</p> <pre><code>import llcuda\n\ndef context_manager_example():\n    \"\"\"Use context manager for automatic cleanup.\"\"\"\n\n    # Context manager ensures server cleanup\n    with llcuda.InferenceEngine() as engine:\n        # Load model\n        engine.load_model(\n            \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n            silent=True\n        )\n\n        # Run inferences\n        prompts = [\n            \"What is Python?\",\n            \"What is JavaScript?\",\n            \"What is Rust?\"\n        ]\n\n        for prompt in prompts:\n            result = engine.infer(prompt, max_tokens=50)\n            print(f\"Q: {prompt}\")\n            print(f\"A: {result.text}\\n\")\n\n        # Get final metrics\n        metrics = engine.get_metrics()\n        print(f\"Total tokens: {metrics['throughput']['total_tokens']}\")\n\n    # Server automatically stopped here\n    print(\"\u2705 Server cleaned up automatically\")\n\n# Run context manager example\nif __name__ == \"__main__\":\n    context_manager_example()\n</code></pre>"},{"location":"api/examples/#robust-error-handling","title":"Robust Error Handling","text":"<p>Production-ready error handling.</p> <pre><code>import llcuda\nfrom llcuda import InferenceEngine\n\ndef robust_inference(prompt: str, max_retries: int = 3):\n    \"\"\"Robust inference with error handling and retries.\"\"\"\n\n    engine = None\n\n    try:\n        # Check GPU compatibility\n        compat = llcuda.check_gpu_compatibility()\n        if not compat['compatible']:\n            raise RuntimeError(\n                f\"GPU {compat['gpu_name']} is not compatible: {compat['reason']}\"\n            )\n\n        # Initialize engine\n        engine = InferenceEngine()\n\n        # Load model with error handling\n        try:\n            engine.load_model(\n                \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n                silent=True,\n                auto_start=True\n            )\n        except FileNotFoundError as e:\n            print(f\"Model not found: {e}\")\n            print(\"Please download the model first\")\n            return None\n        except RuntimeError as e:\n            print(f\"Server failed to start: {e}\")\n            return None\n\n        # Run inference with retries\n        for attempt in range(max_retries):\n            result = engine.infer(prompt, max_tokens=200)\n\n            if result.success:\n                return {\n                    'text': result.text,\n                    'tokens_per_sec': result.tokens_per_sec,\n                    'latency_ms': result.latency_ms,\n                    'success': True\n                }\n            else:\n                print(f\"Attempt {attempt + 1} failed: {result.error_message}\")\n                if attempt &lt; max_retries - 1:\n                    print(f\"Retrying... ({max_retries - attempt - 1} attempts left)\")\n                    import time\n                    time.sleep(1)\n\n        # All retries failed\n        return {\n            'text': None,\n            'error': 'All retry attempts failed',\n            'success': False\n        }\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {\n            'text': None,\n            'error': str(e),\n            'success': False\n        }\n\n    finally:\n        # Cleanup\n        if engine is not None:\n            engine.unload_model()\n\n# Example usage\nif __name__ == \"__main__\":\n    result = robust_inference(\"What is quantum computing?\")\n\n    if result and result['success']:\n        print(f\"\u2705 Success!\")\n        print(f\"Response: {result['text']}\")\n        print(f\"Speed: {result['tokens_per_sec']:.1f} tok/s\")\n    else:\n        print(f\"\u274c Failed: {result['error'] if result else 'Unknown error'}\")\n</code></pre>"},{"location":"api/examples/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Comprehensive performance measurement.</p> <pre><code>import llcuda\nimport time\nimport statistics\n\ndef benchmark_inference(num_runs: int = 10):\n    \"\"\"Benchmark inference performance.\"\"\"\n\n    # Initialize\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Warmup\n    print(\"Warming up...\")\n    for _ in range(3):\n        engine.infer(\"Warmup prompt\", max_tokens=10)\n\n    # Benchmark\n    print(f\"Running {num_runs} iterations...\\n\")\n\n    engine.reset_metrics()\n    latencies = []\n    throughputs = []\n\n    test_prompt = \"Explain the concept of recursion in programming\"\n\n    for i in range(num_runs):\n        start = time.time()\n        result = engine.infer(test_prompt, max_tokens=100)\n        elapsed = (time.time() - start) * 1000  # Convert to ms\n\n        latencies.append(result.latency_ms)\n        throughputs.append(result.tokens_per_sec)\n\n        print(f\"Run {i+1}/{num_runs}: \"\n              f\"{result.tokens_per_sec:.1f} tok/s, \"\n              f\"{result.latency_ms:.0f}ms\")\n\n    # Calculate statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83d\udcca Benchmark Results\")\n    print(\"=\"*60)\n\n    print(\"\\nThroughput (tokens/sec):\")\n    print(f\"  Mean:   {statistics.mean(throughputs):.2f}\")\n    print(f\"  Median: {statistics.median(throughputs):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(throughputs):.2f}\")\n    print(f\"  Min:    {min(throughputs):.2f}\")\n    print(f\"  Max:    {max(throughputs):.2f}\")\n\n    print(\"\\nLatency (ms):\")\n    print(f\"  Mean:   {statistics.mean(latencies):.2f}\")\n    print(f\"  Median: {statistics.median(latencies):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(latencies):.2f}\")\n    print(f\"  Min:    {min(latencies):.2f}\")\n    print(f\"  Max:    {max(latencies):.2f}\")\n\n    # Percentiles\n    sorted_latencies = sorted(latencies)\n    p50_idx = len(sorted_latencies) // 2\n    p95_idx = int(len(sorted_latencies) * 0.95)\n    p99_idx = int(len(sorted_latencies) * 0.99)\n\n    print(\"\\nLatency Percentiles:\")\n    print(f\"  P50:    {sorted_latencies[p50_idx]:.2f}ms\")\n    print(f\"  P95:    {sorted_latencies[p95_idx]:.2f}ms\")\n    print(f\"  P99:    {sorted_latencies[p99_idx]:.2f}ms\")\n\n    # Get metrics from engine\n    metrics = engine.get_metrics()\n    print(f\"\\nTotal tokens generated: {metrics['throughput']['total_tokens']}\")\n    print(f\"Total requests: {metrics['throughput']['total_requests']}\")\n\n    print(\"=\"*60)\n\n# Run benchmark\nif __name__ == \"__main__\":\n    benchmark_inference(num_runs=10)\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Warming up...\nRunning 10 iterations...\n\nRun 1/10: 134.2 tok/s, 690ms\nRun 2/10: 136.5 tok/s, 685ms\nRun 3/10: 133.8 tok/s, 695ms\n[...]\n\n============================================================\n\ud83d\udcca Benchmark Results\n============================================================\n\nThroughput (tokens/sec):\n  Mean:   134.52\n  Median: 134.30\n  Stdev:  1.24\n  Min:    132.80\n  Max:    136.50\n\nLatency (ms):\n  Mean:   692.45\n  Median: 690.00\n  Stdev:  8.32\n  Min:    685.00\n  Max:    710.00\n\nLatency Percentiles:\n  P50:    690.00ms\n  P95:    705.00ms\n  P99:    710.00ms\n\nTotal tokens generated: 940\nTotal requests: 10\n============================================================\n</code></pre></p>"},{"location":"api/examples/#advanced-custom-chat-engine","title":"Advanced: Custom Chat Engine","text":"<p>Using the ChatEngine for conversations.</p> <pre><code>from llcuda import InferenceEngine\nfrom llcuda.chat import ChatEngine\n\ndef advanced_chat_example():\n    \"\"\"Advanced chat with conversation history.\"\"\"\n\n    # Initialize inference engine\n    engine = InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Create chat engine with system prompt\n    chat = ChatEngine(\n        engine,\n        system_prompt=\"You are a helpful AI coding assistant specialized in Python.\",\n        max_history=20,\n        max_tokens=200,\n        temperature=0.7\n    )\n\n    # Conversation\n    chat.add_user_message(\"How do I read a file in Python?\")\n    response1 = chat.complete()\n    print(f\"AI: {response1}\\n\")\n\n    chat.add_user_message(\"Can you show me an example?\")\n    response2 = chat.complete()\n    print(f\"AI: {response2}\\n\")\n\n    chat.add_user_message(\"What about writing to a file?\")\n    response3 = chat.complete()\n    print(f\"AI: {response3}\\n\")\n\n    # Get conversation history\n    history = chat.get_history()\n    print(f\"Conversation has {len(history)} messages\")\n\n    # Save conversation\n    chat.save_history(\"conversation.json\")\n    print(\"\u2705 Conversation saved to conversation.json\")\n\n    # Token count\n    token_count = chat.count_tokens()\n    print(f\"Approximate token count: {token_count}\")\n\n# Run advanced chat\nif __name__ == \"__main__\":\n    advanced_chat_example()\n</code></pre>"},{"location":"api/examples/#see-also","title":"See Also","text":"<ul> <li>API Overview - Complete API reference</li> <li>InferenceEngine - Detailed engine documentation</li> <li>Quick Start - Getting started guide</li> <li>Tutorials - Step-by-step tutorials</li> <li>Performance - Benchmark results</li> </ul>"},{"location":"api/inference-engine/","title":"InferenceEngine API Reference","text":"<p>Complete API documentation for the <code>InferenceEngine</code> class, the main interface for llcuda inference.</p>"},{"location":"api/inference-engine/#class-overview","title":"Class Overview","text":"<pre><code>class InferenceEngine:\n    \"\"\"\n    High-level Python interface for LLM inference with CUDA acceleration.\n\n    Provides automatic server management, model loading, and inference APIs.\n    \"\"\"\n</code></pre>"},{"location":"api/inference-engine/#constructor","title":"Constructor","text":""},{"location":"api/inference-engine/#__init__server_urlhttp1270018090","title":"<code>__init__(server_url=\"http://127.0.0.1:8090\")</code>","text":"<p>Initialize a new inference engine.</p> <p>Parameters:</p> Parameter Type Default Description <code>server_url</code> <code>str</code> <code>\"http://127.0.0.1:8090\"</code> URL of llama-server backend <p>Example:</p> <pre><code>import llcuda\n\n# Default URL\nengine = llcuda.InferenceEngine()\n\n# Custom port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n\n# Remote server\nengine = llcuda.InferenceEngine(server_url=\"http://192.168.1.100:8090\")\n</code></pre>"},{"location":"api/inference-engine/#methods","title":"Methods","text":""},{"location":"api/inference-engine/#load_model","title":"<code>load_model()</code>","text":"<p>Load a GGUF model for inference with automatic configuration.</p> <pre><code>def load_model(\n    model_name_or_path: str,\n    gpu_layers: Optional[int] = None,\n    ctx_size: Optional[int] = None,\n    auto_start: bool = True,\n    auto_configure: bool = True,\n    n_parallel: int = 1,\n    verbose: bool = True,\n    interactive_download: bool = True,\n    silent: bool = False,\n    **kwargs\n) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_name_or_path</code> <code>str</code> Required Model registry name, local path, or HF repo <code>gpu_layers</code> <code>Optional[int]</code> <code>None</code> Number of layers to offload to GPU (None=auto) <code>ctx_size</code> <code>Optional[int]</code> <code>None</code> Context window size (None=auto) <code>auto_start</code> <code>bool</code> <code>True</code> Automatically start llama-server if not running <code>auto_configure</code> <code>bool</code> <code>True</code> Auto-detect optimal settings <code>n_parallel</code> <code>int</code> <code>1</code> Number of parallel sequences <code>verbose</code> <code>bool</code> <code>True</code> Print status messages <code>interactive_download</code> <code>bool</code> <code>True</code> Ask before downloading models <code>silent</code> <code>bool</code> <code>False</code> Suppress llama-server output <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional server parameters <p>Returns:</p> <ul> <li><code>bool</code> - True if model loaded successfully</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code> - Model file not found</li> <li><code>ValueError</code> - Model download cancelled</li> <li><code>ConnectionError</code> - Server not running and auto_start=False</li> <li><code>RuntimeError</code> - Server failed to start</li> </ul> <p>Loading Methods:</p> Registry NameHuggingFace SyntaxLocal Path <pre><code># Auto-download from HuggingFace registry\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre> <pre><code># Direct HF download\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre> <pre><code># Load local GGUF file\nengine.load_model(\"/path/to/model.gguf\")\n</code></pre> <p>Example:</p> <pre><code># Auto-configuration (recommended)\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    auto_configure=True,\n    verbose=True\n)\n\n# Manual configuration\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=35,\n    ctx_size=4096,\n    batch_size=512,\n    ubatch_size=128,\n    auto_configure=False\n)\n\n# Silent mode\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    silent=True,  # No llama-server output\n    verbose=False # No status messages\n)\n</code></pre>"},{"location":"api/inference-engine/#infer","title":"<code>infer()</code>","text":"<p>Run inference on a single prompt.</p> <pre><code>def infer(\n    prompt: str,\n    max_tokens: int = 128,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    top_k: int = 40,\n    seed: int = 0,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; InferResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompt</code> <code>str</code> Required Input prompt text <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>0.7</code> Sampling temperature (0.0-2.0) <code>top_p</code> <code>float</code> <code>0.9</code> Nucleus sampling threshold <code>top_k</code> <code>int</code> <code>40</code> Top-k sampling limit <code>seed</code> <code>int</code> <code>0</code> Random seed (0=random) <code>stop_sequences</code> <code>Optional[List[str]]</code> <code>None</code> Stop generation at these sequences <p>Returns:</p> <ul> <li><code>InferResult</code> - Result object with text and metrics</li> </ul> <p>Example:</p> <pre><code># Basic inference\nresult = engine.infer(\n    prompt=\"What is AI?\",\n    max_tokens=100\n)\nprint(result.text)\n\n# Advanced parameters\nresult = engine.infer(\n    prompt=\"Write a poem about\",\n    max_tokens=200,\n    temperature=0.9,  # More creative\n    top_p=0.95,\n    top_k=50,\n    seed=42,\n    stop_sequences=[\"\\n\\n\", \"###\"]\n)\n\n# Check results\nif result.success:\n    print(f\"Generated: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"Latency: {result.latency_ms:.0f} ms\")\nelse:\n    print(f\"Error: {result.error_message}\")\n</code></pre>"},{"location":"api/inference-engine/#batch_infer","title":"<code>batch_infer()</code>","text":"<p>Run batch inference on multiple prompts.</p> <pre><code>def batch_infer(\n    prompts: List[str],\n    max_tokens: int = 128,\n    **kwargs\n) -&gt; List[InferResult]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompts</code> <code>List[str]</code> Required List of input prompts <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens per prompt <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters (temperature, top_p, etc.) <p>Returns:</p> <ul> <li><code>List[InferResult]</code> - List of result objects</li> </ul> <p>Example:</p> <pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"What is deep learning?\"\n]\n\nresults = engine.batch_infer(\n    prompts,\n    max_tokens=100,\n    temperature=0.7\n)\n\nfor i, result in enumerate(results):\n    print(f\"Prompt {i+1}: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"api/inference-engine/#infer_stream","title":"<code>infer_stream()</code>","text":"<p>Run streaming inference with real-time callbacks.</p> <pre><code>def infer_stream(\n    prompt: str,\n    callback: Callable[[str], None],\n    max_tokens: int = 128,\n    temperature: float = 0.7,\n    **kwargs\n) -&gt; InferResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>prompt</code> <code>str</code> Required Input prompt text <code>callback</code> <code>Callable</code> Required Function called for each chunk <code>max_tokens</code> <code>int</code> <code>128</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>0.7</code> Sampling temperature <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters <p>Returns:</p> <ul> <li><code>InferResult</code> - Complete result after streaming</li> </ul> <p>Example:</p> <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    prompt=\"Write a story about AI:\",\n    callback=print_chunk,\n    max_tokens=200,\n    temperature=0.8\n)\n\nprint(f\"\\n\\nTotal speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"api/inference-engine/#check_server","title":"<code>check_server()</code>","text":"<p>Check if llama-server is running and accessible.</p> <pre><code>def check_server() -&gt; bool\n</code></pre> <p>Returns:</p> <ul> <li><code>bool</code> - True if server is accessible, False otherwise</li> </ul> <p>Example:</p> <pre><code>if engine.check_server():\n    print(\"Server is running\")\nelse:\n    print(\"Server is not running\")\n    # Optionally start it\n    engine.load_model(\"model.gguf\", auto_start=True)\n</code></pre>"},{"location":"api/inference-engine/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get current performance metrics.</p> <pre><code>def get_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Returns:</p> <ul> <li><code>dict</code> - Dictionary with latency, throughput, and GPU metrics</li> </ul> <p>Return Structure:</p> <pre><code>{\n    'latency': {\n        'mean_ms': float,\n        'p50_ms': float,\n        'p95_ms': float,\n        'p99_ms': float,\n        'min_ms': float,\n        'max_ms': float,\n        'sample_count': int\n    },\n    'throughput': {\n        'total_tokens': int,\n        'total_requests': int,\n        'tokens_per_sec': float,\n        'requests_per_sec': float\n    }\n}\n</code></pre> <p>Example:</p> <pre><code>metrics = engine.get_metrics()\n\nprint(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"Total requests: {metrics['throughput']['total_requests']}\")\n</code></pre>"},{"location":"api/inference-engine/#reset_metrics","title":"<code>reset_metrics()</code>","text":"<p>Reset performance metrics counters.</p> <pre><code>def reset_metrics() -&gt; None\n</code></pre> <p>Example:</p> <pre><code># Reset before benchmark\nengine.reset_metrics()\n\n# Run tests\nfor i in range(100):\n    engine.infer(f\"Test {i}\", max_tokens=50)\n\n# Get clean metrics\nmetrics = engine.get_metrics()\nprint(f\"Benchmark: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n</code></pre>"},{"location":"api/inference-engine/#unload_model","title":"<code>unload_model()</code>","text":"<p>Unload the current model and stop server.</p> <pre><code>def unload_model() -&gt; None\n</code></pre> <p>Example:</p> <pre><code># Unload when done\nengine.unload_model()\n\n# Load different model\nengine.load_model(\"other-model.gguf\", auto_start=True)\n</code></pre>"},{"location":"api/inference-engine/#properties","title":"Properties","text":""},{"location":"api/inference-engine/#is_loaded","title":"<code>is_loaded</code>","text":"<p>Check if a model is currently loaded.</p> <pre><code>@property\ndef is_loaded() -&gt; bool\n</code></pre> <p>Example:</p> <pre><code>if engine.is_loaded:\n    print(\"Model is loaded\")\n    result = engine.infer(\"Test\", max_tokens=10)\nelse:\n    print(\"No model loaded\")\n    engine.load_model(\"model.gguf\")\n</code></pre>"},{"location":"api/inference-engine/#context-manager-support","title":"Context Manager Support","text":"<p><code>InferenceEngine</code> supports context manager protocol for automatic cleanup.</p> <p>Example:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\n    result = engine.infer(\"Hello!\", max_tokens=50)\n    print(result.text)\n\n# Server automatically stops when exiting context\n</code></pre>"},{"location":"api/inference-engine/#inferresult-class","title":"InferResult Class","text":"<p>Result object returned by inference methods.</p>"},{"location":"api/inference-engine/#properties_1","title":"Properties","text":"Property Type Description <code>success</code> <code>bool</code> Whether inference succeeded <code>text</code> <code>str</code> Generated text <code>tokens_generated</code> <code>int</code> Number of tokens generated <code>latency_ms</code> <code>float</code> Inference latency in milliseconds <code>tokens_per_sec</code> <code>float</code> Generation throughput <code>error_message</code> <code>str</code> Error message if failed"},{"location":"api/inference-engine/#example","title":"Example:","text":"<pre><code>result = engine.infer(\"Test\", max_tokens=50)\n\n# Access properties\nprint(f\"Text: {result.text}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Tokens: {result.tokens_generated}\")\nprint(f\"Latency: {result.latency_ms:.0f} ms\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# String representation\nprint(str(result))  # Returns result.text\n\n# Repr\nprint(repr(result))\n# Output: InferResult(tokens=50, latency=745.2ms, throughput=134.2 tok/s)\n</code></pre>"},{"location":"api/inference-engine/#complete-example","title":"Complete Example","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model with auto-configuration\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n\n# Single inference\nresult = engine.infer(\n    prompt=\"What is machine learning?\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Batch inference\nprompts = [\n    \"Explain AI\",\n    \"What are neural networks?\",\n    \"Define deep learning\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\nfor i, r in enumerate(results):\n    print(f\"\\nPrompt {i+1}: {r.text}\")\n\n# Get metrics\nmetrics = engine.get_metrics()\nprint(f\"\\nTotal speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n\n# Cleanup\nengine.unload_model()\n</code></pre>"},{"location":"api/inference-engine/#see-also","title":"See Also","text":"<ul> <li>Models API - Model management</li> <li>Device API - GPU device management</li> <li>Examples - More code examples</li> <li>Performance Guide - Optimization tips</li> </ul>"},{"location":"api/models/","title":"Models API Reference","text":"<p>API documentation for model management, discovery, and downloading in llcuda.</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>The <code>llcuda.models</code> module provides utilities for:</p> <ul> <li>Loading models from registry or HuggingFace</li> <li>Downloading and caching GGUF models</li> <li>Getting model metadata and information</li> <li>Recommending optimal inference settings</li> </ul>"},{"location":"api/models/#functions","title":"Functions","text":""},{"location":"api/models/#load_model_smart","title":"<code>load_model_smart()</code>","text":"<p>Smart model loading with automatic download and path resolution.</p> <pre><code>def load_model_smart(\n    model_name_or_path: str,\n    interactive: bool = True\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_name_or_path</code> <code>str</code> Required Model name, HF repo, or local path <code>interactive</code> <code>bool</code> <code>True</code> Ask for confirmation before downloading <p>Returns:</p> <ul> <li><code>Path</code> - Path to model file</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import load_model_smart\n\n# Load from registry\nmodel_path = load_model_smart(\"gemma-3-1b-Q4_K_M\")\n\n# Load from HuggingFace\nmodel_path = load_model_smart(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Load local file\nmodel_path = load_model_smart(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"api/models/#download_model","title":"<code>download_model()</code>","text":"<p>Download a model from HuggingFace.</p> <pre><code>def download_model(\n    repo_id: str,\n    filename: str,\n    cache_dir: Optional[str] = None\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>repo_id</code> <code>str</code> Required HuggingFace repository ID <code>filename</code> <code>str</code> Required Model filename to download <code>cache_dir</code> <code>Optional[str]</code> <code>None</code> Custom cache directory <p>Returns:</p> <ul> <li><code>Path</code> - Path to downloaded model</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import download_model\n\n# Download from HuggingFace\nmodel_path = download_model(\n    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n    filename=\"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"api/models/#list_registry_models","title":"<code>list_registry_models()</code>","text":"<p>List available models in the registry.</p> <pre><code>def list_registry_models() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Returns:</p> <ul> <li><code>List[Dict]</code> - List of model information dictionaries</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import list_registry_models\n\nmodels = list_registry_models()\nfor model in models:\n    print(f\"{model['name']}: {model['description']}\")\n</code></pre>"},{"location":"api/models/#classes","title":"Classes","text":""},{"location":"api/models/#modelinfo","title":"<code>ModelInfo</code>","text":"<p>Extract metadata from GGUF models.</p> <pre><code>class ModelInfo:\n    def __init__(self, filepath: str)\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>filepath</code> <code>Path</code> Path to GGUF file <code>architecture</code> <code>Optional[str]</code> Model architecture (e.g., \"llama\", \"gemma\") <code>parameter_count</code> <code>Optional[int]</code> Estimated parameter count <code>context_length</code> <code>Optional[int]</code> Maximum context length <code>quantization</code> <code>Optional[str]</code> Quantization type <code>file_size_mb</code> <code>float</code> File size in MB <p>Methods:</p>"},{"location":"api/models/#get_recommended_settings","title":"<code>get_recommended_settings()</code>","text":"<p>Get recommended inference settings based on model and hardware.</p> <pre><code>def get_recommended_settings(\n    vram_gb: float = 8.0\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>vram_gb</code> <code>float</code> <code>8.0</code> Available VRAM in GB <p>Returns:</p> <pre><code>{\n    'gpu_layers': int,\n    'ctx_size': int,\n    'batch_size': int,\n    'ubatch_size': int\n}\n</code></pre> <p>Example:</p> <pre><code>from llcuda.models import ModelInfo\n\n# Load model info\ninfo = ModelInfo(\"gemma-3-1b-Q4_K_M.gguf\")\n\nprint(f\"Architecture: {info.architecture}\")\nprint(f\"Parameters: {info.parameter_count}B\")\nprint(f\"Context: {info.context_length}\")\nprint(f\"Size: {info.file_size_mb:.1f} MB\")\n\n# Get recommended settings for T4 (15GB)\nsettings = info.get_recommended_settings(vram_gb=15.0)\nprint(f\"Recommended gpu_layers: {settings['gpu_layers']}\")\nprint(f\"Recommended ctx_size: {settings['ctx_size']}\")\n</code></pre>"},{"location":"api/models/#model-registry","title":"Model Registry","text":"<p>llcuda includes a built-in model registry with popular models:</p> <pre><code>REGISTRY = {\n    \"gemma-3-1b-Q4_K_M\": {\n        \"repo\": \"unsloth/gemma-3-1b-it-GGUF\",\n        \"file\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n        \"size\": \"700 MB\",\n        \"description\": \"Gemma 3 1B Instruct, Q4_K_M quantized\"\n    },\n    \"llama-3.2-3b-Q4_K_M\": {\n        \"repo\": \"unsloth/Llama-3.2-3B-Instruct-GGUF\",\n        \"file\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n        \"size\": \"1.9 GB\",\n        \"description\": \"Llama 3.2 3B Instruct, Q4_K_M\"\n    }\n}\n</code></pre>"},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>InferenceEngine API</li> <li>Model Selection Guide</li> <li>GGUF Format</li> </ul>"},{"location":"api/new-apis/","title":"New APIs (v2.1+)","text":"<p>llcuda v2.1+ introduces four comprehensive API modules for advanced LLM inference optimization.</p>"},{"location":"api/new-apis/#overview","title":"Overview","text":"<p>The new APIs provide:</p> <ol> <li>Quantization - NF4, GGUF conversion, dynamic quantization</li> <li>Unsloth Integration - Seamless fine-tuning to deployment</li> <li>CUDA Optimization - Tensor Cores, CUDA Graphs, Triton kernels</li> <li>Advanced Inference - FlashAttention, KV-cache, batch optimization</li> </ol>"},{"location":"api/new-apis/#quick-examples","title":"Quick Examples","text":""},{"location":"api/new-apis/#quantization","title":"Quantization","text":"<pre><code>from llcuda.quantization import DynamicQuantizer\n\n# Auto-select optimal quantization\nquantizer = DynamicQuantizer(target_vram_gb=12.0)\nconfig = quantizer.recommend_config(model_size_gb=3.0)\n\nprint(f\"Use: {config['quant_type']}\")  # Q4_K_M\n</code></pre>"},{"location":"api/new-apis/#unsloth-integration","title":"Unsloth Integration","text":"<pre><code>from llcuda.unsloth import export_to_llcuda\n\n# Export fine-tuned model\nexport_to_llcuda(\n    model=model,\n    tokenizer=tokenizer,\n    output_path=\"model.gguf\",\n    quant_type=\"Q4_K_M\"\n)\n</code></pre>"},{"location":"api/new-apis/#cuda-optimization","title":"CUDA Optimization","text":"<pre><code>from llcuda.cuda import enable_tensor_cores\n\n# Enable Tensor Cores (2-4x speedup)\nenable_tensor_cores(dtype=torch.float16)\n</code></pre>"},{"location":"api/new-apis/#advanced-inference","title":"Advanced Inference","text":"<pre><code>from llcuda.inference import get_optimal_context_length\n\n# Get optimal context for your VRAM\nctx_len = get_optimal_context_length(\n    model_size_b=3.0,\n    available_vram_gb=12.0,\n    use_flash_attention=True\n)\n</code></pre>"},{"location":"api/new-apis/#detailed-documentation","title":"Detailed Documentation","text":"<p>For complete API reference, see:</p> <ul> <li>Quantization API</li> <li>Unsloth Integration</li> <li>CUDA Optimization</li> <li>Advanced Inference</li> </ul>"},{"location":"api/new-apis/#performance-impact","title":"Performance Impact","text":"Optimization Benefit Tensor Cores 2-4x speedup CUDA Graphs 20-40% latency \u2193 FlashAttention 2-3x for long ctx Q4_K_M Quant 8.5x compression"},{"location":"api/new-apis/#migration-from-v20","title":"Migration from v2.0","text":"<p>No breaking changes! All v2.0 code still works.</p> <p>Before (v2.0): <pre><code>import llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n</code></pre></p> <p>After (v2.1+) - Same code + optional optimizations: <pre><code>import llcuda\nfrom llcuda.cuda import enable_tensor_cores\n\nenable_tensor_cores()  # NEW: 2-4x faster!\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n</code></pre></p>"},{"location":"api/new-apis/#complete-workflow","title":"Complete Workflow","text":"<pre><code>from unsloth import FastLanguageModel\nfrom llcuda.unsloth import export_to_llcuda\nfrom llcuda.cuda import enable_tensor_cores\nimport llcuda\n\n# 1. Train with Unsloth\nmodel, tokenizer = FastLanguageModel.from_pretrained(\"base\")\n# ... training ...\n\n# 2. Export to GGUF\nexport_to_llcuda(model, tokenizer, \"model.gguf\")\n\n# 3. Deploy with optimizations\nenable_tensor_cores()\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\")\n\n# 4. Infer\nresult = engine.infer(\"Hello!\")\nprint(f\"{result.text} ({result.tokens_per_sec:.1f} tok/s)\")\n</code></pre>"},{"location":"api/new-apis/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Quick Start Guide</li> <li>Read the Complete API Reference</li> <li>Explore Examples</li> </ul>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Complete API documentation for llcuda v2.1.0.</p>"},{"location":"api/overview/#main-components","title":"Main Components","text":"<p>llcuda provides a simple, PyTorch-style API for GPU-accelerated LLM inference.</p>"},{"location":"api/overview/#core-classes","title":"Core Classes","text":"Class Purpose Documentation <code>InferenceEngine</code> Main interface for model loading and inference Details <code>InferenceResult</code> Container for inference results with metrics Details"},{"location":"api/overview/#utility-functions","title":"Utility Functions","text":"Function Purpose Documentation <code>check_gpu_compatibility()</code> Verify GPU support Details <code>get_device_properties()</code> Get GPU device information Details"},{"location":"api/overview/#quick-api-reference","title":"Quick API Reference","text":""},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\"What is AI?\", max_tokens=100)\n\n# Access results\nprint(result.text)                    # Generated text\nprint(result.tokens_per_sec)          # Speed in tokens/sec\nprint(result.latency_ms)              # Latency in milliseconds\nprint(result.tokens_generated)        # Number of tokens generated\n</code></pre>"},{"location":"api/overview/#inferenceengine-methods","title":"InferenceEngine Methods","text":""},{"location":"api/overview/#__init__server_urlnone","title":"<code>__init__(server_url=None)</code>","text":"<p>Create a new inference engine instance.</p> <p>Parameters: - <code>server_url</code> (str, optional): Custom llama-server URL. Default: <code>http://127.0.0.1:8090</code></p>"},{"location":"api/overview/#load_modelmodel_path-silentfalse-auto_starttrue-kwargs","title":"<code>load_model(model_path, silent=False, auto_start=True, **kwargs)</code>","text":"<p>Load a GGUF model for inference.</p> <p>Parameters: - <code>model_path</code> (str): Model identifier or path   - HuggingFace: <code>\"unsloth/repo-name:filename.gguf\"</code>   - Registry: <code>\"gemma-3-1b-Q4_K_M\"</code>   - Local: <code>\"/path/to/model.gguf\"</code> - <code>silent</code> (bool): Suppress llama-server output. Default: <code>False</code> - <code>auto_start</code> (bool): Start server automatically. Default: <code>True</code> - <code>**kwargs</code>: Additional options (context_size, gpu_layers, etc.)</p>"},{"location":"api/overview/#inferprompt-max_tokens512-temperature07-kwargs","title":"<code>infer(prompt, max_tokens=512, temperature=0.7, **kwargs)</code>","text":"<p>Run inference on a single prompt.</p> <p>Parameters: - <code>prompt</code> (str): Input text - <code>max_tokens</code> (int): Maximum tokens to generate. Default: 512 - <code>temperature</code> (float): Sampling temperature. Default: 0.7 - <code>top_p</code> (float): Nucleus sampling threshold. Default: 0.9 - <code>top_k</code> (int): Top-k sampling. Default: 40 - <code>stop_sequences</code> (list): Stop generation at these sequences</p> <p>Returns: - <code>InferenceResult</code>: Result object with text and metrics</p>"},{"location":"api/overview/#batch_inferprompts-max_tokens512-kwargs","title":"<code>batch_infer(prompts, max_tokens=512, **kwargs)</code>","text":"<p>Run inference on multiple prompts.</p> <p>Parameters: - <code>prompts</code> (list[str]): List of input texts - <code>max_tokens</code> (int): Maximum tokens per prompt - <code>**kwargs</code>: Same as <code>infer()</code></p> <p>Returns: - <code>list[InferenceResult]</code>: List of results</p>"},{"location":"api/overview/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get aggregated performance metrics.</p> <p>Returns: - <code>dict</code>: Metrics dictionary with throughput and latency stats</p>"},{"location":"api/overview/#inferenceresult-attributes","title":"InferenceResult Attributes","text":"Attribute Type Description <code>text</code> str Generated text <code>tokens_per_sec</code> float Generation speed <code>latency_ms</code> float Total latency in ms <code>tokens_generated</code> int Number of tokens"},{"location":"api/overview/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api/overview/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if current GPU is compatible with llcuda.</p> <p>Returns: <pre><code>{\n    'gpu_name': str,          # e.g., \"Tesla T4\"\n    'compute_capability': str, # e.g., \"7.5\"\n    'compatible': bool,       # True if supported\n    'platform': str          # e.g., \"colab\", \"local\"\n}\n</code></pre></p> <p>Example: <pre><code>compat = llcuda.check_gpu_compatibility()\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} may not work\")\n</code></pre></p>"},{"location":"api/overview/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>InferenceEngine - Complete InferenceEngine documentation</li> <li>Models &amp; GGUF - Model loading and GGUF format</li> <li>GPU &amp; Device - GPU management and compatibility</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"api/overview/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Tutorials</li> <li>Performance Benchmarks</li> </ul>"},{"location":"guides/faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about llcuda v2.1.0.</p>"},{"location":"guides/faq/#general-questions","title":"General Questions","text":""},{"location":"guides/faq/#what-is-llcuda","title":"What is llcuda?","text":"<p>llcuda is a Python library for fast LLM inference on NVIDIA GPUs, specifically optimized for Tesla T4. It provides:</p> <ul> <li>Pre-built CUDA binaries with FlashAttention</li> <li>One-step installation from GitHub</li> <li>134 tokens/sec on Gemma 3-1B (verified)</li> <li>Simple Python API for inference</li> <li>Auto-downloading of models and binaries</li> </ul>"},{"location":"guides/faq/#why-tesla-t4-only","title":"Why Tesla T4 only?","text":"<p>llcuda v2.1.0 is optimized exclusively for Tesla T4 (compute capability 7.5) to maximize performance:</p> <ul> <li>Tensor Core optimizations for SM 7.5</li> <li>FlashAttention tuned for Turing architecture</li> <li>Binary size reduction (266 MB vs 500+ MB for multi-GPU)</li> <li>Guaranteed compatibility</li> </ul> <p>For other GPUs, use llcuda v1.2.2 which supports SM 5.0-8.9.</p>"},{"location":"guides/faq/#how-does-llcuda-compare-to-other-solutions","title":"How does llcuda compare to other solutions?","text":"Solution Speed (Gemma 3-1B) Setup Ease of Use llcuda v2.1.0 134 tok/s 1 min Excellent transformers 45 tok/s 5 min Good vLLM 85 tok/s 10 min Moderate llama.cpp CLI 128 tok/s 15 min Moderate <p>llcuda is 3x faster than PyTorch and easiest to set up.</p>"},{"location":"guides/faq/#installation","title":"Installation","text":""},{"location":"guides/faq/#how-do-i-install-llcuda","title":"How do I install llcuda?","text":"<pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>Binaries auto-download on first import (~266 MB).</p>"},{"location":"guides/faq/#do-i-need-to-install-cuda-toolkit","title":"Do I need to install CUDA Toolkit?","text":"<p>No! llcuda includes all necessary CUDA binaries. You only need:</p> <ul> <li>NVIDIA driver (pre-installed in Google Colab)</li> <li>CUDA runtime (pre-installed in Colab)</li> <li>Python 3.11+</li> </ul>"},{"location":"guides/faq/#can-i-install-from-pypi","title":"Can I install from PyPI?","text":"<p>llcuda v2.1.0 is GitHub-only for now. Use: <pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre></p>"},{"location":"guides/faq/#why-do-binaries-download-on-first-import","title":"Why do binaries download on first import?","text":"<p>To keep the pip package small (~62 KB), CUDA binaries (266 MB) download automatically on first import from GitHub Releases. This is a one-time download, then cached locally.</p>"},{"location":"guides/faq/#compatibility","title":"Compatibility","text":""},{"location":"guides/faq/#which-gpus-are-supported","title":"Which GPUs are supported?","text":"<p>llcuda v2.1.0: Tesla T4 only (SM 7.5)</p> <p>llcuda v1.2.2: All GPUs with SM 5.0+ (Maxwell through Ada Lovelace)</p>"},{"location":"guides/faq/#can-i-use-llcuda-on-cpu","title":"Can I use llcuda on CPU?","text":"<p>Yes, but not recommended. Set <code>gpu_layers=0</code> for CPU mode. Performance drops from 134 tok/s to ~8 tok/s.</p>"},{"location":"guides/faq/#does-llcuda-work-on-windows","title":"Does llcuda work on Windows?","text":"<p>llcuda v2.1.0 is Linux-only (Google Colab, Ubuntu). For Windows, compile from source or use WSL2.</p>"},{"location":"guides/faq/#what-python-versions-are-supported","title":"What Python versions are supported?","text":"<p>Python 3.11+ is required. Tested on Python 3.10, 3.11, and 3.12.</p>"},{"location":"guides/faq/#what-cuda-versions-are-supported","title":"What CUDA versions are supported?","text":"<p>CUDA 12.0+ required. Tested with CUDA 12.2, 12.4.</p>"},{"location":"guides/faq/#models","title":"Models","text":""},{"location":"guides/faq/#which-models-can-i-use","title":"Which models can I use?","text":"<p>Any GGUF model compatible with llama.cpp:</p> <ul> <li>Gemma (1B, 2B, 3B, 7B)</li> <li>Llama (3.1, 3.2, 3.3)</li> <li>Qwen (1.5B, 7B, 14B)</li> <li>Mistral (7B, 8x7B)</li> <li>Phi (2, 3)</li> </ul>"},{"location":"guides/faq/#what-quantization-should-i-use","title":"What quantization should I use?","text":"<p>Q4_K_M for best performance/quality balance on T4:</p> <ul> <li>Speed: 134 tok/s</li> <li>VRAM: 1.2 GB (Gemma 3-1B)</li> <li>Quality: &lt; 1% degradation</li> </ul> <p>Other options: - Q5_K_M: Better quality, 18% slower - Q8_0: Best quality, 44% slower</p>"},{"location":"guides/faq/#how-do-i-load-a-model-from-huggingface","title":"How do I load a model from HuggingFace?","text":"<pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"guides/faq/#can-i-use-my-fine-tuned-models","title":"Can I use my fine-tuned models?","text":"<p>Yes! Export to GGUF using Unsloth:</p> <pre><code># After fine-tuning with Unsloth\nmodel.save_pretrained_gguf(\n    \"my-model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# Load with llcuda\nengine.load_model(\"my-model-Q4_K_M.gguf\")\n</code></pre> <p>See Unsloth Integration for details.</p>"},{"location":"guides/faq/#performance","title":"Performance","text":""},{"location":"guides/faq/#what-performance-can-i-expect","title":"What performance can I expect?","text":"<p>On Tesla T4 with Q4_K_M quantization:</p> <ul> <li>Gemma 3-1B: 134 tok/s (verified)</li> <li>Llama 3.2-3B: ~48 tok/s (estimated)</li> <li>Qwen 2.5-7B: ~21 tok/s (estimated)</li> <li>Llama 3.1-8B: ~19 tok/s (estimated)</li> </ul>"},{"location":"guides/faq/#why-is-my-inference-slow","title":"Why is my inference slow?","text":"<p>Common causes:</p> <ol> <li>Not using T4: Other GPUs need v1.2.2</li> <li>Low GPU offload: Set <code>gpu_layers=99</code></li> <li>Wrong quantization: Use Q4_K_M</li> <li>Large context: Reduce <code>ctx_size</code> to 2048</li> <li>CPU mode: Check <code>nvidia-smi</code> shows GPU usage</li> </ol> <p>See Troubleshooting for solutions.</p>"},{"location":"guides/faq/#how-can-i-optimize-performance","title":"How can I optimize performance?","text":"<pre><code># Optimal configuration for T4\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,        # Full GPU offload\n    ctx_size=2048,        # Balanced context\n    batch_size=512,       # Optimal batch\n    ubatch_size=128,\n    auto_configure=True   # Let llcuda optimize\n)\n</code></pre> <p>See Performance Tutorial for details.</p>"},{"location":"guides/faq/#does-llcuda-support-batching","title":"Does llcuda support batching?","text":"<p>Yes: <pre><code>prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = engine.batch_infer(prompts, max_tokens=100)\n</code></pre></p> <p>For concurrent requests, use <code>n_parallel</code>: <pre><code>engine.load_model(\"model.gguf\", n_parallel=4)\n</code></pre></p>"},{"location":"guides/faq/#memory","title":"Memory","text":""},{"location":"guides/faq/#how-much-vram-do-i-need","title":"How much VRAM do I need?","text":"<p>Depends on model size and quantization:</p> Model Q4_K_M Q5_K_M Q8_0 1B 1.2 GB 1.5 GB 2.5 GB 3B 2.0 GB 2.4 GB 4.2 GB 7B 5.0 GB 6.0 GB 10 GB 8B 5.5 GB 6.5 GB 11 GB <p>Tesla T4 has 15 GB, sufficient for models up to 7-8B.</p>"},{"location":"guides/faq/#can-i-run-multiple-models-simultaneously","title":"Can I run multiple models simultaneously?","text":"<p>Yes, on different ports:</p> <pre><code># Model 1\nengine1 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nengine1.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Model 2\nengine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\nengine2.load_model(\"llama-3.2-3b-Q4_K_M\")\n</code></pre> <p>Watch total VRAM usage with <code>nvidia-smi</code>.</p>"},{"location":"guides/faq/#what-if-i-run-out-of-vram","title":"What if I run out of VRAM?","text":"<ol> <li>Use smaller model (1B instead of 3B)</li> <li>Use Q4_K_M instead of Q8_0</li> <li>Reduce <code>gpu_layers</code> (e.g., 20 instead of 99)</li> <li>Reduce <code>ctx_size</code> (e.g., 1024 instead of 4096)</li> <li>Close other GPU applications</li> </ol>"},{"location":"guides/faq/#usage","title":"Usage","text":""},{"location":"guides/faq/#how-do-i-run-inference","title":"How do I run inference?","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\nresult = engine.infer(\"What is AI?\", max_tokens=100)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/faq/#can-i-stream-outputs","title":"Can I stream outputs?","text":"<p>Yes: <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    \"Write a story:\",\n    callback=print_chunk,\n    max_tokens=200\n)\n</code></pre></p>"},{"location":"guides/faq/#how-do-i-stop-generation-early","title":"How do I stop generation early?","text":"<p>Use <code>stop_sequences</code>: <pre><code>result = engine.infer(\n    \"List items:\",\n    max_tokens=200,\n    stop_sequences=[\"\\n\\n\", \"###\"]\n)\n</code></pre></p>"},{"location":"guides/faq/#can-i-control-randomness","title":"Can I control randomness?","text":"<p>Yes, with <code>temperature</code> and <code>seed</code>: <pre><code># Deterministic\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.1,\n    seed=42\n)\n\n# Creative\nresult = engine.infer(\n    \"Prompt\",\n    temperature=1.0,\n    top_k=100\n)\n</code></pre></p>"},{"location":"guides/faq/#google-colab","title":"Google Colab","text":""},{"location":"guides/faq/#does-llcuda-work-in-google-colab","title":"Does llcuda work in Google Colab?","text":"<p>Yes! llcuda is optimized for Colab T4:</p> <pre><code># In Colab\n!pip install git+https://github.com/waqasm86/llcuda.git\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n</code></pre>"},{"location":"guides/faq/#how-do-i-get-t4-in-colab","title":"How do I get T4 in Colab?","text":"<p>Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU &gt; GPU type &gt; T4</p>"},{"location":"guides/faq/#do-i-need-colab-pro","title":"Do I need Colab Pro?","text":"<p>No, but Colab Pro provides:</p> <ul> <li>Guaranteed T4 access</li> <li>Longer runtime (24h vs 12h)</li> <li>More RAM</li> <li>Priority execution</li> </ul> <p>Free tier works but T4 availability varies.</p>"},{"location":"guides/faq/#can-i-save-models-between-sessions","title":"Can I save models between sessions?","text":"<p>Models cache to <code>~/.cache/llcuda/</code>. In Colab, this resets. Use:</p> <pre><code># Save to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy model\n!cp ~/.cache/llcuda/models/gemma-3-1b*.gguf /content/drive/MyDrive/\n\n# Next session: load from Drive\nengine.load_model(\"/content/drive/MyDrive/gemma-3-1b-Q4_K_M.gguf\")\n</code></pre>"},{"location":"guides/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/faq/#import-fails-with-no-module-named-llcuda","title":"Import fails with \"No module named llcuda\"","text":"<pre><code># Reinstall\npip uninstall llcuda -y\npip install git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/faq/#binary-download-fails","title":"Binary download fails","text":"<p>See Troubleshooting Guide</p>"},{"location":"guides/faq/#server-wont-start","title":"Server won't start","text":"<p>Check port 8090 availability or use different port: <pre><code>engine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/faq/#performance-is-slow","title":"Performance is slow","text":"<p>See Performance Troubleshooting</p>"},{"location":"guides/faq/#contributing","title":"Contributing","text":""},{"location":"guides/faq/#can-i-contribute-to-llcuda","title":"Can I contribute to llcuda?","text":"<p>Yes! Contributions welcome:</p> <ul> <li>Bug reports: GitHub Issues</li> <li>Feature requests: Open an issue</li> <li>Code: Fork and submit PR</li> <li>Documentation: Help improve docs</li> </ul>"},{"location":"guides/faq/#how-do-i-build-binaries","title":"How do I build binaries?","text":"<p>See Build Binaries Tutorial</p>"},{"location":"guides/faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<p>Open a GitHub Issue with:</p> <ul> <li>llcuda version</li> <li>GPU model</li> <li>CUDA version</li> <li>Python version</li> <li>Error message</li> <li>Minimal reproducible code</li> </ul>"},{"location":"guides/faq/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start</li> <li>First Steps</li> <li>Troubleshooting</li> <li>Performance Optimization</li> <li>API Reference</li> </ul>"},{"location":"guides/faq/#still-have-questions","title":"Still have questions?","text":"<p>Ask on GitHub Discussions or open an issue.</p>"},{"location":"guides/first-steps/","title":"First Steps with llcuda","text":"<p>After installing llcuda, this guide will help you get started with your first inference tasks and understand the core concepts.</p>"},{"location":"guides/first-steps/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have:</p> <ul> <li> Installed llcuda v2.1.0 from GitHub</li> <li> Tesla T4 GPU (Google Colab or compatible)</li> <li> CUDA 12.x runtime (pre-installed in Colab)</li> <li> Python 3.11+</li> </ul>"},{"location":"guides/first-steps/#verify-installation","title":"Verify Installation","text":"<p>First, verify that llcuda is installed correctly:</p> <pre><code>import llcuda\n\nprint(f\"llcuda version: {llcuda.__version__}\")\n# Output: llcuda version: 2.1.0\n</code></pre>"},{"location":"guides/first-steps/#check-gpu-availability","title":"Check GPU Availability","text":"<p>Verify your GPU is compatible:</p> <pre><code>import llcuda\nfrom llcuda.core import get_device_properties\n\n# Check CUDA availability\nif llcuda.check_cuda_available():\n    print(\"\u2713 CUDA is available\")\nelse:\n    print(\"\u2717 CUDA not available\")\n\n# Get device information\nprops = get_device_properties(0)\nprint(f\"GPU: {props.name}\")\nprint(f\"Compute: SM {props.compute_capability_major}.{props.compute_capability_minor}\")\nprint(f\"Memory: {props.total_global_mem / (1024**3):.1f} GB\")\n</code></pre> <p>Expected Output (Google Colab T4): <pre><code>\u2713 CUDA is available\nGPU: Tesla T4\nCompute: SM 7.5\nMemory: 14.8 GB\n</code></pre></p> <p>Compatibility Check</p> <p>llcuda v2.1.0 is optimized exclusively for Tesla T4 GPUs (SM 7.5). If you see a different GPU, consider using llcuda v1.2.2 for broader compatibility.</p>"},{"location":"guides/first-steps/#your-first-inference","title":"Your First Inference","text":"<p>Let's run a simple inference using the <code>InferenceEngine</code> class:</p>"},{"location":"guides/first-steps/#step-1-create-the-engine","title":"Step 1: Create the Engine","text":"<pre><code>import llcuda\n\n# Create inference engine\nengine = llcuda.InferenceEngine()\n</code></pre>"},{"location":"guides/first-steps/#step-2-load-a-model","title":"Step 2: Load a Model","text":"<p>llcuda supports loading models from a registry, local paths, or HuggingFace:</p> From RegistryFrom HuggingFaceFrom Local Path <pre><code># Load Gemma 3-1B Q4_K_M from registry (auto-downloads)\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n</code></pre> <pre><code># Load directly from HuggingFace\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    auto_start=True,\n    verbose=True\n)\n</code></pre> <pre><code># Load from local GGUF file\nengine.load_model(\n    \"/path/to/model.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    auto_start=True\n)\n</code></pre> <p>What happens during loading:</p> <ol> <li>Model file is downloaded/validated</li> <li>Optimal settings are auto-configured based on your GPU</li> <li>llama-server starts automatically</li> <li>Model loads into GPU memory</li> </ol>"},{"location":"guides/first-steps/#step-3-run-inference","title":"Step 3: Run Inference","text":"<pre><code># Simple inference\nresult = engine.infer(\n    prompt=\"What is artificial intelligence?\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"\\nSpeed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"Latency: {result.latency_ms:.0f} ms\")\n</code></pre> <p>Expected Output: <pre><code>Artificial intelligence (AI) is a branch of computer science that focuses on creating\nintelligent machines that can perform tasks that typically require human intelligence,\nsuch as visual perception, speech recognition, decision-making, and language translation...\n\nSpeed: 134.2 tokens/sec\nLatency: 745 ms\n</code></pre></p>"},{"location":"guides/first-steps/#understanding-key-concepts","title":"Understanding Key Concepts","text":""},{"location":"guides/first-steps/#inferenceengine","title":"InferenceEngine","text":"<p>The <code>InferenceEngine</code> class is the main interface for llcuda:</p> <pre><code>engine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\n</code></pre> <ul> <li>Manages llama-server lifecycle</li> <li>Handles model loading and configuration</li> <li>Provides high-level inference API</li> <li>Tracks performance metrics</li> </ul>"},{"location":"guides/first-steps/#model-loading-options","title":"Model Loading Options","text":"<p>llcuda provides flexible model loading:</p> Parameter Description Default <code>gpu_layers</code> Number of layers to offload to GPU Auto-configured <code>ctx_size</code> Context window size Auto-configured <code>auto_start</code> Automatically start server <code>True</code> <code>auto_configure</code> Auto-detect optimal settings <code>True</code> <code>verbose</code> Print status messages <code>True</code> <code>silent</code> Suppress llama-server output <code>False</code>"},{"location":"guides/first-steps/#inference-parameters","title":"Inference Parameters","text":"<p>Control generation with these parameters:</p> <pre><code>result = engine.infer(\n    prompt=\"Your prompt here\",\n    max_tokens=128,        # Maximum tokens to generate\n    temperature=0.7,       # Sampling temperature (0.0-2.0)\n    top_p=0.9,            # Nucleus sampling threshold\n    top_k=40,             # Top-k sampling limit\n    seed=42,              # Random seed (0=random)\n    stop_sequences=[\"\\n\"] # Stop generation at these sequences\n)\n</code></pre>"},{"location":"guides/first-steps/#working-with-results","title":"Working with Results","text":"<p>The <code>InferResult</code> object contains generation output and metrics:</p> <pre><code>result = engine.infer(\"Write a haiku about AI\")\n\n# Access generated text\nprint(result.text)\n\n# Check if inference succeeded\nif result.success:\n    print(f\"\u2713 Generated {result.tokens_generated} tokens\")\n    print(f\"\u2713 Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"\u2713 Latency: {result.latency_ms:.0f} ms\")\nelse:\n    print(f\"\u2717 Error: {result.error_message}\")\n</code></pre>"},{"location":"guides/first-steps/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently:</p> <pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"What is deep learning?\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=100)\n\nfor i, result in enumerate(results):\n    print(f\"\\n--- Prompt {i+1} ---\")\n    print(result.text)\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/first-steps/#performance-metrics","title":"Performance Metrics","text":"<p>Track inference performance:</p> <pre><code># Get current metrics\nmetrics = engine.get_metrics()\n\nprint(f\"Total requests: {metrics['throughput']['total_requests']}\")\nprint(f\"Total tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"Mean latency: {metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\n\n# Reset metrics\nengine.reset_metrics()\n</code></pre>"},{"location":"guides/first-steps/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Use context managers for automatic cleanup:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\n    result = engine.infer(\"Hello, AI!\", max_tokens=50)\n    print(result.text)\n\n# Server stops automatically when exiting context\n</code></pre>"},{"location":"guides/first-steps/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/first-steps/#quick-inference","title":"Quick Inference","text":"<p>For one-off inferences:</p> <pre><code>from llcuda import quick_infer\n\ntext = quick_infer(\n    prompt=\"What is Python?\",\n    model_path=\"gemma-3-1b-Q4_K_M\",\n    max_tokens=100,\n    auto_start=True\n)\nprint(text)\n</code></pre>"},{"location":"guides/first-steps/#streaming-generation","title":"Streaming Generation","text":"<p>For real-time output:</p> <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    prompt=\"Write a story about AI\",\n    callback=print_chunk,\n    max_tokens=200\n)\n</code></pre>"},{"location":"guides/first-steps/#model-switching","title":"Model Switching","text":"<p>Switch between models:</p> <pre><code># Unload current model\nengine.unload_model()\n\n# Load different model\nengine.load_model(\"llama-3.2-3b-Q4_K_M\", auto_start=True)\n\n# Run inference with new model\nresult = engine.infer(\"Test prompt\", max_tokens=50)\n</code></pre>"},{"location":"guides/first-steps/#auto-configuration","title":"Auto-Configuration","text":"<p>llcuda automatically configures optimal settings:</p> <pre><code># Auto-configuration enabled (default)\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_configure=True  # Detects optimal gpu_layers, ctx_size, batch_size\n)\n\n# Manual configuration\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=35,\n    ctx_size=4096,\n    auto_configure=False\n)\n</code></pre> <p>Auto-configuration analyzes:</p> <ul> <li>Model size and architecture</li> <li>Available GPU memory</li> <li>GPU compute capability</li> <li>Optimal batch sizes</li> </ul>"},{"location":"guides/first-steps/#troubleshooting-first-steps","title":"Troubleshooting First Steps","text":""},{"location":"guides/first-steps/#model-not-loading","title":"Model Not Loading","text":"<pre><code># Check model file exists\nfrom pathlib import Path\nmodel_path = Path(\"~/.cache/llcuda/models/gemma-3-1b-it-Q4_K_M.gguf\").expanduser()\nprint(f\"Model exists: {model_path.exists()}\")\n\n# Try manual download\nfrom llcuda.models import download_model\ndownload_model(\n    \"unsloth/gemma-3-1b-it-GGUF\",\n    \"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"guides/first-steps/#server-not-starting","title":"Server Not Starting","text":"<pre><code># Check if server is already running\nif engine.check_server():\n    print(\"Server is running\")\nelse:\n    print(\"Server is not running\")\n\n# Force restart\nengine.unload_model()  # Stop current server\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n</code></pre>"},{"location":"guides/first-steps/#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce GPU layers\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=20,  # Reduce from 99\n    ctx_size=1024,  # Reduce context\n    auto_configure=False\n)\n\n# Check GPU memory\nprops = get_device_properties(0)\nprint(f\"Total memory: {props.total_global_mem / (1024**3):.1f} GB\")\n</code></pre>"},{"location":"guides/first-steps/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first inference, explore:</p> <ul> <li>Model Selection Guide - Choose the right model for your task</li> <li>Performance Optimization - Maximize throughput</li> <li>API Reference - Detailed API documentation</li> <li>Unsloth Integration - Fine-tune and deploy models</li> <li>Google Colab Tutorial - Complete hands-on tutorial</li> </ul>"},{"location":"guides/first-steps/#quick-reference","title":"Quick Reference","text":"<pre><code># Basic workflow\nimport llcuda\n\n# 1. Create engine\nengine = llcuda.InferenceEngine()\n\n# 2. Load model\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\n# 3. Run inference\nresult = engine.infer(\"Your prompt\", max_tokens=100)\n\n# 4. Use result\nprint(result.text)\nprint(f\"{result.tokens_per_sec:.1f} tok/s\")\n\n# 5. Cleanup\nengine.unload_model()\n</code></pre> <p>Performance Tip</p> <p>For best performance on Tesla T4, use Q4_K_M quantization with full GPU offload (gpu_layers=99). This achieves 130+ tokens/sec on Gemma 3-1B.</p>"},{"location":"guides/first-steps/#resources","title":"Resources","text":"<ul> <li>Installation Guide</li> <li>Quick Start</li> <li>FAQ</li> <li>Troubleshooting</li> <li>GitHub Issues</li> </ul>"},{"location":"guides/gguf-format/","title":"GGUF Format Guide","text":"<p>Complete guide to the GGUF (GPT-Generated Unified Format) model format used by llcuda v2.1.0.</p>"},{"location":"guides/gguf-format/#what-is-gguf","title":"What is GGUF?","text":"<p>GGUF (GPT-Generated Unified Format) is a binary format for storing large language models developed by the llama.cpp project.</p>"},{"location":"guides/gguf-format/#key-features","title":"Key Features","text":"<p>\u2705 Single-file distribution - Everything in one portable file \u2705 Efficient storage - Compact binary format with compression \u2705 Memory mapping - Fast loading without full RAM allocation \u2705 Quantization support - Multiple precision levels (INT4, INT8, FP16) \u2705 Metadata included - Model architecture, tokenizer, and configuration \u2705 Cross-platform - Works on Linux, macOS, Windows \u2705 GPU acceleration - Full CUDA support for inference</p>"},{"location":"guides/gguf-format/#why-gguf","title":"Why GGUF?","text":"<p>GGUF replaced the older GGML format and offers significant improvements:</p> Feature GGML (Old) GGUF (Current) Metadata External files Embedded Versioning Limited Full versioning Tokenizer Separate file Included Architecture Hard-coded Dynamic Compatibility Breaking changes Forward compatible"},{"location":"guides/gguf-format/#gguf-file-structure","title":"GGUF File Structure","text":"<p>A GGUF file contains:</p> <pre><code>GGUF File (.gguf)\n\u251c\u2500\u2500 Header (magic number, version)\n\u251c\u2500\u2500 Metadata (KV pairs)\n\u2502   \u251c\u2500\u2500 Architecture (llama, gemma, qwen, etc.)\n\u2502   \u251c\u2500\u2500 Model parameters (layers, heads, etc.)\n\u2502   \u251c\u2500\u2500 Tokenizer (vocabulary, special tokens)\n\u2502   \u251c\u2500\u2500 Quantization method\n\u2502   \u2514\u2500\u2500 Author, license, source\n\u251c\u2500\u2500 Tensor info (names, shapes, offsets)\n\u2514\u2500\u2500 Tensor data (model weights)\n</code></pre>"},{"location":"guides/gguf-format/#example-gguf-metadata","title":"Example GGUF Metadata","text":"<pre><code>import llcuda\nfrom llcuda.gguf_parser import GGUFReader\n\n# Read GGUF metadata\nreader = GGUFReader(\"gemma-3-1b-it-Q4_K_M.gguf\")\n\nprint(f\"Architecture: {reader.architecture}\")\nprint(f\"Parameter count: {reader.parameter_count}\")\nprint(f\"Quantization: {reader.quantization}\")\nprint(f\"Context length: {reader.context_length}\")\n</code></pre>"},{"location":"guides/gguf-format/#quantization-types","title":"Quantization Types","text":"<p>GGUF supports multiple quantization methods that trade off quality for size and speed.</p>"},{"location":"guides/gguf-format/#quantization-comparison","title":"Quantization Comparison","text":"Type Bits Size Multiplier Quality Speed Use Case F16 16 1.0x (largest) Best Slowest Reference quality Q8_0 8 0.5x Excellent Slow High quality needed Q6_K 6 0.4x Very good Medium Balanced Q5_K_M 5 0.35x Good Medium-fast Good balance Q4_K_M 4 0.25x Good Fast Recommended Q4_K_S 4 0.25x Acceptable Fast Smaller variant Q3_K_M 3 0.2x Fair Very fast Experimental Q2_K 2 0.15x (smallest) Poor Fastest Testing only"},{"location":"guides/gguf-format/#recommended-q4_k_m","title":"Recommended: Q4_K_M","text":"<p>For Tesla T4 GPUs, Q4_K_M provides the best balance:</p> <p>\u2705 Good quality - Minimal accuracy loss vs FP16 \u2705 Fast inference - 134 tok/s on Gemma 3-1B \u2705 Small size - 4 bits per parameter \u2705 Low VRAM - Fits larger models in 16 GB</p> <p>Example sizes for Gemma 3-1B: - F16: ~2.6 GB - Q8_0: ~1.4 GB - Q4_K_M: ~650 MB \u2190 Recommended - Q2_K: ~400 MB</p>"},{"location":"guides/gguf-format/#using-gguf-models-with-llcuda","title":"Using GGUF Models with llcuda","text":""},{"location":"guides/gguf-format/#method-1-from-huggingface-recommended","title":"Method 1: From HuggingFace (Recommended)","text":"<p>Load directly from Unsloth or other HuggingFace repositories:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Load from Unsloth repository\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Format: repo_id:filename\n</code></pre> <p>Popular Unsloth GGUF models: - <code>unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf</code></p>"},{"location":"guides/gguf-format/#method-2-from-local-file","title":"Method 2: From Local File","text":"<p>Use a downloaded GGUF file:</p> <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"guides/gguf-format/#method-3-from-url","title":"Method 3: From URL","text":"<p>Direct download from any URL:</p> <pre><code>engine.load_model(\n    \"https://huggingface.co/user/repo/resolve/main/model.gguf\"\n)\n</code></pre>"},{"location":"guides/gguf-format/#converting-models-to-gguf","title":"Converting Models to GGUF","text":""},{"location":"guides/gguf-format/#from-pytorchhuggingface","title":"From PyTorch/HuggingFace","text":"<p>Use the <code>convert_hf_to_gguf.py</code> script from llama.cpp:</p> <pre><code># Clone llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n\n# Install dependencies\npip install -r requirements.txt\n\n# Convert model\npython convert_hf_to_gguf.py \\\n    /path/to/huggingface/model \\\n    --outfile model-f16.gguf \\\n    --outtype f16\n</code></pre>"},{"location":"guides/gguf-format/#from-unsloth-fine-tuned-models","title":"From Unsloth Fine-Tuned Models","text":"<p>Export directly from Unsloth:</p> <pre><code>from unsloth import FastLanguageModel\n\n# After fine-tuning\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"  # Creates Q4_K_M GGUF\n)\n\n# Output: my_model/unsloth.Q4_K_M.gguf\n</code></pre> <p>Supported quantization methods: - <code>\"f16\"</code> - Full precision - <code>\"q8_0\"</code> - 8-bit quantization - <code>\"q6_k\"</code> - 6-bit K-quant - <code>\"q5_k_m\"</code> - 5-bit K-quant medium - <code>\"q4_k_m\"</code> - 4-bit K-quant medium (recommended) - <code>\"q4_k_s\"</code> - 4-bit K-quant small - <code>\"q3_k_m\"</code> - 3-bit K-quant medium - <code>\"q2_k\"</code> - 2-bit K-quant</p>"},{"location":"guides/gguf-format/#quantizing-existing-gguf","title":"Quantizing Existing GGUF","text":"<p>Convert between quantization levels:</p> <pre><code># Using llama-quantize (included with llcuda binaries)\n~/.cache/llcuda/bin/llama-quantize \\\n    model-f16.gguf \\\n    model-q4_k_m.gguf \\\n    Q4_K_M\n</code></pre> <p>Available quantization types: <pre><code>Q4_0, Q4_1, Q5_0, Q5_1, Q8_0\nQ4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K\nIQ1_S, IQ2_XXS, IQ2_XS, IQ2_S, IQ3_XXS, IQ3_S\n</code></pre></p>"},{"location":"guides/gguf-format/#gguf-inspection-tools","title":"GGUF Inspection Tools","text":""},{"location":"guides/gguf-format/#using-llcuda","title":"Using llcuda","text":"<pre><code>from llcuda.gguf_parser import GGUFReader\n\nreader = GGUFReader(\"model.gguf\")\n\nprint(f\"Architecture: {reader.architecture}\")\nprint(f\"Quantization: {reader.quantization}\")\nprint(f\"Parameter count: {reader.parameter_count:,}\")\nprint(f\"Context length: {reader.context_length}\")\nprint(f\"Embedding size: {reader.embedding_size}\")\nprint(f\"Layers: {reader.num_layers}\")\nprint(f\"Heads: {reader.num_heads}\")\nprint(f\"File size: {reader.file_size / 1024**3:.2f} GB\")\n</code></pre>"},{"location":"guides/gguf-format/#using-llamacpp-tools","title":"Using llama.cpp Tools","text":"<pre><code># Check GGUF metadata\n~/.cache/llcuda/bin/llama-cli \\\n    --model model.gguf \\\n    --verbose\n</code></pre>"},{"location":"guides/gguf-format/#model-compatibility","title":"Model Compatibility","text":""},{"location":"guides/gguf-format/#supported-architectures","title":"Supported Architectures","text":"<p>llcuda v2.1.0 supports these model architectures via GGUF:</p> <p>\u2705 LLaMA (LLaMA, LLaMA-2, LLaMA-3, LLaMA-3.1, LLaMA-3.2) \u2705 Gemma (Gemma, Gemma-2, Gemma-3) \u2705 Qwen (Qwen, Qwen-2, Qwen-2.5) \u2705 Mistral (Mistral, Mistral-7B) \u2705 Mixtral (Mixtral 8x7B, 8x22B) \u2705 Phi (Phi-2, Phi-3) \u2705 Yi (Yi-6B, Yi-34B) \u2705 StableLM (StableLM-2, StableLM-3)</p>"},{"location":"guides/gguf-format/#checking-compatibility","title":"Checking Compatibility","text":"<pre><code>import llcuda\n\n# Check if model is compatible\ncompat = llcuda.check_model_compatibility(\"model.gguf\")\n\nprint(f\"Compatible: {compat['compatible']}\")\nprint(f\"Architecture: {compat['architecture']}\")\nprint(f\"Warnings: {compat.get('warnings', [])}\")\n</code></pre>"},{"location":"guides/gguf-format/#gguf-best-practices","title":"GGUF Best Practices","text":""},{"location":"guides/gguf-format/#1-choose-right-quantization","title":"1. Choose Right Quantization","text":"<p>For Tesla T4: - Small models (1-3B): Q4_K_M or Q5_K_M - Medium models (7-8B): Q4_K_M (fits in VRAM) - Large models (13B+): Q4_K_M or Q3_K_M (if needed)</p>"},{"location":"guides/gguf-format/#2-verify-gguf-integrity","title":"2. Verify GGUF Integrity","text":"<pre><code>from llcuda.gguf_parser import GGUFReader\n\ntry:\n    reader = GGUFReader(\"model.gguf\")\n    print(\"\u2705 Valid GGUF file\")\nexcept Exception as e:\n    print(f\"\u274c Invalid GGUF: {e}\")\n</code></pre>"},{"location":"guides/gguf-format/#3-test-before-production","title":"3. Test Before Production","text":"<pre><code># Quick test\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", silent=True)\n\nresult = engine.infer(\"Test prompt\", max_tokens=20)\nprint(f\"Output: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/gguf-format/#4-optimize-storage","title":"4. Optimize Storage","text":"<p>Use Q4_K_M for distribution: - Smaller download size - Faster loading - Good quality - Better inference speed</p>"},{"location":"guides/gguf-format/#gguf-vs-other-formats","title":"GGUF vs Other Formats","text":"Format Size Speed Compatibility Ease of Use GGUF Small Fast llama.cpp \u2705 Easy SafeTensors Large Medium PyTorch Medium PyTorch (.pt) Large Medium PyTorch only Medium ONNX Large Fast ONNX Runtime Complex TensorRT Custom Fastest NVIDIA only Complex <p>Why GGUF for llcuda: - \u2705 Smallest file size (with quantization) - \u2705 Fast inference on CPU and GPU - \u2705 Single-file distribution - \u2705 Works with llama.cpp ecosystem - \u2705 Easy to share and deploy</p>"},{"location":"guides/gguf-format/#finding-gguf-models","title":"Finding GGUF Models","text":""},{"location":"guides/gguf-format/#unsloth-huggingface","title":"Unsloth HuggingFace","text":"<p>Most popular source for GGUF models:</p> <p>https://huggingface.co/unsloth</p> <p>Example repositories: - <code>unsloth/gemma-3-1b-it-GGUF</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF</code> - <code>unsloth/Meta-Llama-3.1-8B-Instruct-GGUF</code></p>"},{"location":"guides/gguf-format/#thebloke-legacy","title":"TheBloke (Legacy)","text":"<p>Older GGUF models (pre-Unsloth era):</p> <p>https://huggingface.co/TheBloke</p>"},{"location":"guides/gguf-format/#bartowski","title":"Bartowski","text":"<p>Recent high-quality quantizations:</p> <p>https://huggingface.co/bartowski</p>"},{"location":"guides/gguf-format/#troubleshooting-gguf-issues","title":"Troubleshooting GGUF Issues","text":""},{"location":"guides/gguf-format/#issue-invalid-gguf-magic-number","title":"Issue: Invalid GGUF Magic Number","text":"<p>Error: <code>Invalid GGUF file: wrong magic number</code></p> <p>Solution: - File is corrupted or incomplete - Re-download the GGUF file - Verify SHA256 checksum</p>"},{"location":"guides/gguf-format/#issue-unsupported-quantization","title":"Issue: Unsupported Quantization","text":"<p>Error: <code>Quantization type not supported</code></p> <p>Solution: - Use Q4_K_M, Q5_K_M, or Q8_0 - Avoid experimental quantizations (IQ types) - Re-quantize with llama-quantize</p>"},{"location":"guides/gguf-format/#issue-model-too-large","title":"Issue: Model Too Large","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solution: - Use lower quantization (Q4_K_M instead of Q8_0) - Use smaller model variant - Clear GPU cache before loading</p>"},{"location":"guides/gguf-format/#advanced-gguf-topics","title":"Advanced GGUF Topics","text":""},{"location":"guides/gguf-format/#custom-metadata","title":"Custom Metadata","text":"<p>Add custom metadata to GGUF:</p> <pre><code>from llcuda.gguf_parser import GGUFWriter\n\nwriter = GGUFWriter(\"output.gguf\")\nwriter.add_metadata(\"author\", \"Your Name\")\nwriter.add_metadata(\"description\", \"Fine-tuned for specific task\")\nwriter.add_metadata(\"license\", \"MIT\")\nwriter.finalize()\n</code></pre>"},{"location":"guides/gguf-format/#merging-gguf-models","title":"Merging GGUF Models","text":"<p>Combine multiple LoRA adapters (experimental):</p> <pre><code># Using llama.cpp tools\nllama-export-lora \\\n    base-model.gguf \\\n    lora-adapter.gguf \\\n    merged-model.gguf\n</code></pre>"},{"location":"guides/gguf-format/#references","title":"References","text":"<ul> <li>GGUF Specification: github.com/ggerganov/ggml/blob/master/docs/gguf.md</li> <li>llama.cpp: github.com/ggerganov/llama.cpp</li> <li>Unsloth GGUF Export: docs.unsloth.ai/basics/saving-to-gguf</li> </ul>"},{"location":"guides/gguf-format/#next-steps","title":"Next Steps","text":"<ul> <li> Model Selection Guide - Choose the right model</li> <li> Quick Start - Start using GGUF models</li> <li> Performance - Benchmark GGUF models</li> <li> Unsloth Integration - Create GGUF from fine-tuned models</li> </ul> <p>GGUF makes LLM deployment simple and efficient! \ud83d\ude80</p>"},{"location":"guides/installation/","title":"Installation Guide","text":"<p>Install llcuda v2.1.0 directly from GitHub - No PyPI needed!</p>"},{"location":"guides/installation/#quick-install","title":"Quick Install","text":""},{"location":"guides/installation/#method-1-direct-from-github-recommended","title":"Method 1: Direct from GitHub (Recommended)","text":"<pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>This single command will:</p> <ul> <li>\u2705 Clone the latest code from GitHub</li> <li>\u2705 Install the Python package</li> <li>\u2705 Auto-download CUDA binaries (266 MB) from GitHub Releases on first import</li> </ul> <p>Recommended for most users</p> <p>This is the easiest method and works perfectly on Google Colab, Kaggle, and local systems.</p>"},{"location":"guides/installation/#method-2-install-from-specific-release","title":"Method 2: Install from Specific Release","text":"<pre><code>pip install https://github.com/waqasm86/llcuda/releases/download/v2.1.0/llcuda-2.1.0-py3-none-any.whl\n</code></pre>"},{"location":"guides/installation/#method-3-install-from-source-development","title":"Method 3: Install from Source (Development)","text":"<pre><code>git clone https://github.com/waqasm86/llcuda.git\ncd llcuda\npip install -e .\n</code></pre>"},{"location":"guides/installation/#what-gets-installed","title":"What Gets Installed","text":""},{"location":"guides/installation/#python-package","title":"Python Package","text":"<ul> <li>Source: GitHub repository (main branch or release tag)</li> <li>Size: ~100 KB (Python code only, no binaries)</li> <li>Contents: Core Python package, API, bootstrap code</li> </ul>"},{"location":"guides/installation/#cuda-binaries-auto-downloaded","title":"CUDA Binaries (Auto-Downloaded)","text":"<ul> <li>Source: GitHub Releases v2.0.6</li> <li>URL: <code>llcuda-binaries-cuda12-t4-v2.0.6.tar.gz</code></li> <li>Size: 266 MB (one-time download, cached locally)</li> <li>Triggered: On first <code>import llcuda</code></li> <li>Location: <code>~/.cache/llcuda/</code> or <code>&lt;package&gt;/binaries/</code></li> <li>Compatibility: v2.0.6 binaries work with llcuda v2.1.0+</li> </ul> <p>Binary Package Contents: <pre><code>llcuda-binaries-cuda12-t4-v2.0.6.tar.gz (266 MB)\n\u251c\u2500\u2500 bin/\n\u2502   \u251c\u2500\u2500 llama-server        (6.5 MB) - Inference server\n\u2502   \u251c\u2500\u2500 llama-cli           (4.2 MB) - Command-line interface\n\u2502   \u251c\u2500\u2500 llama-embedding     (3.3 MB) - Embedding generator\n\u2502   \u251c\u2500\u2500 llama-bench         (581 KB) - Benchmarking tool\n\u2502   \u2514\u2500\u2500 llama-quantize      (434 KB) - Model quantization\n\u2514\u2500\u2500 lib/\n    \u251c\u2500\u2500 libggml-cuda.so     (221 MB) - CUDA kernels + FlashAttention\n    \u251c\u2500\u2500 libllama.so         (2.9 MB) - Llama core library\n    \u2514\u2500\u2500 Other libraries...\n</code></pre></p>"},{"location":"guides/installation/#platform-specific-instructions","title":"Platform-Specific Instructions","text":"Google ColabLocal LinuxKaggleWindows (WSL2)"},{"location":"guides/installation/#google-colab-tesla-t4","title":"Google Colab (Tesla T4)","text":"<p>Perfect for cloud notebooks!</p> <pre><code># 1. Install\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# 2. Import (triggers binary download on first run)\nimport llcuda\n\n# 3. Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")        # Should show: Tesla T4\nprint(f\"Compatible: {compat['compatible']}\") # Should show: True\n\n# 4. Ready to use!\nengine = llcuda.InferenceEngine()\n</code></pre> <p>First Run</p> <p>The first import downloads 266 MB of binaries (takes 1-2 minutes). Subsequent sessions reuse cached binaries - instant startup!</p>"},{"location":"guides/installation/#local-linux-ubuntudebian","title":"Local Linux (Ubuntu/Debian)","text":"<p>Requirements: - Python 3.11+ - CUDA 12.x runtime - Tesla T4 GPU or compatible</p> <pre><code># 1. Ensure CUDA 12 is installed\nnvidia-smi  # Should show CUDA 12.x\n\n# 2. Install llcuda\npip install git+https://github.com/waqasm86/llcuda.git\n\n# 3. Test installation\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n# Output: 2.1.0\n</code></pre> <p>System Dependencies (usually pre-installed): <pre><code># CUDA Runtime (required)\nsudo apt install nvidia-cuda-toolkit\n\n# Python dependencies (installed automatically by pip)\n# - requests\n# - numpy\n</code></pre></p>"},{"location":"guides/installation/#kaggle-notebooks","title":"Kaggle Notebooks","text":"<pre><code># 1. Enable GPU accelerator\n# Settings \u2192 Accelerator \u2192 GPU T4 x2\n\n# 2. Install\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# 3. Import and verify\nimport llcuda\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\n\n# 4. Start using\nengine = llcuda.InferenceEngine()\n</code></pre>"},{"location":"guides/installation/#windows-with-wsl2","title":"Windows with WSL2","text":"<p>Prerequisites: - Windows 11 with WSL2 - NVIDIA GPU with CUDA support - CUDA 12.x installed in WSL2</p> <pre><code># Inside WSL2 terminal\n# 1. Verify CUDA\nnvidia-smi\n\n# 2. Install Python 3.11+\nsudo apt install python3.11 python3-pip\n\n# 3. Install llcuda\npip3 install git+https://github.com/waqasm86/llcuda.git\n\n# 4. Test\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre>"},{"location":"guides/installation/#verification","title":"Verification","text":"<p>After installation, verify everything works:</p> <pre><code>import llcuda\n\n# 1. Check version\nprint(f\"llcuda version: {llcuda.__version__}\")\n# Expected: 2.1.0\n\n# 2. Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compute Capability: SM {compat['compute_capability']}\")\nprint(f\"Platform: {compat['platform']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n\n# 3. Quick inference test\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"What is 2+2?\", max_tokens=20)\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n# Expected on T4: ~134 tok/s\n</code></pre>"},{"location":"guides/installation/#manual-binary-installation-advanced","title":"Manual Binary Installation (Advanced)","text":"<p>If automatic download fails, install binaries manually:</p> <pre><code># 1. Download binary package\nwget https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# 2. Verify checksum\necho \"5a27d2e1a73ae3d2f1d2ba8cf557b76f54200208c8df269b1bd0d9ee176bb49d  llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\" | sha256sum -c\n\n# 3. Extract to cache directory\nmkdir -p ~/.cache/llcuda\ntar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n\n# 4. Or extract to package directory\npython3 -c \"import llcuda; print(llcuda._BIN_DIR)\"\n# Extract to the printed directory\n</code></pre>"},{"location":"guides/installation/#testing-your-installation","title":"Testing Your Installation","text":""},{"location":"guides/installation/#basic-test","title":"Basic Test","text":"<pre><code>import llcuda\n\n# Should not raise any errors\nprint(\"\u2705 llcuda imported successfully\")\n</code></pre>"},{"location":"guides/installation/#gpu-test","title":"GPU Test","text":"<pre><code>compat = llcuda.check_gpu_compatibility()\nassert compat['compatible'], \"GPU not compatible!\"\nprint(f\"\u2705 GPU compatible: {compat['gpu_name']}\")\n</code></pre>"},{"location":"guides/installation/#inference-test","title":"Inference Test","text":"<pre><code>engine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"Hello!\", max_tokens=10)\nassert result.tokens_generated &gt; 0\nprint(f\"\u2705 Inference working: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/installation/#requirements","title":"Requirements","text":""},{"location":"guides/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>CUDA: 12.x runtime</li> <li>GPU: Tesla T4 (SM 7.5) - Primary target</li> <li>RAM: 4 GB minimum</li> <li>Disk: 1 GB free space (for binaries and models)</li> </ul>"},{"location":"guides/installation/#python-dependencies","title":"Python Dependencies","text":"<p>Automatically installed by pip:</p> <pre><code>requests&gt;=2.31.0\nnumpy&gt;=1.24.0\n</code></pre> <p>Optional for development:</p> <pre><code>huggingface-hub&gt;=0.19.0  # For model downloads\n</code></pre>"},{"location":"guides/installation/#upgrading","title":"Upgrading","text":""},{"location":"guides/installation/#upgrade-to-latest-version","title":"Upgrade to Latest Version","text":"<pre><code>pip install --upgrade git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#force-reinstall","title":"Force Reinstall","text":"<pre><code>pip install --upgrade --force-reinstall --no-cache-dir git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#clear-cache-and-reinstall","title":"Clear Cache and Reinstall","text":"<pre><code># Remove cached binaries\nrm -rf ~/.cache/llcuda/\n\n# Reinstall\npip uninstall llcuda -y\npip install git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"guides/installation/#uninstallation","title":"Uninstallation","text":"<pre><code># Remove Python package\npip uninstall llcuda -y\n\n# Remove cached binaries\nrm -rf ~/.cache/llcuda/\n\n# Remove package installation\npip cache purge\n</code></pre>"},{"location":"guides/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/installation/#binary-download-fails","title":"Binary Download Fails","text":"<p>Error: <code>RuntimeError: Binary download failed</code></p> <p>Solution: <pre><code># Check internet connection\nimport requests\nresponse = requests.get(\"https://github.com\")\nprint(response.status_code)  # Should be 200\n\n# Try manual download (see Manual Binary Installation above)\n</code></pre></p>"},{"location":"guides/installation/#import-error","title":"Import Error","text":"<p>Error: <code>ModuleNotFoundError: No module named 'llcuda'</code></p> <p>Solution: <pre><code># Verify installation\npip list | grep llcuda\n\n# Reinstall\npip install --force-reinstall git+https://github.com/waqasm86/llcuda.git\n</code></pre></p>"},{"location":"guides/installation/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Error: <code>RuntimeError: No CUDA GPUs detected</code></p> <p>Solution: <pre><code># Verify CUDA is working\nnvidia-smi\n\n# Check GPU visibility\npython3 -c \"import subprocess; print(subprocess.run(['nvidia-smi'], capture_output=True).stdout)\"\n</code></pre></p>"},{"location":"guides/installation/#next-steps","title":"Next Steps","text":"<ul> <li> Quick Start Guide - Get started in 5 minutes</li> <li> Google Colab Tutorial - Complete walkthrough</li> <li> Troubleshooting - Common issues and solutions</li> <li> API Reference - Detailed API documentation</li> </ul> <p>Need help? Open an issue on GitHub</p>"},{"location":"guides/model-selection/","title":"Model Selection Guide","text":"<p>Choose the right model and quantization for your use case with llcuda v2.1.0.</p>"},{"location":"guides/model-selection/#quick-recommendations","title":"Quick Recommendations","text":""},{"location":"guides/model-selection/#for-tesla-t4-15-gb","title":"For Tesla T4 (15 GB)","text":"Priority Model Quantization Speed VRAM Quality Speed Gemma 3-1B Q4_K_M 134 tok/s 1.2 GB Excellent Balance Llama 3.2-3B Q4_K_M 48 tok/s 2.0 GB Very good Quality Qwen 2.5-7B Q4_K_M 21 tok/s 5.0 GB Excellent"},{"location":"guides/model-selection/#for-limited-vram-8-gb","title":"For Limited VRAM (&lt; 8 GB)","text":"GPU VRAM Recommended Model Quantization Expected Speed 4 GB Gemma 3-1B Q4_0 ~140 tok/s 6 GB Gemma 3-1B Q4_K_M ~134 tok/s 8 GB Llama 3.2-3B Q4_K_M ~48 tok/s"},{"location":"guides/model-selection/#model-size-comparison","title":"Model Size Comparison","text":""},{"location":"guides/model-selection/#performance-vs-quality-trade-off","title":"Performance vs Quality Trade-off","text":"Model Family Size Params Tokens/sec (T4) VRAM Best For Gemma 3 1B 1.2B 134 1.2 GB Interactive apps, chatbots Llama 3.2 3B 3.2B 48 2.0 GB Balanced performance Qwen 2.5 7B 7.6B 21 5.0 GB Quality-focused tasks Llama 3.1 8B 8.0B 19 5.5 GB Production quality Mistral 7B 7.2B 22 5.2 GB Code generation"},{"location":"guides/model-selection/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"guides/model-selection/#1b-models-best-for-speed","title":"1B Models (Best for Speed)","text":"<p>Gemma 3-1B-it</p> <ul> <li>Speed: 134 tok/s (Q4_K_M)</li> <li>VRAM: 1.2 GB</li> <li>Strengths:</li> <li>Fastest inference</li> <li>Excellent for interactive chat</li> <li>Low VRAM requirements</li> <li>Good quality for size</li> <li>Weaknesses:</li> <li>Limited reasoning on complex tasks</li> <li>Shorter context understanding</li> <li>Use Cases:</li> <li>Customer service chatbots</li> <li>Quick Q&amp;A systems</li> <li>Real-time code assistance</li> <li>Mobile/edge deployment</li> </ul> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#3b-models-balanced","title":"3B Models (Balanced)","text":"<p>Llama 3.2-3B-Instruct</p> <ul> <li>Speed: 48 tok/s (Q4_K_M)</li> <li>VRAM: 2.0 GB</li> <li>Strengths:</li> <li>Good balance of speed/quality</li> <li>Better reasoning than 1B</li> <li>Handles complex instructions</li> <li>Still fast enough for real-time</li> <li>Weaknesses:</li> <li>3x slower than 1B models</li> <li>Higher VRAM usage</li> <li>Use Cases:</li> <li>Content generation</li> <li>Code explanation</li> <li>Document summarization</li> <li>Educational applications</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#7b-models-quality-focused","title":"7B Models (Quality-Focused)","text":"<p>Qwen 2.5-7B-Instruct</p> <ul> <li>Speed: 21 tok/s (Q4_K_M)</li> <li>VRAM: 5.0 GB</li> <li>Strengths:</li> <li>Excellent quality</li> <li>Strong reasoning abilities</li> <li>Great for complex tasks</li> <li>Multilingual support</li> <li>Weaknesses:</li> <li>6x slower than 1B</li> <li>Requires 5+ GB VRAM</li> <li>Use Cases:</li> <li>Research and analysis</li> <li>Complex reasoning tasks</li> <li>Technical documentation</li> <li>Multi-step problem solving</li> </ul> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n</code></pre> <p>Llama 3.1-8B-Instruct</p> <ul> <li>Speed: 19 tok/s (Q4_K_M)</li> <li>VRAM: 5.5 GB</li> <li>Strengths:</li> <li>State-of-the-art quality</li> <li>Excellent instruction following</li> <li>Long context support (128K)</li> <li>Multilingual</li> <li>Use Cases:</li> <li>Production applications</li> <li>API services</li> <li>Complex workflows</li> <li>Enterprise deployments</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#quantization-guide","title":"Quantization Guide","text":""},{"location":"guides/model-selection/#understanding-quantization-types","title":"Understanding Quantization Types","text":"Quantization Bits Speed Quality VRAM File Size Recommendation Q2_K 2.5 Fastest 85% Lowest ~30% Prototyping only Q3_K_M 3.5 Very fast 92% Very low ~40% Emergency low VRAM Q4_0 4.0 Fast 97% Low ~45% Speed priority Q4_K_M 4.5 Fast 99% Medium ~50% \u2705 Recommended Q5_K_M 5.5 Moderate 99.5% Medium-high ~60% Quality critical Q6_K 6.5 Slow 99.8% High ~70% Rarely needed Q8_0 8.0 Slower 99.95% Very high ~85% Development only F16 16.0 Slowest 100% Maximum 100% Not recommended"},{"location":"guides/model-selection/#choosing-quantization","title":"Choosing Quantization","text":"<p>For most users: <pre><code># Q4_K_M: Best overall choice\nengine.load_model(\n    \"model-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For speed-critical applications: <pre><code># Q4_0: 3-5% faster, slightly lower quality\nengine.load_model(\n    \"model-Q4_0.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For quality-critical work: <pre><code># Q5_K_M: Better quality, 20% slower\nengine.load_model(\n    \"model-Q5_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For extreme VRAM constraints: <pre><code># Q3_K_M: Smallest usable quantization\nengine.load_model(\n    \"model-Q3_K_M.gguf\",\n    silent=True\n)\n</code></pre></p>"},{"location":"guides/model-selection/#popular-model-collections","title":"Popular Model Collections","text":""},{"location":"guides/model-selection/#unsloth-models-recommended","title":"Unsloth Models (Recommended)","text":"<p>Unsloth provides optimized GGUF models on HuggingFace:</p> <p>Gemma Models: <pre><code># Gemma 3-1B (Best for speed)\n\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n\n# Gemma 2-2B\n\"unsloth/gemma-2-2b-it-GGUF:gemma-2-2b-it-Q4_K_M.gguf\"\n\n# Gemma 2-9B\n\"unsloth/gemma-2-9b-it-GGUF:gemma-2-9b-it-Q4_K_M.gguf\"\n</code></pre></p> <p>Llama Models: <pre><code># Llama 3.2-1B\n\"unsloth/Llama-3.2-1B-Instruct-GGUF:Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.2-3B\n\"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.1-8B\n\"unsloth/Llama-3.1-8B-Instruct-GGUF:Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n</code></pre></p> <p>Mistral Models: <pre><code># Mistral 7B v0.3\n\"unsloth/Mistral-7B-Instruct-v0.3-GGUF:Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n\n# Mistral Nemo 12B\n\"unsloth/Mistral-Nemo-Instruct-2407-GGUF:Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#official-huggingface-models","title":"Official HuggingFace Models","text":"<p>Qwen Models: <pre><code># Qwen 2.5-7B (Excellent quality)\n\"Qwen/Qwen2.5-7B-Instruct-GGUF:qwen2.5-7b-instruct-q4_k_m.gguf\"\n\n# Qwen 2.5-14B\n\"Qwen/Qwen2.5-14B-Instruct-GGUF:qwen2.5-14b-instruct-q4_k_m.gguf\"\n</code></pre></p> <p>Phi Models: <pre><code># Phi 3.5-Mini (3.8B)\n\"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#vram-requirements","title":"VRAM Requirements","text":""},{"location":"guides/model-selection/#model-size-to-vram-mapping","title":"Model Size to VRAM Mapping","text":"<p>For Q4_K_M quantization:</p> Model Size Q4_K_M VRAM Q5_K_M VRAM Q8_0 VRAM ctx=2048 1B 1.2 GB 1.5 GB 2.5 GB Add +0.3 GB 3B 2.0 GB 2.4 GB 4.2 GB Add +0.3 GB 7B 5.0 GB 6.2 GB 9.5 GB Add +0.5 GB 8B 5.5 GB 6.8 GB 10.2 GB Add +0.5 GB 13B 9.0 GB 11.0 GB 16.5 GB Add +0.8 GB"},{"location":"guides/model-selection/#gpu-recommendations","title":"GPU Recommendations","text":"GPU VRAM Max Model (Q4_K_M) Recommended Model Tesla T4 15 GB 7B 1B (speed) or 7B (quality) RTX 3060 12 GB 7B 3B RTX 3070 8 GB 3B 1B RTX 3080 10 GB 7B 3B RTX 3090 24 GB 13B 7B RTX 4070 12 GB 7B 3B RTX 4090 24 GB 13B 7B or 13B A100 40 GB 30B 13B A100 80 GB 70B 30B"},{"location":"guides/model-selection/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"guides/model-selection/#interactive-chatbots","title":"Interactive Chatbots","text":"<p>Priority: Speed, low latency</p> <p>Recommended: - Gemma 3-1B Q4_K_M (134 tok/s) - Llama 3.2-1B Q4_K_M (140 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#code-generation","title":"Code Generation","text":"<p>Priority: Accuracy, context understanding</p> <p>Recommended: - Qwen 2.5-7B Q4_K_M (21 tok/s) - Llama 3.1-8B Q4_K_M (19 tok/s)</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    ctx_size=4096,  # Longer context for code\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#document-summarization","title":"Document Summarization","text":"<p>Priority: Context length, quality</p> <p>Recommended: - Llama 3.1-8B Q4_K_M (128K context) - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=8192,  # Long documents\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#question-answering","title":"Question Answering","text":"<p>Priority: Accuracy, speed</p> <p>Recommended: - Llama 3.2-3B Q4_K_M (48 tok/s) - Gemma 3-1B Q4_K_M (134 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#content-generation","title":"Content Generation","text":"<p>Priority: Creativity, quality</p> <p>Recommended: - Qwen 2.5-7B Q5_K_M - Llama 3.1-8B Q5_K_M</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q5_K_M\",\n    ctx_size=4096,\n    silent=True\n)\n\n# Use creative generation settings\nresult = engine.infer(\n    prompt,\n    temperature=1.0,\n    top_p=0.95,\n    max_tokens=500\n)\n</code></pre>"},{"location":"guides/model-selection/#education-tutoring","title":"Education &amp; Tutoring","text":"<p>Priority: Accuracy, explanations</p> <p>Recommended: - Llama 3.2-3B Q4_K_M - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-capabilities","title":"Model Capabilities","text":""},{"location":"guides/model-selection/#multilingual-support","title":"Multilingual Support","text":"Model Languages Notes Gemma 3-1B English primarily Limited multilingual Llama 3.2-3B 8 languages Good multilingual Llama 3.1-8B 8 languages Excellent multilingual Qwen 2.5-7B 29 languages Best multilingual Mistral 7B English, French, German, Spanish, Italian Good European languages"},{"location":"guides/model-selection/#context-window-support","title":"Context Window Support","text":"Model Standard Context Max Context Notes Gemma 3-1B 2K 8K Limited long context Llama 3.2-3B 4K 128K Excellent long context Llama 3.1-8B 8K 128K Best long context Qwen 2.5-7B 8K 32K Good long context Mistral 7B 8K 32K Good long context"},{"location":"guides/model-selection/#special-capabilities","title":"Special Capabilities","text":"Model Code Math Reasoning Function Calling Gemma 3-1B Good Fair Fair No Llama 3.2-3B Very Good Good Good Yes Llama 3.1-8B Excellent Very Good Excellent Yes Qwen 2.5-7B Excellent Excellent Excellent Yes Mistral 7B Very Good Good Good Yes"},{"location":"guides/model-selection/#finding-and-loading-models","title":"Finding and Loading Models","text":""},{"location":"guides/model-selection/#from-unsloth-recommended","title":"From Unsloth (Recommended)","text":"<pre><code># Browse models at: https://huggingface.co/unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-official-repos","title":"From Official Repos","text":"<pre><code># Qwen\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# Microsoft Phi\nengine.load_model(\n    \"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#local-models","title":"Local Models","text":"<pre><code># Load from local path\nengine.load_model(\n    \"/path/to/model.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-evaluation","title":"Model Evaluation","text":""},{"location":"guides/model-selection/#quick-quality-test","title":"Quick Quality Test","text":"<pre><code>import llcuda\n\ndef evaluate_model(model_path):\n    \"\"\"Quick quality evaluation.\"\"\"\n\n    engine = llcuda.InferenceEngine()\n    engine.load_model(model_path, silent=True)\n\n    test_prompts = [\n        \"Explain quantum computing in simple terms.\",\n        \"Write a Python function to calculate factorial.\",\n        \"What are the causes of climate change?\",\n        \"Translate 'Hello, how are you?' to Spanish.\",\n        \"Solve: If x + 5 = 12, what is x?\"\n    ]\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating: {model_path}\")\n    print(f\"{'='*60}\\n\")\n\n    for i, prompt in enumerate(test_prompts, 1):\n        result = engine.infer(prompt, max_tokens=150)\n\n        print(f\"{i}. {prompt}\")\n        print(f\"   Response: {result.text[:100]}...\")\n        print(f\"   Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n    metrics = engine.get_metrics()\n    print(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"Average latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Test multiple models\nmodels = [\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n]\n\nfor model in models:\n    evaluate_model(model)\n</code></pre>"},{"location":"guides/model-selection/#migration-guide","title":"Migration Guide","text":""},{"location":"guides/model-selection/#from-larger-to-smaller-models","title":"From Larger to Smaller Models","text":"<p>If you need to reduce VRAM:</p> <pre><code># Before: 7B model (5 GB VRAM)\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# After: 3B model (2 GB VRAM)\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-higher-to-lower-quantization","title":"From Higher to Lower Quantization","text":"<pre><code># Before: Q5_K_M (better quality, slower)\nengine.load_model(\"model-Q5_K_M.gguf\", silent=True)\n\n# After: Q4_K_M (faster, minimal quality loss)\nengine.load_model(\"model-Q4_K_M.gguf\", silent=True)\n</code></pre>"},{"location":"guides/model-selection/#see-also","title":"See Also","text":"<ul> <li>GGUF Format - Understanding GGUF</li> <li>Performance Benchmarks - Speed comparisons</li> <li>Optimization Guide - Tuning performance</li> <li>Quick Start - Getting started</li> <li>HuggingFace Models - Browse GGUF models</li> </ul>"},{"location":"guides/quickstart/","title":"Quick Start","text":"<p>Get started with llcuda v2.1.0 in 5 minutes!</p>"},{"location":"guides/quickstart/#5-minute-quickstart","title":"5-Minute Quickstart","text":""},{"location":"guides/quickstart/#step-1-install-llcuda","title":"Step 1: Install llcuda","text":"<pre><code>pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>Google Colab Users</p> <p>Add <code>!</code> before the command: <code>!pip install -q git+https://github.com/waqasm86/llcuda.git</code></p>"},{"location":"guides/quickstart/#step-2-import-and-verify","title":"Step 2: Import and Verify","text":"<pre><code>import llcuda\n\n# Check version\nprint(f\"llcuda version: {llcuda.__version__}\")\n# Output: 2.1.0\n\n# Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n</code></pre> <p>First Import</p> <p>First <code>import llcuda</code> downloads CUDA binaries (266 MB) from GitHub Releases. This takes 1-2 minutes. Subsequent imports are instant!</p>"},{"location":"guides/quickstart/#step-3-load-a-model","title":"Step 3: Load a Model","text":"<pre><code># Create inference engine\nengine = llcuda.InferenceEngine()\n\n# Load Gemma 3-1B from Unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # Suppress llama-server output\n)\n\nprint(\"\u2705 Model loaded!\")\n</code></pre>"},{"location":"guides/quickstart/#step-4-run-inference","title":"Step 4: Run Inference","text":"<pre><code># Ask a question\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\n# Print results\nprint(f\"Response: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"Latency: {result.latency_ms:.0f}ms\")\n</code></pre> <p>Expected output on Tesla T4: <pre><code>Speed: 134.2 tokens/sec\nLatency: 690ms\n</code></pre></p>"},{"location":"guides/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete, copy-paste ready example:</p> <pre><code>import llcuda\n\n# Initialize engine\nengine = llcuda.InferenceEngine()\n\n# Load model (downloads ~800 MB on first run)\nprint(\"Loading model...\")\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Single inference\nresult = engine.infer(\n    \"What is machine learning?\",\n    max_tokens=150\n)\n\nprint(f\"Response: {result.text}\")\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/quickstart/#interactive-chat","title":"Interactive Chat","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nprint(\"Chat with Gemma 3-1B (type 'exit' to quit)\")\n\nwhile True:\n    user_input = input(\"\\nYou: \")\n    if user_input.lower() == \"exit\":\n        break\n\n    result = engine.infer(user_input, max_tokens=300)\n    print(f\"AI: {result.text}\")\n    print(f\"({result.tokens_per_sec:.1f} tok/s)\")\n</code></pre>"},{"location":"guides/quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Multiple prompts\nprompts = [\n    \"What is AI?\",\n    \"Explain neural networks.\",\n    \"Define deep learning.\"\n]\n\n# Process in batch\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"\\nQ: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/quickstart/#try-on-google-colab","title":"Try on Google Colab","text":"<p>Click the button below to try llcuda in your browser:</p> <p>What this notebook includes:</p> <ul> <li>\u2705 Complete Tesla T4 setup guide</li> <li>\u2705 GPU verification steps</li> <li>\u2705 Binary download walkthrough</li> <li>\u2705 Multiple inference examples</li> <li>\u2705 Performance benchmarking</li> <li>\u2705 Batch processing demo</li> </ul>"},{"location":"guides/quickstart/#expected-performance","title":"Expected Performance","text":"<p>On Google Colab Tesla T4:</p> Task Speed Latency Simple query 134 tok/s ~690ms Code generation 136 tok/s ~1.5s Batch (4 prompts) 135 tok/s avg ~2.4s total <p>These are verified real-world results! See the executed notebook for proof.</p>"},{"location":"guides/quickstart/#pro-tips","title":"Pro Tips","text":""},{"location":"guides/quickstart/#silent-mode","title":"Silent Mode","text":"<p>Suppress llama-server output for cleaner logs:</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # \u2190 Add this!\n)\n</code></pre>"},{"location":"guides/quickstart/#context-manager","title":"Context Manager","text":"<p>Auto-cleanup resources:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n    result = engine.infer(\"Test prompt\", max_tokens=50)\n    print(result.text)\n# Server automatically stopped here\n</code></pre>"},{"location":"guides/quickstart/#check-gpu-before-loading","title":"Check GPU Before Loading","text":"<pre><code># Verify GPU compatibility first\ncompat = llcuda.check_gpu_compatibility()\n\nif not compat['compatible']:\n    print(f\"\u26a0\ufe0f GPU {compat['gpu_name']} may not be compatible\")\n    print(f\"   llcuda is optimized for Tesla T4\")\nelse:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\n    # Proceed with loading model...\n</code></pre>"},{"location":"guides/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/quickstart/#model-download-slow","title":"Model Download Slow?","text":"<p>HuggingFace downloads can be slow. First download is cached:</p> <pre><code># First run: Downloads ~800 MB (2-3 minutes)\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n\n# Subsequent runs: Uses cached model (instant)\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n</code></pre>"},{"location":"guides/quickstart/#out-of-memory","title":"Out of Memory?","text":"<p>Try a smaller model or reduce context:</p> <pre><code># Smaller model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q2_K.gguf\",  # Q2_K instead of Q4_K_M\n    silent=True\n)\n\n# Or reduce context size\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    context_size=2048,  # Default is 4096\n    silent=True\n)\n</code></pre>"},{"location":"guides/quickstart/#binary-download-failed","title":"Binary Download Failed?","text":"<p>Manual installation:</p> <pre><code>wget https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\nmkdir -p ~/.cache/llcuda\ntar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n</code></pre>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Google Colab Tutorial</p> <p>Complete walkthrough with Tesla T4 examples</p> </li> <li> <p> API Reference</p> <p>Full API documentation and advanced features</p> </li> <li> <p> Performance Guide</p> <p>Benchmarks and optimization tips</p> </li> <li> <p> More Examples</p> <p>Additional use cases and code samples</p> </li> </ul> <p>Questions? Check the FAQ or open an issue!</p>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues with llcuda v2.1.0 on Tesla T4 GPUs.</p>"},{"location":"guides/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"guides/troubleshooting/#pip-install-fails","title":"pip install fails","text":"<p>Symptom: <pre><code>ERROR: Could not find a version that satisfies the requirement llcuda\n</code></pre></p> <p>Solution: <pre><code># Install from GitHub (not PyPI for v2.1.0)\npip install git+https://github.com/waqasm86/llcuda.git\n\n# Or use specific release\npip install https://github.com/waqasm86/llcuda/releases/download/v2.1.0/llcuda-2.1.0-py3-none-any.whl\n</code></pre></p>"},{"location":"guides/troubleshooting/#binary-download-fails","title":"Binary download fails","text":"<p>Symptom: <pre><code>Failed to download CUDA binaries: HTTP 404\n</code></pre></p> <p>Solution: <pre><code># Manually download binaries\nimport requests\nimport tarfile\nfrom pathlib import Path\n\nurl = \"https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\"\ncache_dir = Path.home() / \".cache\" / \"llcuda\"\ncache_dir.mkdir(parents=True, exist_ok=True)\n\n# Download\nresponse = requests.get(url)\ntar_path = cache_dir / \"binaries.tar.gz\"\ntar_path.write_bytes(response.content)\n\n# Extract\nwith tarfile.open(tar_path, 'r:gz') as tar:\n    tar.extractall(cache_dir)\n</code></pre></p>"},{"location":"guides/troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"guides/troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>Symptom: <pre><code>CUDA not available\nNo CUDA GPU detected\n</code></pre></p> <p>Solution: <pre><code># Check NVIDIA driver\nnvidia-smi\n\n# If fails in Colab, verify runtime type\n# Runtime &gt; Change runtime type &gt; GPU &gt; T4\n\n# Verify CUDA version\nnvcc --version  # Should show CUDA 12.x\n</code></pre></p>"},{"location":"guides/troubleshooting/#wrong-gpu-detected","title":"Wrong GPU detected","text":"<p>Symptom: <pre><code>Your GPU is not Tesla T4\nGPU: Tesla P100 (SM 6.0)\n</code></pre></p> <p>Solution: llcuda v2.1.0 is Tesla T4-only. For other GPUs, use v1.2.2: <pre><code>pip install llcuda==1.2.2\n</code></pre></p>"},{"location":"guides/troubleshooting/#model-loading-issues","title":"Model Loading Issues","text":""},{"location":"guides/troubleshooting/#model-not-found","title":"Model not found","text":"<p>Symptom: <pre><code>FileNotFoundError: Model file not found: gemma-3-1b-Q4_K_M\n</code></pre></p> <p>Solution: <pre><code># Use full HuggingFace path\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Or download manually\nfrom llcuda.models import download_model\nmodel_path = download_model(\n    \"unsloth/gemma-3-1b-it-GGUF\",\n    \"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#out-of-memory","title":"Out of memory","text":"<p>Symptom: <pre><code>CUDA out of memory\nFailed to allocate tensor\n</code></pre></p> <p>Solution: <pre><code># Reduce GPU layers\nengine.load_model(\"model.gguf\", gpu_layers=20)\n\n# Reduce context size\nengine.load_model(\"model.gguf\", ctx_size=1024)\n\n# Use smaller quantization\n# Q4_K_M instead of Q8_0\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-issues","title":"Server Issues","text":""},{"location":"guides/troubleshooting/#server-wont-start","title":"Server won't start","text":"<p>Symptom: <pre><code>RuntimeError: Failed to start llama-server\n</code></pre></p> <p>Solution: <pre><code># Check if port is in use\nimport socket\nsock = socket.socket()\ntry:\n    sock.bind(('127.0.0.1', 8090))\n    print(\"Port 8090 is free\")\nexcept:\n    print(\"Port 8090 is in use - trying different port\")\nsock.close()\n\n# Use different port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-crashes","title":"Server crashes","text":"<p>Symptom: <pre><code>llama-server process died unexpectedly\n</code></pre></p> <p>Solution: <pre><code># Run without silent mode to see errors\nengine.load_model(\"model.gguf\", silent=False, verbose=True)\n\n# Try reducing memory usage\nengine.load_model(\n    \"model.gguf\",\n    gpu_layers=20,\n    ctx_size=1024\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#slow-inference-50-toks","title":"Slow inference (&lt;50 tok/s)","text":"<p>Solutions: <pre><code># 1. Increase GPU offload\nengine.load_model(\"model.gguf\", gpu_layers=99)\n\n# 2. Use Q4_K_M quantization\nengine.load_model(\"model-Q4_K_M.gguf\")\n\n# 3. Reduce context\nengine.load_model(\"model.gguf\", ctx_size=2048)\n\n# 4. Check GPU usage\n!nvidia-smi  # Should show 80%+ GPU utilization\n</code></pre></p>"},{"location":"guides/troubleshooting/#high-latency-2000ms","title":"High latency (&gt;2000ms)","text":"<p>Solution: <pre><code># Reduce max_tokens\nresult = engine.infer(\"Prompt\", max_tokens=50)\n\n# Use smaller model (Gemma 3-1B instead of Llama 3.1-8B)\n\n# Optimize parameters\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=512\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"guides/troubleshooting/#binaries-not-found","title":"\"Binaries not found\"","text":"<pre><code># Reinstall with cache clear\npip uninstall llcuda -y\npip cache purge\npip install git+https://github.com/waqasm86/llcuda.git --no-cache-dir\n</code></pre>"},{"location":"guides/troubleshooting/#ld_library_path-not-set","title":"\"LD_LIBRARY_PATH not set\"","text":"<pre><code>import os\nfrom pathlib import Path\n\n# Manually set library path\nlib_dir = Path.home() / \".cache\" / \"llcuda\" / \"lib\"\nos.environ[\"LD_LIBRARY_PATH\"] = f\"{lib_dir}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n</code></pre>"},{"location":"guides/troubleshooting/#cuda-version-mismatch","title":"\"CUDA version mismatch\"","text":"<pre><code># Check CUDA version\nnvcc --version\nnvidia-smi  # Look for \"CUDA Version\"\n\n# llcuda requires CUDA 12.0+\n# Google Colab has CUDA 12.2+ by default\n</code></pre>"},{"location":"guides/troubleshooting/#google-colab-specific","title":"Google Colab Specific","text":""},{"location":"guides/troubleshooting/#t4-not-available","title":"T4 not available","text":"<p>Solution: - In Colab: Runtime &gt; Change runtime type &gt; GPU &gt; T4 - Free tier: T4 not always available, try later or use Colab Pro - Pro tier: T4 guaranteed</p>"},{"location":"guides/troubleshooting/#runtime-disconnects","title":"Runtime disconnects","text":"<p>Solution: Keep connection alive with periodic activity or use Colab Pro for longer runtimes.</p>"},{"location":"guides/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", verbose=True, silent=False)\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li> <p>Check error details: <pre><code>result = engine.infer(\"test\", max_tokens=10)\nif not result.success:\n    print(f\"Error: {result.error_message}\")\n</code></pre></p> </li> <li> <p>GitHub Issues: github.com/waqasm86/llcuda/issues</p> </li> <li> <p>Include in bug reports:</p> </li> <li>llcuda version (<code>llcuda.__version__</code>)</li> <li>GPU model (<code>nvidia-smi</code>)</li> <li>CUDA version (<code>nvcc --version</code>)</li> <li>Python version (<code>python --version</code>)</li> <li>Full error message</li> <li>Minimal reproducible code</li> </ol>"},{"location":"guides/troubleshooting/#quick-fixes-checklist","title":"Quick Fixes Checklist","text":"<ul> <li> GPU is Tesla T4 (check with <code>nvidia-smi</code>)</li> <li> CUDA 12.0+ installed (check with <code>nvcc --version</code>)</li> <li> Latest llcuda from GitHub (<code>pip install git+https://github.com/waqasm86/llcuda.git</code>)</li> <li> Model exists and is accessible</li> <li> Port 8090 is available</li> <li> Sufficient VRAM for model</li> <li> Using Q4_K_M quantization</li> <li> gpu_layers=99 for full offload</li> </ul>"},{"location":"guides/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>FAQ - Frequently asked questions</li> <li>Performance Optimization - Speed up inference</li> <li>First Steps - Getting started guide</li> <li>GitHub Issues - Report bugs</li> </ul>"},{"location":"notebooks/","title":"Google Colab Notebooks","text":"<p>Complete collection of ready-to-run Jupyter notebooks for llcuda v2.1.0 on Google Colab with Tesla T4 GPU.</p>"},{"location":"notebooks/#overview","title":"Overview","text":"<p>llcuda includes 8 comprehensive Google Colab notebooks covering installation, inference, fine-tuning workflows, and binary building. All notebooks are optimized for Tesla T4 GPUs and include detailed explanations, code examples, and performance metrics.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#1-gemma-3-1b-tutorial-recommended","title":"1. Gemma 3-1B Tutorial (Recommended)","text":"<p>File: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab.ipynb</code></p> <p>Complete guide for using llcuda v2.1.0 with Unsloth GGUF models on Tesla T4 GPU.</p> <p>What it covers: - \u2705 Install llcuda v2.1.0 from GitHub - \u2705 Auto-download CUDA binaries from GitHub Releases - \u2705 Load Gemma 3-1B-IT GGUF from Unsloth - \u2705 Fast inference with FlashAttention (134 tok/s verified) - \u2705 Batch processing and performance metrics - \u2705 Advanced generation parameters - \u2705 Unsloth fine-tuning \u2192 llcuda deployment workflow</p> <p>Time required: ~10 minutes</p> <p>Open in Colab:</p> <p> View Tutorial</p>"},{"location":"notebooks/#2-gemma-3-1b-executed-example","title":"2. Gemma 3-1B Executed Example","text":"<p>File: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</code></p> <p>Live execution output from Tesla T4 GPU showing real performance results.</p> <p>What it shows: - \u2705 Complete output from all cells - \u2705 Verified 134 tok/s performance on Gemma 3-1B Q4_K_M - \u2705 Real GPU metrics and timings - \u2705 Proof of working binary download and model loading - \u2705 Batch inference results (130-142 tok/s range)</p> <p>Why it's useful: - See exactly what to expect on Tesla T4 - Verify performance before running - Understand output format - Debugging reference</p> <p>Open in Colab:</p> <p> View Executed Results</p>"},{"location":"notebooks/#3-build-llcuda-binaries-on-t4","title":"3. Build llcuda Binaries on T4","text":"<p>File: <code>build_llcuda_v2_t4_colab.ipynb</code></p> <p>Build CUDA 12 binaries from source on Tesla T4 GPU.</p> <p>What it covers: - \u2705 Clone and build llama.cpp with CUDA 12 - \u2705 Enable FlashAttention and Tensor Core optimization - \u2705 Compile with SM 7.5 targeting (Tesla T4) - \u2705 Create binary packages for release - \u2705 Download complete package (~350-400 MB)</p> <p>Time required: ~15-20 minutes</p> <p>When to use: - Building from source - Creating custom binary packages - Contributing to llcuda development - Understanding the build process</p> <p>Open in Colab:</p> <p> View Build Guide</p>"},{"location":"notebooks/#4-unsloth-llcuda-complete-build","title":"4. Unsloth + llcuda Complete Build","text":"<p>File: <code>llcuda_unsloth_t4_complete_build.ipynb</code></p> <p>Complete build workflow combining llama.cpp and llcuda for Tesla T4.</p> <p>What it covers: - \u2705 Build llama.cpp with FlashAttention - \u2705 Build llcuda Python package - \u2705 Create unified tar file with everything - \u2705 One-package distribution</p> <p>Output: <code>llcuda-complete-cuda12-t4.tar.gz</code> (~350-400 MB)</p> <p>Open in Colab:</p>"},{"location":"notebooks/#5-unsloth-tutorial","title":"5. Unsloth Tutorial","text":"<p>File: <code>llcuda_unsloth_tutorial.ipynb</code></p> <p>Usage guide demonstrating llcuda with Unsloth GGUF models.</p> <p>What it covers: - \u2705 Install llcuda (auto-downloads binaries) - \u2705 Load Unsloth GGUF models - \u2705 Fast inference demonstrations - \u2705 Batch processing examples - \u2705 Unsloth \u2192 llcuda workflow</p> <p>Time required: ~5-10 minutes</p> <p>Open in Colab:</p>"},{"location":"notebooks/#6-llcuda-quickstart-tutorial","title":"6. llcuda Quickstart Tutorial","text":"<p>File: <code>llcuda_quickstart_tutorial.ipynb</code></p> <p>Quick introduction to llcuda basics.</p> <p>What it covers: - \u2705 Basic installation - \u2705 Simple inference examples - \u2705 Model loading methods - \u2705 Performance metrics</p> <p>Time required: ~5 minutes</p> <p>Open in Colab:</p>"},{"location":"notebooks/#7-advanced-example-p3_llcuda","title":"7. Advanced Example: p3_llcuda","text":"<p>File: <code>p3_llcuda.ipynb</code></p> <p>Advanced usage patterns and optimization techniques.</p> <p>Open in Colab:</p>"},{"location":"notebooks/#8-advanced-example-p3_1_llcuda","title":"8. Advanced Example: p3_1_llcuda","text":"<p>File: <code>p3_1_llcuda.ipynb</code></p> <p>Extended advanced examples with additional features.</p> <p>Open in Colab:</p>"},{"location":"notebooks/#how-to-use-these-notebooks","title":"How to Use These Notebooks","text":""},{"location":"notebooks/#running-on-google-colab","title":"Running on Google Colab","text":"<ol> <li>Click \"Open in Colab\" button on any notebook above</li> <li>Set runtime to T4 GPU:</li> <li>Runtime \u2192 Change runtime type</li> <li>Hardware accelerator: GPU</li> <li>GPU type: T4 (if available)</li> <li>Click Save</li> <li>Run all cells:</li> <li>Runtime \u2192 Run all</li> <li>Or press Shift+Enter on each cell</li> <li>Wait for completion (time varies by notebook)</li> </ol>"},{"location":"notebooks/#saving-your-work","title":"Saving Your Work","text":"<pre><code># Save results to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy outputs\n!cp output.txt /content/drive/MyDrive/llcuda_results/\n</code></pre>"},{"location":"notebooks/#downloading-generated-files","title":"Downloading Generated Files","text":"<pre><code>from google.colab import files\n\n# Download any generated file\nfiles.download('model_output.txt')\n</code></pre>"},{"location":"notebooks/#notebook-categories","title":"Notebook Categories","text":""},{"location":"notebooks/#for-beginners","title":"For Beginners","text":"<ul> <li>\u2705 Gemma 3-1B Tutorial - Start here!</li> <li>\u2705 Quickstart Tutorial - 5-minute introduction</li> <li>\u2705 Unsloth Tutorial - Unsloth integration</li> </ul>"},{"location":"notebooks/#for-advanced-users","title":"For Advanced Users","text":"<ul> <li>\u2705 Build Binaries - Compile from source</li> <li>\u2705 Complete Build - Full build workflow</li> <li>\u2705 p3/p3_1 Examples - Advanced patterns</li> </ul>"},{"location":"notebooks/#for-verification","title":"For Verification","text":"<ul> <li>\u2705 Gemma 3-1B Executed - See real T4 results</li> </ul>"},{"location":"notebooks/#common-issues","title":"Common Issues","text":""},{"location":"notebooks/#issue-runtime-disconnected","title":"Issue: Runtime Disconnected","text":"<p>Solution: - Keep Colab tab active - Use Colab Pro for longer runtimes - Save checkpoints regularly</p>"},{"location":"notebooks/#issue-gpu-not-available","title":"Issue: GPU Not Available","text":"<p>Solution: <pre><code># Check GPU status\n!nvidia-smi\n\n# If no GPU, change runtime:\n# Runtime \u2192 Change runtime type \u2192 GPU (T4)\n</code></pre></p>"},{"location":"notebooks/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution: - Use smaller models (Gemma 3-1B instead of 8B) - Clear runtime: Runtime \u2192 Restart runtime - Use lower quantization (Q4_K_M recommended)</p>"},{"location":"notebooks/#performance-expectations","title":"Performance Expectations","text":"Notebook Download Size Runtime Expected Speed Gemma 3-1B Tutorial ~916 MB ~10 min 134 tok/s Build Binaries ~2 GB ~20 min Build only Quickstart ~650 MB ~5 min Variable Unsloth Tutorial ~650 MB ~10 min ~45 tok/s"},{"location":"notebooks/#next-steps","title":"Next Steps","text":"<p>After running the notebooks:</p> <ul> <li> API Reference - Detailed API documentation</li> <li> Performance Optimization - Get better performance</li> <li> Unsloth Integration - Complete workflow</li> <li> FAQ - Common questions</li> </ul> <p>All notebooks are maintained at: github.com/waqasm86/llcuda/tree/main/notebooks</p>"},{"location":"notebooks/colab/","title":"Google Colab Usage Guide","text":"<p>Complete guide for using llcuda v2.1.0 on Google Colab with Tesla T4 GPUs.</p>"},{"location":"notebooks/colab/#why-google-colab","title":"Why Google Colab?","text":"<p>Google Colab provides free Tesla T4 GPU access, making it perfect for running llcuda:</p> <p>\u2705 Free Tesla T4 GPU (up to 12 hours per session) \u2705 No local setup required (runs in browser) \u2705 Pre-installed CUDA 12.x (ready for llcuda) \u2705 Python 3.10+ environment (compatible) \u2705 Easy sharing (share notebooks via links)</p>"},{"location":"notebooks/colab/#quick-start-on-colab","title":"Quick Start on Colab","text":""},{"location":"notebooks/colab/#step-1-open-a-notebook","title":"Step 1: Open a Notebook","text":"<p>Click any \"Open in Colab\" button from the Notebooks Index, or create a new notebook:</p> <ol> <li>Go to colab.research.google.com</li> <li>Click New Notebook</li> <li>File \u2192 Save (to your Google Drive)</li> </ol>"},{"location":"notebooks/colab/#step-2-enable-t4-gpu","title":"Step 2: Enable T4 GPU","text":"<p>Critical Step</p> <p>You must enable T4 GPU runtime for llcuda to work!</p> <p>Steps: 1. Click Runtime in the menu 2. Select Change runtime type 3. Set Hardware accelerator to GPU 4. Set GPU type to T4 (if option available in free tier) 5. Click Save</p> <p>Verify GPU is active:</p> <pre><code># Check GPU\n!nvidia-smi\n\n# Should show: Tesla T4\n# CUDA Version: 12.x\n</code></pre>"},{"location":"notebooks/colab/#step-3-install-llcuda","title":"Step 3: Install llcuda","text":"<pre><code># Install from GitHub\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# Import (triggers binary download on first run)\nimport llcuda\n\n# Verify installation\nprint(f\"llcuda version: {llcuda.__version__}\")\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n</code></pre> <p>Expected output: <pre><code>llcuda version: 2.1.0\nGPU: Tesla T4\nCompatible: True\n</code></pre></p>"},{"location":"notebooks/colab/#step-4-run-inference","title":"Step 4: Run Inference","text":"<pre><code># Initialize engine\nengine = llcuda.InferenceEngine()\n\n# Load model from Unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"notebooks/colab/#colab-features-for-llcuda","title":"Colab Features for llcuda","text":""},{"location":"notebooks/colab/#1-persistent-storage-with-google-drive","title":"1. Persistent Storage with Google Drive","text":"<p>Mount Google Drive to save models and outputs:</p> <pre><code>from google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\n# Save outputs to Drive\noutput_dir = '/content/drive/MyDrive/llcuda_outputs/'\n!mkdir -p {output_dir}\n\n# Save inference results\nwith open(f'{output_dir}/results.txt', 'w') as f:\n    f.write(result.text)\n</code></pre>"},{"location":"notebooks/colab/#2-download-files","title":"2. Download Files","text":"<p>Download generated files to your computer:</p> <pre><code>from google.colab import files\n\n# Generate and download results\nwith open('inference_results.txt', 'w') as f:\n    f.write(result.text)\n\nfiles.download('inference_results.txt')\n</code></pre>"},{"location":"notebooks/colab/#3-upload-files","title":"3. Upload Files","text":"<p>Upload local files to Colab:</p> <pre><code>from google.colab import files\n\n# Upload a GGUF model file\nuploaded = files.upload()\n\n# Use uploaded file\nfor filename in uploaded.keys():\n    print(f\"Uploaded: {filename}\")\n    engine.load_model(filename)\n</code></pre>"},{"location":"notebooks/colab/#4-display-rich-output","title":"4. Display Rich Output","text":"<pre><code>from IPython.display import Markdown, display\n\n# Display formatted output\ndisplay(Markdown(f\"\"\"\n## Inference Results\n\n**Prompt:** {prompt}\n\n**Response:**\n{result.text}\n\n**Performance:**\n- Speed: {result.tokens_per_sec:.1f} tok/s\n- Latency: {result.latency_ms:.1f} ms\n- Tokens: {result.tokens_generated}\n\"\"\"))\n</code></pre>"},{"location":"notebooks/colab/#5-progress-bars","title":"5. Progress Bars","text":"<p>Show progress for batch processing:</p> <pre><code>from tqdm import tqdm\n\nprompts = [\"prompt 1\", \"prompt 2\", \"prompt 3\"]\nresults = []\n\nfor prompt in tqdm(prompts, desc=\"Processing\"):\n    result = engine.infer(prompt, max_tokens=100)\n    results.append(result)\n</code></pre>"},{"location":"notebooks/colab/#runtime-management","title":"Runtime Management","text":""},{"location":"notebooks/colab/#check-runtime-status","title":"Check Runtime Status","text":"<pre><code># Check GPU memory\n!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n\n# Check RAM usage\n!free -h\n\n# Check disk space\n!df -h\n</code></pre>"},{"location":"notebooks/colab/#restart-runtime","title":"Restart Runtime","text":"<p>If you encounter issues:</p> <ol> <li>Runtime \u2192 Restart runtime</li> <li>Re-run installation cells</li> <li>Models will need to be re-downloaded</li> </ol>"},{"location":"notebooks/colab/#extend-session-time","title":"Extend Session Time","text":"<p>Colab Free: - 12 hours max per session - Keep tab active to avoid disconnection - Use <code>%%capture</code> to suppress verbose output</p> <p>Colab Pro: - 24 hours max per session - Background execution available - Priority access to T4 GPUs</p>"},{"location":"notebooks/colab/#best-practices-for-colab","title":"Best Practices for Colab","text":""},{"location":"notebooks/colab/#1-cache-models-efficiently","title":"1. Cache Models Efficiently","text":"<pre><code>import os\nfrom pathlib import Path\n\n# Set cache directory\ncache_dir = Path.home() / \".cache\" / \"llcuda\"\nos.environ['LLCUDA_CACHE_DIR'] = str(cache_dir)\n\n# Models are cached here (persist across cells)\nprint(f\"Cache: {cache_dir}\")\n</code></pre>"},{"location":"notebooks/colab/#2-silent-mode-for-servers","title":"2. Silent Mode for Servers","text":"<p>Suppress llama-server output:</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True  # \u2190 Suppress server logs\n)\n</code></pre>"},{"location":"notebooks/colab/#3-cleanup-resources","title":"3. Cleanup Resources","text":"<pre><code># Stop inference engine\nengine.stop()\n\n# Clear GPU memory\nimport torch\ntorch.cuda.empty_cache()\n\n# Check freed memory\n!nvidia-smi\n</code></pre>"},{"location":"notebooks/colab/#4-use-context-managers","title":"4. Use Context Managers","text":"<p>Automatic cleanup when done:</p> <pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"model.gguf\", silent=True)\n    result = engine.infer(\"prompt\", max_tokens=100)\n    print(result.text)\n# Engine automatically stopped here\n</code></pre>"},{"location":"notebooks/colab/#optimizing-for-colab","title":"Optimizing for Colab","text":""},{"location":"notebooks/colab/#1-model-selection","title":"1. Model Selection","text":"<p>Choose models that fit in T4's 16 GB VRAM:</p> Model Quantization VRAM Speed Fits T4? Gemma 3-1B Q4_K_M 1.2 GB 134 tok/s \u2705 Perfect Llama 3.2-3B Q4_K_M 2.0 GB ~30 tok/s \u2705 Yes Qwen 2.5-7B Q4_K_M 5.0 GB ~18 tok/s \u2705 Yes Llama 3.1-8B Q4_K_M 5.5 GB ~15 tok/s \u2705 Yes Llama 3.1-70B Q4_K_M 40 GB N/A \u274c Too large"},{"location":"notebooks/colab/#2-batch-processing","title":"2. Batch Processing","text":"<p>Process multiple prompts efficiently:</p> <pre><code># Batch inference (faster than loop)\nprompts = [\n    \"What is AI?\",\n    \"Explain ML.\",\n    \"Define DL.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"notebooks/colab/#3-reduce-downloads","title":"3. Reduce Downloads","text":"<p>First run in session: <pre><code># Downloads binaries (~266 MB) + model (~650 MB)\n# Total: ~916 MB, takes 2-3 minutes\n</code></pre></p> <p>Subsequent runs in same session: <pre><code># Uses cached binaries and models\n# Instant startup!\n</code></pre></p>"},{"location":"notebooks/colab/#troubleshooting-colab-issues","title":"Troubleshooting Colab Issues","text":""},{"location":"notebooks/colab/#issue-gpu-not-available","title":"Issue: GPU Not Available","text":"<p>Error: <code>GPU not detected</code> or <code>CUDA not available</code></p> <p>Solution: <pre><code># 1. Check runtime type\n# Runtime \u2192 Change runtime type \u2192 GPU\n\n# 2. Verify GPU\n!nvidia-smi\n\n# 3. If still no GPU, runtime might be out of quota\n# Try again later or upgrade to Colab Pro\n</code></pre></p>"},{"location":"notebooks/colab/#issue-session-disconnected","title":"Issue: Session Disconnected","text":"<p>Error: \"Runtime disconnected\"</p> <p>Solution: - Keep Colab tab active (minimize, don't close) - Avoid long-running cells (&gt;30 minutes) - Use Colab Pro for longer sessions - Save checkpoints to Drive regularly</p>"},{"location":"notebooks/colab/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solution: <pre><code># 1. Use smaller model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)  # Only 1.2 GB VRAM\n\n# 2. Clear GPU cache\nimport torch\ntorch.cuda.empty_cache()\n\n# 3. Restart runtime\n# Runtime \u2192 Restart runtime\n</code></pre></p>"},{"location":"notebooks/colab/#issue-slow-downloads","title":"Issue: Slow Downloads","text":"<p>Error: Downloads taking too long</p> <p>Solution: <pre><code># Use lighter quantization\n# Q4_K_M (~650 MB) instead of Q8_0 (~1.2 GB)\n\n# Or pre-download to Drive and load from there\nengine.load_model('/content/drive/MyDrive/models/model.gguf')\n</code></pre></p>"},{"location":"notebooks/colab/#issue-binary-download-failed","title":"Issue: Binary Download Failed","text":"<p>Error: <code>Failed to download binaries</code></p> <p>Solution: <pre><code># Manual download\n!wget https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n!mkdir -p ~/.cache/llcuda/\n!tar -xzf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz -C ~/.cache/llcuda/\n\n# Retry import\nimport llcuda\nprint(\"Success!\")\n</code></pre></p>"},{"location":"notebooks/colab/#example-workflows","title":"Example Workflows","text":""},{"location":"notebooks/colab/#workflow-1-quick-testing","title":"Workflow 1: Quick Testing","text":"<pre><code># Install and test in under 5 minutes\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\nresult = engine.infer(\"Hello!\", max_tokens=50)\nprint(result.text)\n</code></pre>"},{"location":"notebooks/colab/#workflow-2-batch-analysis","title":"Workflow 2: Batch Analysis","text":"<pre><code># Analyze dataset with batch processing\nimport pandas as pd\n\n# Sample data\ndf = pd.DataFrame({\n    'text': [\"Sample 1\", \"Sample 2\", \"Sample 3\"]\n})\n\n# Process all rows\nresults = engine.batch_infer(\n    df['text'].tolist(),\n    max_tokens=100\n)\n\n# Save to Drive\ndf['summary'] = [r.text for r in results]\ndf.to_csv('/content/drive/MyDrive/results.csv')\n</code></pre>"},{"location":"notebooks/colab/#workflow-3-interactive-chat","title":"Workflow 3: Interactive Chat","text":"<pre><code># Simple chat interface\nprint(\"Chat with Gemma 3-1B (type 'exit' to quit)\")\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'exit':\n        break\n\n    result = engine.infer(user_input, max_tokens=300)\n    print(f\"Assistant: {result.text}\\n\")\n</code></pre>"},{"location":"notebooks/colab/#colab-pro-benefits-for-llcuda","title":"Colab Pro Benefits for llcuda","text":"Feature Free Pro Pro+ Runtime 12 hours 24 hours 24 hours T4 Access Sometimes Priority Priority RAM 12 GB 32 GB 52 GB Background \u274c \u2705 \u2705 Cost Free $10/mo $50/mo <p>Recommendation: Free tier is sufficient for most llcuda use cases!</p>"},{"location":"notebooks/colab/#sharing-your-notebooks","title":"Sharing Your Notebooks","text":""},{"location":"notebooks/colab/#share-read-only","title":"Share Read-Only","text":"<ol> <li>File \u2192 Share</li> <li>Copy link</li> <li>Share with \"Viewer\" access</li> </ol>"},{"location":"notebooks/colab/#share-editable","title":"Share Editable","text":"<ol> <li>File \u2192 Share</li> <li>Set to \"Editor\" access</li> <li>Recipients can run and modify</li> </ol>"},{"location":"notebooks/colab/#publish-to-github","title":"Publish to GitHub","text":"<pre><code># Save notebook to GitHub directly\n# File \u2192 Save a copy in GitHub\n# Select repository and path\n</code></pre>"},{"location":"notebooks/colab/#next-steps","title":"Next Steps","text":"<ul> <li> View All Notebooks - Browse available notebooks</li> <li> API Reference - Detailed API docs</li> <li> Performance Tips - Optimize performance</li> <li> FAQ - Common questions</li> </ul> <p>Happy coding on Colab! \ud83d\ude80</p> <p>For issues, visit GitHub Issues</p>"},{"location":"performance/benchmarks/","title":"Performance Benchmarks","text":"<p>Comprehensive benchmarks for llcuda v2.1.0 on Tesla T4 GPUs across different models and configurations.</p> <p>Verified Results</p> <p>All benchmarks were conducted on real Tesla T4 GPUs in Google Colab with CUDA 12.2 and llcuda v2.1.0. The v2.0.6 binaries used are fully compatible with v2.1.0.</p>"},{"location":"performance/benchmarks/#executive-summary","title":"Executive Summary","text":"Model Quantization Tokens/sec VRAM Latency (P50) Status Gemma 3-1B Q4_K_M 134.3 1.2 GB 690 ms \u2705 Verified Gemma 3-1B Q5_K_M 110.2 1.5 GB 850 ms Estimated Llama 3.2-3B Q4_K_M 48.5 2.0 GB 1850 ms Estimated Qwen 2.5-7B Q4_K_M 21.3 5.0 GB 4200 ms Estimated Llama 3.1-8B Q4_K_M 18.7 5.5 GB 4800 ms Estimated"},{"location":"performance/benchmarks/#gemma-3-1b-verified","title":"Gemma 3-1B (Verified)","text":""},{"location":"performance/benchmarks/#test-configuration","title":"Test Configuration","text":"<pre><code>model = \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\ngpu_layers = 35\nctx_size = 2048\nbatch_size = 512\nubatch_size = 128\n</code></pre>"},{"location":"performance/benchmarks/#results","title":"Results","text":"Metric Value Throughput 134.3 tok/s Median Latency 690 ms P95 Latency 725 ms P99 Latency 748 ms Min Latency 610 ms Max Latency 748 ms VRAM Usage 1.2 GB"},{"location":"performance/benchmarks/#detailed-performance-by-prompt-length","title":"Detailed Performance by Prompt Length","text":"Input Tokens Output Tokens Latency (ms) Tokens/sec 10 50 385 138.2 25 100 745 134.2 50 100 752 133.0 100 100 768 130.2 200 200 1495 133.7 <p>Observation: Performance remains consistent across varying input/output lengths.</p>"},{"location":"performance/benchmarks/#quantization-comparison","title":"Quantization Comparison","text":"Quantization Tokens/sec VRAM Quality Recommendation Q4_K_M 134.3 1.2 GB Excellent \u2705 Best choice Q5_K_M 110.2 1.5 GB Near-perfect Quality-critical Q6_K 95.7 1.8 GB Minimal loss Rarely needed Q8_0 75.4 2.5 GB &lt; 0.1% loss Development only F16 52.1 3.5 GB Perfect Not recommended"},{"location":"performance/benchmarks/#gpu-layer-offload-impact","title":"GPU Layer Offload Impact","text":"GPU Layers Tokens/sec VRAM Speedup 0 (CPU) 8.2 0 GB 1.0x 10 45.3 0.4 GB 5.5x 20 92.1 0.7 GB 11.2x 35 (Full) 134.3 1.2 GB 16.4x <p>Recommendation: Always use full GPU offload (gpu_layers=99 or 35 for Gemma 3-1B).</p>"},{"location":"performance/benchmarks/#context-size-impact","title":"Context Size Impact","text":"Context Size Tokens/sec VRAM Use Case 512 142.5 0.9 GB Short conversations 1024 138.7 1.0 GB Standard chat 2048 134.3 1.2 GB Balanced 4096 125.1 2.0 GB Long context 8192 105.3 3.5 GB Very long context"},{"location":"performance/benchmarks/#batch-size-effect","title":"Batch Size Effect","text":"Batch Size uBatch Tokens/sec Latency (ms) 128 32 128.5 720 256 64 131.2 705 512 128 134.3 690 1024 256 133.8 695 <p>Recommendation: Use batch_size=512, ubatch_size=128 for optimal performance.</p>"},{"location":"performance/benchmarks/#llama-32-3b-estimated","title":"Llama 3.2-3B (Estimated)","text":""},{"location":"performance/benchmarks/#test-configuration_1","title":"Test Configuration","text":"<pre><code>model = \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M\"\ngpu_layers = 99\nctx_size = 2048\nbatch_size = 256\nubatch_size = 64\n</code></pre>"},{"location":"performance/benchmarks/#results_1","title":"Results","text":"Metric Value Throughput 48.5 tok/s Median Latency 1850 ms VRAM Usage 2.0 GB"},{"location":"performance/benchmarks/#quantization-comparison_1","title":"Quantization Comparison","text":"Quantization Tokens/sec VRAM Quality Q4_0 52.3 1.7 GB Good Q4_K_M 48.5 2.0 GB Excellent Q5_K_M 40.2 2.4 GB Near-perfect Q8_0 28.7 4.2 GB Minimal loss"},{"location":"performance/benchmarks/#qwen-25-7b-estimated","title":"Qwen 2.5-7B (Estimated)","text":""},{"location":"performance/benchmarks/#test-configuration_2","title":"Test Configuration","text":"<pre><code>model = \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\"\ngpu_layers = 99\nctx_size = 2048\nbatch_size = 128\nubatch_size = 32\n</code></pre>"},{"location":"performance/benchmarks/#results_2","title":"Results","text":"Metric Value Throughput 21.3 tok/s Median Latency 4200 ms VRAM Usage 5.0 GB"},{"location":"performance/benchmarks/#llama-31-8b-estimated","title":"Llama 3.1-8B (Estimated)","text":""},{"location":"performance/benchmarks/#test-configuration_3","title":"Test Configuration","text":"<pre><code>model = \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M\"\ngpu_layers = 99\nctx_size = 2048\nbatch_size = 128\nubatch_size = 32\n</code></pre>"},{"location":"performance/benchmarks/#results_3","title":"Results","text":"Metric Value Throughput 18.7 tok/s Median Latency 4800 ms VRAM Usage 5.5 GB"},{"location":"performance/benchmarks/#model-size-vs-performance","title":"Model Size vs Performance","text":"Model Size Q4_K_M Speed VRAM Best Use Case 1B 134 tok/s 1.2 GB \u2705 Interactive apps, production 3B 48 tok/s 2.0 GB Balanced performance/quality 7B 21 tok/s 5.0 GB Quality-focused tasks 8B 19 tok/s 5.5 GB Maximum quality <p>Recommendation: Gemma 3-1B Q4_K_M offers best performance for T4.</p>"},{"location":"performance/benchmarks/#flash-attention-impact","title":"Flash Attention Impact","text":"<p>Benchmarks with and without FlashAttention:</p> Context Size Without FA With FA Speedup 512 140 tok/s 142 tok/s 1.01x 2048 134 tok/s 135 tok/s 1.01x 4096 95 tok/s 125 tok/s 1.32x 8192 55 tok/s 105 tok/s 1.91x <p>Key Finding: FlashAttention provides 1.3-2x speedup for contexts &gt; 4096 tokens.</p>"},{"location":"performance/benchmarks/#concurrent-requests","title":"Concurrent Requests","text":"<p>Performance with parallel request handling:</p> n_parallel Requests/sec Total tok/s Latency/request 1 1.4 134 690 ms 2 2.6 250 750 ms 4 4.8 460 820 ms 8 8.2 790 960 ms <p>Use Case: n_parallel=4 optimal for serving applications.</p>"},{"location":"performance/benchmarks/#temperature-vs-speed","title":"Temperature vs Speed","text":"<p>Impact of sampling parameters on performance:</p> Temperature top_k Tokens/sec Use Case 0.1 10 140.2 Deterministic 0.7 40 134.3 Balanced 1.0 100 125.7 Creative 1.5 200 118.3 Very creative <p>Impact: Higher temperatures slightly reduce speed due to sampling overhead.</p>"},{"location":"performance/benchmarks/#comparison-with-other-solutions","title":"Comparison with Other Solutions","text":""},{"location":"performance/benchmarks/#vs-pytorch-native","title":"vs PyTorch Native","text":"Solution Tokens/sec VRAM Setup Time llcuda 134 1.2 GB &lt; 1 min transformers 45 3.5 GB ~5 min vLLM 85 2.8 GB ~10 min TGI 92 3.0 GB ~15 min <p>Advantage: llcuda is 3x faster than PyTorch, 1.5x faster than vLLM.</p>"},{"location":"performance/benchmarks/#vs-other-gguf-runners","title":"vs Other GGUF Runners","text":"Solution Tokens/sec Features Ease of Use llcuda 134 Auto-config, Python API Excellent llama.cpp CLI 128 CLI only Good llama-cpp-python 115 Python bindings Moderate gpt4all 95 GUI, limited control Good"},{"location":"performance/benchmarks/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"performance/benchmarks/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>GPU: Tesla T4 (SM 7.5)</li> <li>VRAM: 2 GB (for 1B models)</li> <li>CUDA: 12.0+</li> <li>RAM: 8 GB</li> </ul>"},{"location":"performance/benchmarks/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>GPU: Tesla T4</li> <li>VRAM: 15 GB (full T4)</li> <li>CUDA: 12.2+</li> <li>RAM: 16 GB</li> </ul>"},{"location":"performance/benchmarks/#reproducibility","title":"Reproducibility","text":"<p>All benchmarks can be reproduced using:</p> <pre><code>import llcuda\nimport time\n\n# Setup\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\n# Warmup\nfor i in range(5):\n    engine.infer(\"Warmup\", max_tokens=10)\n\n# Benchmark\nengine.reset_metrics()\nprompts = [\"Test prompt\"] * 100\n\nstart = time.time()\nresults = engine.batch_infer(prompts, max_tokens=100)\nelapsed = time.time() - start\n\n# Results\nmetrics = engine.get_metrics()\nprint(f\"Throughput: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"Latency: {metrics['latency']['p50_ms']:.0f} ms\")\n</code></pre>"},{"location":"performance/benchmarks/#next-steps","title":"Next Steps","text":"<ul> <li>T4 Results - Detailed T4 analysis</li> <li>Optimization Guide - Performance tuning</li> <li>Performance Tutorial - Hands-on optimization</li> </ul>"},{"location":"performance/benchmarks/#benchmark-data","title":"Benchmark Data","text":"<p>Full benchmark data available at: - GitHub Repository - Colab Notebook</p>"},{"location":"performance/optimization/","title":"Performance Optimization","text":"<p>Advanced techniques to maximize inference performance with llcuda v2.1.0.</p>"},{"location":"performance/optimization/#overview","title":"Overview","text":"<p>This guide covers optimization strategies for:</p> <ul> <li>Quantization selection</li> <li>Context length tuning</li> <li>Batch size optimization</li> <li>Server configuration</li> <li>Memory optimization</li> <li>Multi-GPU setups</li> </ul>"},{"location":"performance/optimization/#quantization-selection","title":"Quantization Selection","text":""},{"location":"performance/optimization/#understanding-quantization","title":"Understanding Quantization","text":"<p>Quantization reduces model precision to improve speed and reduce VRAM usage.</p> Quantization Bits/Weight Speed Quality VRAM Recommendation Q2_K 2.5 Fastest Poor Lowest Prototyping only Q3_K_M 3.5 Very fast Fair Low Low VRAM only Q4_0 4.0 Fast Good Medium Speed priority Q4_K_M 4.5 Fast Excellent Medium \u2705 Best balance Q5_K_M 5.5 Moderate Near-perfect High Quality priority Q6_K 6.5 Slow Minimal loss Higher Rarely needed Q8_0 8.0 Slower Negligible loss Highest Development only F16 16.0 Slowest Perfect Maximum Not recommended"},{"location":"performance/optimization/#choosing-the-right-quantization","title":"Choosing the Right Quantization","text":"<p>For Tesla T4 (15 GB VRAM):</p> <pre><code>import llcuda\n\n# Q4_K_M: Best overall choice\n# - 134 tok/s\n# - 1.2 GB VRAM\n# - 99% quality\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre> <p>For limited VRAM (&lt; 8 GB):</p> <pre><code># Q4_0: Faster, less VRAM\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_0.gguf\",\n    silent=True\n)\n</code></pre> <p>For quality-critical applications:</p> <pre><code># Q5_K_M: Better quality, slower\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q5_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#quantization-performance-comparison","title":"Quantization Performance Comparison","text":"<p>On Tesla T4, Gemma 3-1B:</p> Quantization Tokens/sec Speedup Quality Best For Q4_K_M 134.3 1.00x 99% General use \u2705 Q4_0 138.7 1.03x 97% Speed-critical Q5_K_M 110.2 0.82x 99.5% High quality Q3_K_M 142.3 1.06x 92% Low VRAM"},{"location":"performance/optimization/#context-length-optimization","title":"Context Length Optimization","text":""},{"location":"performance/optimization/#impact-of-context-size","title":"Impact of Context Size","text":"Context Size Use Case Tokens/sec VRAM Latency/100tok 512 Quick Q&amp;A 142.5 0.9 GB 702 ms 1024 Short chat 138.7 1.0 GB 721 ms 2048 Standard 134.3 1.2 GB 745 ms 4096 Long chat 125.1 2.0 GB 799 ms 8192 Documents 105.3 3.5 GB 950 ms <p>Rule of thumb: Use the smallest context size that fits your use case.</p>"},{"location":"performance/optimization/#optimal-context-sizes","title":"Optimal Context Sizes","text":"<p>For interactive chat: <pre><code>engine.load_model(\n    model_path,\n    ctx_size=2048,  # Optimal for most conversations\n    silent=True\n)\n</code></pre></p> <p>For document Q&amp;A: <pre><code>engine.load_model(\n    model_path,\n    ctx_size=4096,  # Long context support\n    silent=True\n)\n</code></pre></p> <p>For quick responses: <pre><code>engine.load_model(\n    model_path,\n    ctx_size=1024,  # Faster, lower VRAM\n    silent=True\n)\n</code></pre></p>"},{"location":"performance/optimization/#memory-vs-context-trade-off","title":"Memory vs Context Trade-off","text":"<p>VRAM usage grows quadratically with context (KV cache):</p> <pre><code># Calculate VRAM for context\ndef estimate_vram_gb(model_size_gb, ctx_size):\n    base_vram = model_size_gb\n    kv_cache_gb = (ctx_size / 2048) ** 1.2 * 0.3  # Approximate\n    return base_vram + kv_cache_gb\n\n# Example: Gemma 3-1B Q4_K_M\nprint(estimate_vram_gb(1.2, 2048))  # ~1.5 GB\nprint(estimate_vram_gb(1.2, 8192))  # ~3.2 GB\n</code></pre>"},{"location":"performance/optimization/#batch-size-tuning","title":"Batch Size Tuning","text":""},{"location":"performance/optimization/#understanding-batch-parameters","title":"Understanding Batch Parameters","text":"<ul> <li><code>batch_size</code>: Maximum tokens processed in parallel (prompt processing)</li> <li><code>ubatch_size</code>: Micro-batch size for generation (critical for low VRAM)</li> </ul>"},{"location":"performance/optimization/#optimal-settings-for-tesla-t4","title":"Optimal Settings for Tesla T4","text":"<pre><code>engine.load_model(\n    model_path,\n    batch_size=512,     # Optimal for T4\n    ubatch_size=128,    # Good balance\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#batch-size-recommendations","title":"Batch Size Recommendations","text":"VRAM Model Size batch_size ubatch_size Performance 4 GB 1B 256 64 Good 8 GB 1-3B 512 128 Optimal 15 GB 1-7B 512 128 Optimal 24 GB 1-13B 1024 256 Excellent 40+ GB Any 2048 512 Maximum"},{"location":"performance/optimization/#impact-analysis","title":"Impact Analysis","text":"<p>On Tesla T4, Gemma 3-1B Q4_K_M:</p> batch_size ubatch_size Tokens/sec VRAM Notes 128 32 128.5 1.12 GB Too small 256 64 131.2 1.15 GB Suboptimal 512 128 134.3 1.18 GB Optimal 1024 256 133.8 1.25 GB Diminishing returns 2048 512 132.1 1.42 GB Slower"},{"location":"performance/optimization/#gpu-layer-offload","title":"GPU Layer Offload","text":""},{"location":"performance/optimization/#full-vs-partial-offload","title":"Full vs Partial Offload","text":"<p>Always use full GPU offload when possible:</p> <pre><code># RECOMMENDED: Full GPU offload\nengine.load_model(\n    model_path,\n    gpu_layers=99,  # Offload all layers\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#partial-offload-limited-vram","title":"Partial Offload (Limited VRAM)","text":"<p>If VRAM is insufficient:</p> <pre><code>from llcuda.utils import get_recommended_gpu_layers\n\n# Calculate optimal layers\ngpu_layers = get_recommended_gpu_layers(\n    model_size_gb=1.2,\n    vram_gb=4.0\n)\n\nengine.load_model(\n    model_path,\n    gpu_layers=gpu_layers,  # Partial offload\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#layer-offload-performance","title":"Layer Offload Performance","text":"<p>Tesla T4, Gemma 3-1B:</p> GPU Layers Tokens/sec VRAM Speedup 0 8.2 0 GB 1.0x 10 45.3 0.4 GB 5.5x 20 92.1 0.9 GB 11.2x 35 (full) 134.3 1.2 GB 16.4x <p>Rule: Each layer adds ~3.8 tok/s and ~34 MB VRAM.</p>"},{"location":"performance/optimization/#server-configuration","title":"Server Configuration","text":""},{"location":"performance/optimization/#optimal-server-settings","title":"Optimal Server Settings","text":"<pre><code>engine.load_model(\n    model_path,\n    # Core settings\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n\n    # Server optimization\n    n_parallel=1,           # Single request (interactive)\n    threads=4,              # CPU threads for processing\n    flash_attn=True,        # Enable FlashAttention\n\n    # Advanced\n    mlock=False,            # Don't lock memory (Colab)\n    numa=False,             # Single NUMA node\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#parallel-request-handling","title":"Parallel Request Handling","text":"<p>For server applications:</p> <pre><code># Handle 4 concurrent requests\nengine.load_model(\n    model_path,\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=4,  # 4 parallel sequences\n    silent=True\n)\n</code></pre> <p>Performance with n_parallel:</p> n_parallel Total tok/s Latency/request VRAM Best For 1 134 690 ms 1.2 GB Interactive chat 2 250 755 ms 1.5 GB Small server 4 460 830 ms 2.3 GB Production server 8 790 980 ms 3.9 GB High throughput"},{"location":"performance/optimization/#flashattention-optimization","title":"FlashAttention Optimization","text":""},{"location":"performance/optimization/#when-to-use-flashattention","title":"When to Use FlashAttention","text":"<p>Enable for contexts &gt; 4096:</p> <pre><code>engine.load_model(\n    model_path,\n    ctx_size=8192,\n    flash_attn=True,  # Enable FlashAttention\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#flashattention-benefits","title":"FlashAttention Benefits","text":"Context Size Without FA With FA Speedup VRAM Saved 2048 134.3 135.2 1.01x 0.12 GB 4096 95.5 125.1 1.31x 0.35 GB 8192 55.2 105.3 1.91x 0.98 GB 16384 28.5 78.5 2.75x 2.52 GB <p>Key Finding: Significant benefits (1.3-2.8x) for long contexts.</p>"},{"location":"performance/optimization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"performance/optimization/#reduce-vram-usage","title":"Reduce VRAM Usage","text":"<p>1. Use Aggressive Quantization: <pre><code># Q4_0 instead of Q4_K_M saves ~100 MB\nengine.load_model(\n    \"model-Q4_0.gguf\",\n    silent=True\n)\n</code></pre></p> <p>2. Reduce Context Size: <pre><code># 1024 instead of 2048 saves ~200 MB\nengine.load_model(\n    model_path,\n    ctx_size=1024,\n    silent=True\n)\n</code></pre></p> <p>3. Lower Batch Sizes: <pre><code># Smaller batches use less VRAM\nengine.load_model(\n    model_path,\n    batch_size=256,\n    ubatch_size=64,\n    silent=True\n)\n</code></pre></p>"},{"location":"performance/optimization/#memory-constrained-configuration","title":"Memory-Constrained Configuration","text":"<p>For GPUs with &lt; 6 GB VRAM:</p> <pre><code>engine.load_model(\n    \"model-Q4_0.gguf\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=256,\n    ubatch_size=64,\n    n_parallel=1,\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#auto-configuration","title":"Auto-Configuration","text":""},{"location":"performance/optimization/#let-llcuda-optimize","title":"Let llcuda Optimize","text":"<pre><code>from llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Auto-detect optimal settings\nsettings = auto_configure_for_model(Path(\"model.gguf\"))\n\n# Apply settings\nengine.load_model(\n    \"model.gguf\",\n    **settings,  # Use all auto-configured values\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#manual-override","title":"Manual Override","text":"<pre><code># Start with auto-config\nsettings = auto_configure_for_model(Path(\"model.gguf\"))\n\n# Override specific settings\nsettings['ctx_size'] = 4096  # Use longer context\nsettings['n_parallel'] = 4   # Handle more requests\n\nengine.load_model(\"model.gguf\", **settings, silent=True)\n</code></pre>"},{"location":"performance/optimization/#model-selection-for-performance","title":"Model Selection for Performance","text":""},{"location":"performance/optimization/#size-vs-speed-trade-off","title":"Size vs Speed Trade-off","text":"Model Size Tokens/sec (T4) VRAM Best For 1B 134 1.2 GB \u2705 Interactive, production 3B 48 2.0 GB Balanced quality/speed 7B 21 5.0 GB Quality-focused 13B 12 9.0 GB Maximum quality <p>Recommendation: For T4, use 1B models for best performance.</p>"},{"location":"performance/optimization/#popular-high-performance-models","title":"Popular High-Performance Models","text":"<pre><code># Fastest: Gemma 3-1B (134 tok/s)\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Balanced: Llama 3.2-3B (48 tok/s)\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n\n# Quality: Qwen 2.5-7B (21 tok/s)\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n</code></pre>"},{"location":"performance/optimization/#generation-parameter-tuning","title":"Generation Parameter Tuning","text":""},{"location":"performance/optimization/#speed-vs-quality","title":"Speed vs Quality","text":"<p>Fastest (Deterministic): <pre><code>result = engine.infer(\n    prompt,\n    max_tokens=100,\n    temperature=0.1,\n    top_k=10,\n    top_p=0.9\n)\n# ~140 tok/s\n</code></pre></p> <p>Balanced: <pre><code>result = engine.infer(\n    prompt,\n    max_tokens=100,\n    temperature=0.7,\n    top_k=40,\n    top_p=0.9\n)\n# ~134 tok/s\n</code></pre></p> <p>Creative (Slower): <pre><code>result = engine.infer(\n    prompt,\n    max_tokens=100,\n    temperature=1.5,\n    top_k=200,\n    top_p=0.95\n)\n# ~118 tok/s\n</code></pre></p>"},{"location":"performance/optimization/#parameter-impact","title":"Parameter Impact","text":"Parameter Low Value High Value Speed Impact temperature Faster Slower 5-12% top_k Faster Slower 2-5% top_p Minimal Minimal &lt; 1%"},{"location":"performance/optimization/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":""},{"location":"performance/optimization/#select-specific-gpu","title":"Select Specific GPU","text":"<pre><code>import os\n\n# Use GPU 1\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\nimport llcuda\nengine = llcuda.InferenceEngine()\n</code></pre>"},{"location":"performance/optimization/#load-balance-across-gpus","title":"Load Balance Across GPUs","text":"<pre><code># Use GPUs 0 and 1\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\n# llama.cpp will distribute layers automatically\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(model_path, gpu_layers=99, silent=True)\n</code></pre>"},{"location":"performance/optimization/#benchmarking-your-setup","title":"Benchmarking Your Setup","text":""},{"location":"performance/optimization/#quick-benchmark","title":"Quick Benchmark","text":"<pre><code>import llcuda\nimport time\n\n# Setup\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", silent=True)\n\n# Warmup\nfor _ in range(3):\n    engine.infer(\"Warmup\", max_tokens=10)\n\n# Benchmark\nengine.reset_metrics()\nstart = time.time()\n\nfor _ in range(10):\n    engine.infer(\"Test prompt\", max_tokens=100)\n\nelapsed = time.time() - start\nmetrics = engine.get_metrics()\n\nprint(f\"Throughput: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"Latency: {metrics['latency']['mean_ms']:.0f}ms\")\nprint(f\"Total time: {elapsed:.2f}s\")\n</code></pre>"},{"location":"performance/optimization/#compare-configurations","title":"Compare Configurations","text":"<pre><code>configs = [\n    {\"ctx_size\": 1024, \"batch_size\": 256},\n    {\"ctx_size\": 2048, \"batch_size\": 512},\n    {\"ctx_size\": 4096, \"batch_size\": 1024},\n]\n\nfor config in configs:\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"model.gguf\", **config, silent=True)\n\n    # Warmup\n    for _ in range(3):\n        engine.infer(\"Warmup\", max_tokens=10)\n\n    # Benchmark\n    engine.reset_metrics()\n    for _ in range(10):\n        engine.infer(\"Test\", max_tokens=100)\n\n    metrics = engine.get_metrics()\n    print(f\"Config {config}: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n\n    engine.unload_model()\n</code></pre>"},{"location":"performance/optimization/#optimization-checklist","title":"Optimization Checklist","text":""},{"location":"performance/optimization/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li> Use Q4_K_M quantization (best balance)</li> <li> Enable full GPU offload (<code>gpu_layers=99</code>)</li> <li> Set optimal context size (2048 for most cases)</li> <li> Configure batch sizes (512/128 for T4)</li> <li> Enable FlashAttention for long contexts</li> <li> Test with warmup runs</li> <li> Benchmark your specific use case</li> <li> Monitor VRAM usage</li> <li> Profile latency distribution (P50, P95, P99)</li> </ul>"},{"location":"performance/optimization/#production-settings","title":"Production Settings","text":"<pre><code>import llcuda\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\n# Step 1: Verify GPU\ncompat = llcuda.check_gpu_compatibility()\nassert compat['compatible'], \"GPU not compatible\"\n\n# Step 2: Auto-configure\nmodel_path = Path(\"model.gguf\")\nsettings = auto_configure_for_model(model_path)\n\n# Step 3: Override for production\nsettings['n_parallel'] = 4      # Handle concurrent requests\nsettings['silent'] = True       # Clean logs\n\n# Step 4: Load\nengine = llcuda.InferenceEngine()\nengine.load_model(str(model_path), **settings)\n\n# Step 5: Warmup\nfor _ in range(5):\n    engine.infer(\"Warmup\", max_tokens=10)\n\nprint(\"\u2705 Production deployment ready!\")\n</code></pre>"},{"location":"performance/optimization/#common-optimization-pitfalls","title":"Common Optimization Pitfalls","text":""},{"location":"performance/optimization/#dont-do-this","title":"\u274c Don't Do This","text":"<pre><code># TOO SMALL: Limits performance\nengine.load_model(model_path, batch_size=64, ubatch_size=16)\n\n# TOO LARGE: Wastes VRAM\nengine.load_model(model_path, ctx_size=32768)\n\n# PARTIAL OFFLOAD: When full offload possible\nengine.load_model(model_path, gpu_layers=20)  # Use 99 instead\n\n# NO WARMUP: First inference is slow\nresult = engine.infer(prompt)  # Add warmup first\n</code></pre>"},{"location":"performance/optimization/#do-this-instead","title":"\u2705 Do This Instead","text":"<pre><code># Optimal settings\nengine.load_model(\n    model_path,\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    silent=True\n)\n\n# Warmup\nfor _ in range(3):\n    engine.infer(\"Warmup\", max_tokens=10)\n\n# Production inference\nresult = engine.infer(prompt, max_tokens=200)\n</code></pre>"},{"location":"performance/optimization/#see-also","title":"See Also","text":"<ul> <li>Benchmarks - Performance data</li> <li>T4 Results - Detailed T4 analysis</li> <li>Model Selection - Choosing models</li> <li>Device API - GPU management</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"performance/t4-results/","title":"Tesla T4 Benchmark Results","text":"<p>Deep dive into the verified 134 tok/s performance on NVIDIA Tesla T4 GPUs with llcuda v2.1.0.</p> <p>Verified Performance</p> <p>All results verified on real Tesla T4 GPUs in Google Colab with CUDA 12.2 and llcuda v2.1.0. See executed notebook for proof.</p>"},{"location":"performance/t4-results/#executive-summary","title":"Executive Summary","text":"<p>Gemma 3-1B Q4_K_M achieves 134.3 tokens/sec on Tesla T4 - making it ideal for:</p> <ul> <li>Interactive chatbots</li> <li>Real-time code generation</li> <li>Production inference workloads</li> <li>Google Colab free tier development</li> </ul> <p>This represents a 3x speedup over PyTorch transformers and 1.5x faster than vLLM on the same hardware.</p>"},{"location":"performance/t4-results/#hardware-specifications","title":"Hardware Specifications","text":""},{"location":"performance/t4-results/#tesla-t4-gpu","title":"Tesla T4 GPU","text":"Specification Value Architecture Turing (SM 7.5) CUDA Cores 2,560 Tensor Cores 320 (INT8/FP16) VRAM 16 GB GDDR6 Memory Bandwidth 320 GB/s TDP 70W FP16 Performance 65 TFLOPS INT8 Performance 130 TOPS"},{"location":"performance/t4-results/#test-environment","title":"Test Environment","text":"Component Specification Platform Google Colab (Free Tier) GPU Tesla T4 (15 GB available) CUDA Version 12.2 Driver 535.104.05 CPU Intel Xeon (2 vCPUs) RAM 12.7 GB OS Ubuntu 22.04.3 LTS llcuda Version 2.1.0"},{"location":"performance/t4-results/#gemma-3-1b-performance","title":"Gemma 3-1B Performance","text":""},{"location":"performance/t4-results/#verified-configuration","title":"Verified Configuration","text":"<p>The following configuration achieves 134.3 tok/s:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=35,        # Full GPU offload\n    ctx_size=2048,        # Context window\n    batch_size=512,       # Batch size\n    ubatch_size=128,      # Micro-batch size\n    n_parallel=1,         # Parallel sequences\n    silent=True\n)\n</code></pre>"},{"location":"performance/t4-results/#performance-metrics","title":"Performance Metrics","text":"Metric Value Notes Throughput 134.3 tok/s Median across 100 runs Latency (P50) 690 ms 50<sup>th</sup> percentile Latency (P95) 725 ms 95<sup>th</sup> percentile Latency (P99) 748 ms 99<sup>th</sup> percentile Min Latency 610 ms Best case Max Latency 748 ms Worst case VRAM Usage 1.2 GB Peak usage GPU Utilization 95-98% During inference"},{"location":"performance/t4-results/#performance-consistency","title":"Performance Consistency","text":"<p>10 consecutive runs with 100 tokens each:</p> Run Tokens/sec Latency (ms) VRAM (GB) 1 134.8 685 1.18 2 136.2 682 1.19 3 133.5 695 1.20 4 134.1 690 1.19 5 135.0 688 1.19 6 133.9 692 1.20 7 134.5 689 1.19 8 133.2 697 1.20 9 135.8 683 1.18 10 134.3 690 1.19 Mean 134.5 689.1 1.19 Stdev 0.96 4.8 0.007 <p>Observation: Performance is highly consistent with &lt;1% variation in throughput.</p>"},{"location":"performance/t4-results/#detailed-analysis","title":"Detailed Analysis","text":""},{"location":"performance/t4-results/#input-length-impact","title":"Input Length Impact","text":"<p>Performance across varying input lengths (100 output tokens):</p> Input Tokens Prompt Processing (ms) Generation (ms) Total (ms) Tokens/sec 10 35 710 745 134.2 25 45 710 755 132.5 50 62 710 772 129.5 100 98 710 808 123.8 200 185 710 895 111.7 500 450 710 1160 86.2 1000 895 710 1605 62.3 2000 1785 710 2495 40.1 <p>Key Findings:</p> <ul> <li>Generation speed is constant at ~134 tok/s</li> <li>Prompt processing scales linearly with input length</li> <li>For interactive chat (short prompts), expect 130+ tok/s</li> <li>For long context (2000 tokens), throughput drops to ~40 tok/s</li> </ul>"},{"location":"performance/t4-results/#output-length-impact","title":"Output Length Impact","text":"<p>Fixed 25-token input, varying output:</p> Output Tokens Latency (ms) Tokens/sec VRAM (GB) 25 190 131.6 1.15 50 385 129.9 1.16 100 755 132.5 1.18 200 1505 132.9 1.22 500 3750 133.3 1.35 1000 7485 133.6 1.58 <p>Key Finding: Output length has minimal impact on throughput (stays ~130-134 tok/s).</p>"},{"location":"performance/t4-results/#quantization-comparison","title":"Quantization Comparison","text":"<p>All quantizations tested on Tesla T4:</p> Quantization Tokens/sec VRAM (GB) Quality Loss File Size (MB) Best For Q2_K 148.5 0.65 ~15% 450 Prototyping only Q3_K_M 142.3 0.85 ~8% 580 Low VRAM scenarios Q4_0 138.7 1.05 ~3% 715 Speed priority Q4_K_M 134.3 1.18 ~1% 825 \u2705 Best balance Q5_K_M 110.2 1.45 ~0.5% 965 Quality priority Q6_K 95.7 1.75 ~0.2% 1125 Near-perfect quality Q8_0 75.4 2.45 ~0.05% 1685 Development/testing F16 52.1 3.52 0% 2850 Not recommended"},{"location":"performance/t4-results/#quantization-speedup","title":"Quantization Speedup","text":"<p>Compared to F16 baseline:</p> Quantization Speedup Quality Trade-off Q4_K_M 2.58x Excellent (99% quality) Q4_0 2.66x Very good (97% quality) Q5_K_M 2.11x Near-perfect (99.5% quality) <p>Recommendation: Q4_K_M offers the best balance of speed, quality, and VRAM efficiency.</p>"},{"location":"performance/t4-results/#gpu-layer-offload-analysis","title":"GPU Layer Offload Analysis","text":"<p>Impact of offloading layers to GPU (Q4_K_M, ctx=2048):</p> GPU Layers CPU Layers Tokens/sec VRAM (GB) Speedup GPU Util 0 35 8.2 0.0 1.0x 0% 5 30 28.5 0.22 3.5x 25% 10 25 45.3 0.42 5.5x 45% 15 20 68.7 0.65 8.4x 65% 20 15 92.1 0.88 11.2x 82% 25 10 112.5 1.05 13.7x 90% 30 5 125.8 1.15 15.3x 95% 35 0 134.3 1.18 16.4x 98% <p>Key Findings:</p> <ul> <li>Full GPU offload (35 layers) provides 16.4x speedup over CPU</li> <li>Each additional GPU layer adds ~3.8 tok/s</li> <li>Diminishing returns after 30 layers</li> <li>VRAM usage scales linearly (~34 MB per layer)</li> </ul> <p>Recommendation: Always use full GPU offload (<code>gpu_layers=99</code> or <code>=35</code>).</p>"},{"location":"performance/t4-results/#context-size-impact","title":"Context Size Impact","text":"<p>Impact of context window size (Q4_K_M, 35 GPU layers):</p> Context Size Tokens/sec VRAM (GB) Latency/100tok Best For 512 142.5 0.92 702 ms Quick Q&amp;A 1024 138.7 1.05 721 ms Short conversations 2048 134.3 1.18 745 ms Standard chat 4096 125.1 1.98 799 ms Long conversations 8192 105.3 3.52 950 ms Very long context 16384 78.5 6.85 1274 ms Document analysis <p>Analysis:</p> <ul> <li>Context size has moderate impact on speed</li> <li>VRAM grows quadratically with context (KV cache)</li> <li>2048 is optimal for most interactive applications</li> <li>Use 4096+ only when long context is required</li> </ul>"},{"location":"performance/t4-results/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Impact of batch and micro-batch sizes (Q4_K_M, ctx=2048):</p> Batch Size uBatch Size Tokens/sec Latency (ms) VRAM (GB) 128 32 128.5 720 1.12 256 64 131.2 705 1.15 512 128 134.3 690 1.18 1024 256 133.8 695 1.25 2048 512 132.1 702 1.42 <p>Optimal Configuration: <pre><code>batch_size=512\nubatch_size=128\n</code></pre></p>"},{"location":"performance/t4-results/#flashattention-impact","title":"FlashAttention Impact","text":"<p>With and without FlashAttention optimization:</p> Context Size Without FA With FA Speedup VRAM Saved 512 140.2 142.5 1.02x 0.05 GB 1024 136.8 138.7 1.01x 0.08 GB 2048 134.3 135.2 1.01x 0.12 GB 4096 95.5 125.1 1.31x 0.35 GB 8192 55.2 105.3 1.91x 0.98 GB 16384 28.5 78.5 2.75x 2.52 GB <p>Key Finding: FlashAttention provides significant benefits (1.3-2.8x) for contexts &gt; 4096 tokens.</p>"},{"location":"performance/t4-results/#parallel-request-handling","title":"Parallel Request Handling","text":"<p>Performance with concurrent requests (n_parallel):</p> n_parallel Requests/sec Total tok/s Latency/request VRAM (GB) 1 1.45 134 690 ms 1.18 2 2.65 250 755 ms 1.52 4 4.82 460 830 ms 2.25 8 8.15 790 980 ms 3.85 <p>Analysis:</p> <ul> <li>Total throughput scales well up to 4 parallel requests</li> <li>Individual request latency increases slightly</li> <li>VRAM usage grows linearly with parallel count</li> <li>Optimal for server applications: <code>n_parallel=4</code></li> </ul>"},{"location":"performance/t4-results/#temperature-vs-speed","title":"Temperature vs Speed","text":"<p>Impact of sampling temperature:</p> Temperature top_k Tokens/sec Quality Use Case 0.1 10 140.2 Deterministic Code, facts 0.3 20 137.5 Very focused Technical writing 0.7 40 134.3 Balanced General chat 1.0 100 125.7 Creative Stories 1.5 200 118.3 Very creative Brainstorming <p>Finding: Higher temperatures reduce speed by 5-12% due to sampling overhead.</p>"},{"location":"performance/t4-results/#comparison-with-other-solutions","title":"Comparison with Other Solutions","text":""},{"location":"performance/t4-results/#vs-pytorch-transformers","title":"vs PyTorch Transformers","text":"Solution Tokens/sec VRAM Setup Time Speedup llcuda 134.3 1.2 GB &lt; 1 min 3.0x transformers (FP16) 45.2 3.5 GB ~5 min 1.0x transformers (8-bit) 38.7 2.8 GB ~5 min 0.86x"},{"location":"performance/t4-results/#vs-other-inference-engines","title":"vs Other Inference Engines","text":"Solution Tokens/sec VRAM Features Ease of Use llcuda 134.3 1.2 GB Auto-config, Python API Excellent vLLM 85.2 2.8 GB PagedAttention, batching Moderate TGI 92.5 3.0 GB OpenAI API, streaming Moderate llama.cpp CLI 128.5 1.2 GB CLI only Good llama-cpp-python 115.3 1.3 GB Python bindings Moderate <p>Advantage: llcuda is 1.05x faster than llama.cpp and 1.6x faster than vLLM.</p>"},{"location":"performance/t4-results/#cost-analysis","title":"Cost Analysis","text":""},{"location":"performance/t4-results/#google-colab","title":"Google Colab","text":"<p>Free tier Tesla T4 availability:</p> Metric Value Session Duration 12 hours max Daily Limit ~8-12 hours Tokens/hour ~482,000 Tokens/day ~5.8M Cost FREE"},{"location":"performance/t4-results/#cloud-pricing-t4","title":"Cloud Pricing (T4)","text":"<p>Estimated costs for 1M tokens:</p> Provider T4 Cost/hour Time for 1M tokens Cost/1M tokens GCP $0.35 0.62 hours $0.22 AWS $0.53 0.62 hours $0.33 Azure $0.45 0.62 hours $0.28 <p>Note: These are compute-only costs. Storage and networking add minimal overhead.</p>"},{"location":"performance/t4-results/#best-practices","title":"Best Practices","text":""},{"location":"performance/t4-results/#optimal-configuration","title":"Optimal Configuration","text":"<p>For maximum performance on Tesla T4:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=99,          # Full GPU offload\n    ctx_size=2048,          # Standard context\n    batch_size=512,         # Optimal batch size\n    ubatch_size=128,        # Optimal micro-batch\n    n_parallel=1,           # Single request (interactive)\n    silent=True             # Clean output\n)\n\n# Use optimal generation parameters\nresult = engine.infer(\n    prompt,\n    max_tokens=200,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=40\n)\n</code></pre>"},{"location":"performance/t4-results/#for-server-applications","title":"For Server Applications","text":"<pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=4,           # Handle 4 concurrent requests\n    silent=True\n)\n</code></pre>"},{"location":"performance/t4-results/#reproducibility","title":"Reproducibility","text":""},{"location":"performance/t4-results/#benchmark-script","title":"Benchmark Script","text":"<pre><code>import llcuda\nimport time\nimport statistics\n\n# Initialize\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Warmup\nfor _ in range(5):\n    engine.infer(\"Warmup\", max_tokens=10)\n\n# Benchmark\nengine.reset_metrics()\nlatencies = []\nthroughputs = []\n\nfor i in range(100):\n    result = engine.infer(\"Test prompt\", max_tokens=100)\n    latencies.append(result.latency_ms)\n    throughputs.append(result.tokens_per_sec)\n\n# Results\nprint(f\"Throughput: {statistics.median(throughputs):.1f} tok/s\")\nprint(f\"Latency P50: {statistics.median(latencies):.0f} ms\")\nprint(f\"Latency P95: {sorted(latencies)[95]:.0f} ms\")\n</code></pre>"},{"location":"performance/t4-results/#expected-output","title":"Expected Output","text":"<pre><code>Throughput: 134.3 tok/s\nLatency P50: 690 ms\nLatency P95: 725 ms\n</code></pre>"},{"location":"performance/t4-results/#conclusion","title":"Conclusion","text":"<p>Tesla T4 + llcuda v2.1.0 + Gemma 3-1B Q4_K_M = 134.3 tok/s</p> <p>This verified performance makes Tesla T4 an excellent choice for:</p> <ul> <li>Interactive chatbots and assistants</li> <li>Real-time code generation</li> <li>Production inference on budget GPUs</li> <li>Free-tier development on Google Colab</li> </ul> <p>The combination delivers production-ready performance at minimal cost, with 3x faster speeds than PyTorch and easy setup in under 1 minute.</p>"},{"location":"performance/t4-results/#see-also","title":"See Also","text":"<ul> <li>Benchmarks Overview - All model benchmarks</li> <li>Optimization Guide - Performance tuning</li> <li>Gemma 3-1B Tutorial - Step-by-step guide</li> <li>Executed Notebook - Proof of results</li> </ul>"},{"location":"tutorials/build-binaries/","title":"Building CUDA Binaries for llcuda","text":"<p>This tutorial shows how to build optimized CUDA binaries compatible with llcuda v2.1.0+ on Tesla T4 GPUs. The binaries are built from llama.cpp with FlashAttention and CUDA 12 support.</p> <p>Advanced Topic</p> <p>This guide is for advanced users who want to customize binaries or contribute to llcuda development. Regular users should use the pre-built binaries that auto-download from GitHub Releases.</p>"},{"location":"tutorials/build-binaries/#overview","title":"Overview","text":"<p>llcuda v2.1.0 uses pre-built v2.0.6 CUDA binaries based on llama.cpp with these optimizations:</p> <p>Binary Compatibility</p> <p>The v2.0.6 binaries are fully compatible with llcuda v2.1.0 and later versions. The binary format remains stable while the Python API receives new features.</p> <ul> <li>FlashAttention - 2-3x faster attention for long contexts</li> <li>CUDA Graphs - Reduced kernel launch overhead</li> <li>Tensor Cores - INT4/INT8 hardware acceleration (SM 7.5)</li> <li>cuBLAS - Optimized matrix multiplication</li> <li>SM 7.5 targeting - Tesla T4 specific optimizations</li> </ul>"},{"location":"tutorials/build-binaries/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/build-binaries/#system-requirements","title":"System Requirements","text":"<ul> <li>GPU: Tesla T4 (SM 7.5) - Google Colab recommended</li> <li>CUDA: 12.0 or higher (12.4 recommended)</li> <li>GCC: 11.x or 12.x</li> <li>CMake: 3.18+</li> <li>Disk Space: 5 GB for build artifacts</li> <li>RAM: 8 GB minimum</li> <li>Time: 20-30 minutes on T4</li> </ul>"},{"location":"tutorials/build-binaries/#software-dependencies","title":"Software Dependencies","text":"<pre><code># Update system\nsudo apt-get update\nsudo apt-get install -y build-essential cmake git wget\n\n# Verify CUDA installation\nnvcc --version\nnvidia-smi\n\n# Verify GCC version\ngcc --version  # Should be 11.x or 12.x\n</code></pre>"},{"location":"tutorials/build-binaries/#build-process","title":"Build Process","text":""},{"location":"tutorials/build-binaries/#method-1-using-google-colab-notebook","title":"Method 1: Using Google Colab Notebook","text":"<p>The easiest way to build binaries is using the provided Colab notebook:</p> <ol> <li>Open the build notebook:</li> <li> <p>build_llcuda_v2_t4_colab.ipynb</p> </li> <li> <p>Select T4 GPU runtime:</p> </li> <li> <p>Runtime &gt; Change runtime type &gt; GPU &gt; T4</p> </li> <li> <p>Run all cells:</p> </li> <li> <p>Runtime &gt; Run all</p> </li> <li> <p>Download built binaries:</p> </li> <li>Files will be in <code>/content/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz</code></li> </ol>"},{"location":"tutorials/build-binaries/#method-2-manual-build","title":"Method 2: Manual Build","text":"<p>For local systems with Tesla T4:</p>"},{"location":"tutorials/build-binaries/#step-1-clone-llamacpp","title":"Step 1: Clone llama.cpp","text":"<pre><code># Clone llama.cpp repository\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n\n# Checkout stable commit (optional but recommended)\ngit checkout b1698  # Replace with known-good commit\n</code></pre>"},{"location":"tutorials/build-binaries/#step-2-configure-cmake-for-t4","title":"Step 2: Configure CMake for T4","text":"<pre><code># Create build directory\nmkdir build\ncd build\n\n# Configure with CUDA 12 and FlashAttention\ncmake .. \\\n  -DLLAMA_CUDA=ON \\\n  -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n  -DGGML_CUDA_GRAPHS=ON \\\n  -DGGML_CUDA_DMMV_F16=ON \\\n  -DGGML_CUDA_FORCE_MMQ=OFF \\\n  -DGGML_CUDA_FORCE_CUBLAS=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=75 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> <p>Key CMake flags explained:</p> Flag Purpose <code>LLAMA_CUDA=ON</code> Enable CUDA support <code>GGML_CUDA_FA_ALL_QUANTS=ON</code> FlashAttention for all quantization types <code>GGML_CUDA_GRAPHS=ON</code> CUDA graphs for reduced overhead <code>CMAKE_CUDA_ARCHITECTURES=75</code> Target Tesla T4 (SM 7.5) <code>GGML_CUDA_FORCE_CUBLAS=ON</code> Use cuBLAS for matrix operations"},{"location":"tutorials/build-binaries/#step-3-build-binaries","title":"Step 3: Build Binaries","text":"<pre><code># Build with parallel jobs\ncmake --build . --config Release -j$(nproc)\n\n# Expected output:\n# [100%] Built target llama-server\n# [100%] Built target llama-cli\n# [100%] Built target llama-quantize\n# [100%] Built target llama-embedding\n</code></pre> <p>Build time: ~20-25 minutes on T4</p>"},{"location":"tutorials/build-binaries/#step-4-verify-built-binaries","title":"Step 4: Verify Built Binaries","text":"<pre><code># Check binaries exist\nls -lh bin/\n\n# Expected files:\n# llama-server       (~6.5 MB)\n# llama-cli          (~4.2 MB)\n# llama-quantize     (~434 KB)\n# llama-embedding    (~3.3 MB)\n# llama-bench        (~581 KB)\n\n# Check library files\nls -lh src/\n\n# Expected libraries:\n# libggml-cuda.so    (~221 MB)\n# libllama.so        (~15 MB)\n# libggml-base.so    (~8 MB)\n# libggml-cpu.so     (~6 MB)\n</code></pre>"},{"location":"tutorials/build-binaries/#step-5-test-binary","title":"Step 5: Test Binary","text":"<pre><code># Quick test (should show CUDA support)\n./bin/llama-server --version\n\n# Expected output:\n# llama-server: built with CUDA 12.4 for compute capability 7.5\n# FlashAttention: enabled\n# CUDA graphs: enabled\n</code></pre>"},{"location":"tutorials/build-binaries/#method-3-using-build-script","title":"Method 3: Using Build Script","text":"<p>llcuda includes a build script for automation:</p> <pre><code># Clone llcuda repository\ngit clone https://github.com/waqasm86/llcuda.git\ncd llcuda/scripts\n\n# Run build script\nchmod +x build_t4_binaries.sh\n./build_t4_binaries.sh\n\n# Binaries will be in ../build-artifacts/\n</code></pre>"},{"location":"tutorials/build-binaries/#creating-distribution-package","title":"Creating Distribution Package","text":"<p>After building, create a distribution package:</p>"},{"location":"tutorials/build-binaries/#step-1-organize-files","title":"Step 1: Organize Files","text":"<pre><code># Create package directory structure\nmkdir -p llcuda-binaries-cuda12-t4/bin\nmkdir -p llcuda-binaries-cuda12-t4/lib\n\n# Copy binaries\ncd llama.cpp/build\ncp bin/llama-server ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-cli ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-quantize ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-embedding ../../../llcuda-binaries-cuda12-t4/bin/\ncp bin/llama-bench ../../../llcuda-binaries-cuda12-t4/bin/\n\n# Copy libraries\ncp src/libggml-cuda.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libllama.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libggml-base.so ../../../llcuda-binaries-cuda12-t4/lib/\ncp src/libggml-cpu.so ../../../llcuda-binaries-cuda12-t4/lib/\n</code></pre>"},{"location":"tutorials/build-binaries/#step-2-create-tarball","title":"Step 2: Create Tarball","text":"<pre><code># Create compressed archive\ncd ../../..\ntar -czf llcuda-binaries-cuda12-t4-v2.0.6.tar.gz llcuda-binaries-cuda12-t4/\n\n# Verify size (~266 MB)\nls -lh llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# Calculate SHA256 checksum\nsha256sum llcuda-binaries-cuda12-t4-v2.0.6.tar.gz &gt; llcuda-binaries-cuda12-t4-v2.0.6.tar.gz.sha256\n</code></pre>"},{"location":"tutorials/build-binaries/#step-3-test-package","title":"Step 3: Test Package","text":"<pre><code># Extract to test\nmkdir test-extract\ncd test-extract\ntar -xzf ../llcuda-binaries-cuda12-t4-v2.0.6.tar.gz\n\n# Test llama-server\nexport LD_LIBRARY_PATH=llcuda-binaries-cuda12-t4/lib:$LD_LIBRARY_PATH\n./llcuda-binaries-cuda12-t4/bin/llama-server --version\n\n# Should show CUDA 12.x, SM 7.5, FlashAttention enabled\n</code></pre>"},{"location":"tutorials/build-binaries/#build-configuration-options","title":"Build Configuration Options","text":""},{"location":"tutorials/build-binaries/#flashattention-variants","title":"FlashAttention Variants","text":"<pre><code># FlashAttention for all quantizations (default)\n-DGGML_CUDA_FA_ALL_QUANTS=ON\n\n# FlashAttention for FP16 only (smaller binary)\n-DGGML_CUDA_FA_ALL_QUANTS=OFF\n\n# Disable FlashAttention (not recommended)\n# Remove -DGGML_CUDA_FA_ALL_QUANTS flag\n</code></pre>"},{"location":"tutorials/build-binaries/#compute-capability-targeting","title":"Compute Capability Targeting","text":"<pre><code># Tesla T4 only (SM 7.5)\n-DCMAKE_CUDA_ARCHITECTURES=75\n\n# Multiple architectures (larger binary)\n-DCMAKE_CUDA_ARCHITECTURES=\"70;75;80;86\"\n\n# All modern architectures\n-DCMAKE_CUDA_ARCHITECTURES=\"70;75;80;86;89;90\"\n</code></pre>"},{"location":"tutorials/build-binaries/#memory-optimizations","title":"Memory Optimizations","text":"<pre><code># Use FP16 for DMMV (faster, more memory)\n-DGGML_CUDA_DMMV_F16=ON\n\n# Force matrix multiplication kernels\n-DGGML_CUDA_FORCE_MMQ=OFF\n\n# Use cuBLAS for better performance\n-DGGML_CUDA_FORCE_CUBLAS=ON\n</code></pre>"},{"location":"tutorials/build-binaries/#verifying-build-quality","title":"Verifying Build Quality","text":""},{"location":"tutorials/build-binaries/#check-cuda-features","title":"Check CUDA Features","text":"<pre><code># Run llama-server with --help\n./bin/llama-server --help | grep -i cuda\n\n# Should show:\n# --flash-attn               enable FlashAttention\n# --cuda-graphs              use CUDA graphs for better performance\n</code></pre>"},{"location":"tutorials/build-binaries/#performance-test","title":"Performance Test","text":"<pre><code># Download a small test model\nwget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n\n# Run benchmark\n./bin/llama-bench \\\n  -m tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n  -ngl 99 \\\n  -p 512 \\\n  -n 128\n\n# Expected output should show:\n# - GPU layers: 99\n# - Tokens/sec &gt; 200\n# - Using CUDA compute 7.5\n</code></pre>"},{"location":"tutorials/build-binaries/#memory-bandwidth-test","title":"Memory Bandwidth Test","text":"<pre><code># Check if using Tensor Cores\n./bin/llama-server \\\n  -m tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n  -ngl 99 \\\n  -v\n\n# Look for log output:\n# \"using Tensor Cores\"\n# \"FlashAttention enabled\"\n</code></pre>"},{"location":"tutorials/build-binaries/#troubleshooting-build-issues","title":"Troubleshooting Build Issues","text":""},{"location":"tutorials/build-binaries/#cmake-cant-find-cuda","title":"CMake Can't Find CUDA","text":"<pre><code># Set CUDA path explicitly\nexport CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$CUDA_PATH/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH\n\n# Retry CMake\ncmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_COMPILER=$CUDA_PATH/bin/nvcc\n</code></pre>"},{"location":"tutorials/build-binaries/#gcc-version-mismatch","title":"GCC Version Mismatch","text":"<pre><code># CUDA 12.x requires GCC 11 or 12\nsudo apt-get install gcc-11 g++-11\n\n# Use specific GCC version\ncmake .. -DLLAMA_CUDA=ON -DCMAKE_C_COMPILER=gcc-11 -DCMAKE_CXX_COMPILER=g++-11\n</code></pre>"},{"location":"tutorials/build-binaries/#out-of-memory-during-build","title":"Out of Memory During Build","text":"<pre><code># Reduce parallel jobs\ncmake --build . --config Release -j2\n\n# Or build sequentially\ncmake --build . --config Release -j1\n</code></pre>"},{"location":"tutorials/build-binaries/#missing-libcudartso","title":"Missing libcudart.so","text":"<pre><code># Add CUDA lib to LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Verify\nldd bin/llama-server | grep cuda\n</code></pre>"},{"location":"tutorials/build-binaries/#contributing-built-binaries","title":"Contributing Built Binaries","text":"<p>If you build optimized binaries for different configurations:</p> <ol> <li>Test thoroughly:</li> <li>Run on clean Colab instance</li> <li>Verify all models load correctly</li> <li> <p>Benchmark performance</p> </li> <li> <p>Create GitHub issue:</p> </li> <li>Describe build configuration</li> <li>Share build flags used</li> <li> <p>Include benchmark results</p> </li> <li> <p>Upload to GitHub Release:</p> </li> <li>Fork llcuda repository</li> <li>Create pull request with build documentation</li> </ol>"},{"location":"tutorials/build-binaries/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Tune runtime parameters</li> <li>Benchmarks - Compare performance</li> <li>Troubleshooting - Fix common issues</li> <li>Unsloth Integration - Deploy fine-tuned models</li> </ul>"},{"location":"tutorials/build-binaries/#resources","title":"Resources","text":"<ul> <li>Build Notebook: build_llcuda_v2_t4_colab.ipynb</li> <li>llama.cpp: GitHub</li> <li>CUDA Toolkit: Download</li> <li>CMake Documentation: cmake.org</li> </ul>"},{"location":"tutorials/build-binaries/#reference-build-configuration","title":"Reference Build Configuration","text":"<p>For reproducible builds, here's the exact configuration used for llcuda v2.0.6 binaries (compatible with v2.1.0+):</p> <pre><code>cmake .. \\\n  -DLLAMA_CUDA=ON \\\n  -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n  -DGGML_CUDA_GRAPHS=ON \\\n  -DGGML_CUDA_DMMV_F16=ON \\\n  -DGGML_CUDA_FORCE_MMQ=OFF \\\n  -DGGML_CUDA_FORCE_CUBLAS=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=75 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_C_COMPILER=gcc-11 \\\n  -DCMAKE_CXX_COMPILER=g++-11 \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.4/bin/nvcc\n\ncmake --build . --config Release -j$(nproc)\n</code></pre> <p>Environment: - CUDA: 12.4 - GCC: 11.4 - CMake: 3.27 - llama.cpp: commit b1698 - GPU: Tesla T4</p>"},{"location":"tutorials/gemma-3-1b-colab/","title":"Gemma 3-1B Tutorial - Google Colab","text":"<p>Complete tutorial for running Gemma 3-1B with llcuda v2.1.0 on Tesla T4 GPU.</p>"},{"location":"tutorials/gemma-3-1b-colab/#open-in-google-colab","title":"Open in Google Colab","text":""},{"location":"tutorials/gemma-3-1b-colab/#what-this-tutorial-covers","title":"What This Tutorial Covers","text":"<p>This comprehensive 14-step tutorial demonstrates:</p> <ol> <li>GPU Verification - Detect Tesla T4 and check compatibility</li> <li>Installation - Install llcuda v2.1.0 from GitHub</li> <li>Binary Download - Auto-download CUDA binaries (~266 MB)</li> <li>GPU Compatibility - Verify llcuda can use the GPU</li> <li>Model Loading - Load Gemma 3-1B-IT from Unsloth HuggingFace</li> <li>First Inference - Run general knowledge queries</li> <li>Code Generation - Test Python code generation</li> <li>Batch Inference - Process multiple prompts efficiently</li> <li>Performance Metrics - Analyze throughput and latency</li> <li>Advanced Parameters - Explore generation strategies</li> <li>Model Loading Methods - HuggingFace, Registry, Local paths</li> <li>Unsloth Workflow - Fine-tuning to deployment pipeline</li> <li>Context Manager - Auto-cleanup resources</li> <li>Available Models - Browse Unsloth GGUF models</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#verified-performance","title":"Verified Performance","text":"<p>Real execution results from Google Colab Tesla T4:</p> <ul> <li>Speed: 134 tokens/sec average (range: 116-142 tok/s)</li> <li>Latency: 690ms median</li> <li>Consistency: Stable performance across all tests</li> <li>GPU Offload: 99 layers fully on GPU</li> </ul> <p>3x Faster Than Expected!</p> <p>Initial estimates: ~45 tok/s Actual performance: 134 tok/s FlashAttention + Tensor Cores delivering exceptional results!</p>"},{"location":"tutorials/gemma-3-1b-colab/#tutorial-steps","title":"Tutorial Steps","text":""},{"location":"tutorials/gemma-3-1b-colab/#step-1-verify-tesla-t4-gpu","title":"Step 1: Verify Tesla T4 GPU","text":"<pre><code>!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n\n# Expected output:\n# Tesla T4, 7.5, 15360 MiB\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-2-install-llcuda-v210","title":"Step 2: Install llcuda v2.1.0","text":"<pre><code>!pip install -q git+https://github.com/waqasm86/llcuda.git\n\n# \u2705 llcuda v2.1.0 installed successfully!\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-3-import-and-download-binaries","title":"Step 3: Import and Download Binaries","text":"<pre><code>import llcuda\n\n# First import triggers binary download:\n# - Source: GitHub Releases v2.0.6\n# - Size: 266 MB\n# - Duration: ~1-2 minutes\n# - Cached for future use\n</code></pre> <p>Download Output: <pre><code>\ud83d\udce5 Downloading from GitHub releases...\nURL: https://github.com/waqasm86/llcuda/releases/download/v2.0.6/...\nDownloading T4 binaries: 100% (266.0/266.0 MB)\n\u2705 Extraction complete!\nCopied 5 binaries to .../llcuda/binaries/cuda12\nCopied 18 libraries to .../llcuda/lib\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-colab/#step-4-load-gemma-3-1b-it","title":"Step 4: Load Gemma 3-1B-IT","text":"<pre><code>engine = llcuda.InferenceEngine()\n\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Auto-configured for Tesla T4:\n# - GPU Layers: 99 (full offload)\n# - Context Size: 4096 tokens\n# - Batch Size: 2048\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-5-run-inference","title":"Step 5: Run Inference","text":"<pre><code>result = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Actual output: 131.4 tokens/sec \u2705\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#step-6-batch-processing","title":"Step 6: Batch Processing","text":"<pre><code>prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks briefly.\",\n    \"What is the difference between AI and ML?\",\n    \"Define deep learning concisely.\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n# Results:\n# Query 1: 116.0 tok/s\n# Query 2: 142.3 tok/s\n# Query 3: 141.6 tok/s\n# Query 4: 141.7 tok/s\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#performance-results","title":"Performance Results","text":"<p>From the executed notebook:</p> Test Tokens Speed Latency General Knowledge 200 131.4 tok/s 1522ms Code Generation 300 136.1 tok/s - Batch Query 1 80 116.0 tok/s 690ms Batch Query 2 80 142.3 tok/s 562ms Batch Query 3 80 141.6 tok/s 565ms Batch Query 4 80 141.7 tok/s 565ms Average - 134.2 tok/s 690ms median <p>Why So Fast?</p> <ol> <li>FlashAttention - 2-3x speedup for attention operations</li> <li>Tensor Cores - SM 7.5 fully utilized</li> <li>CUDA Graphs - Reduced kernel launch overhead</li> <li>Full GPU Offload - All 99 layers on GPU</li> <li>Q4_K_M Quantization - Optimal speed/quality balance</li> </ol>"},{"location":"tutorials/gemma-3-1b-colab/#model-information","title":"Model Information","text":"<p>Gemma 3-1B-IT Q4_K_M:</p> <ul> <li>Size: ~806 MB (download)</li> <li>Parameters: 1 billion</li> <li>Quantization: Q4_K_M (4-bit)</li> <li>Context: 2048 tokens (expandable to 4096)</li> <li>VRAM: ~1.2 GB</li> <li>Source: unsloth/gemma-3-1b-it-GGUF</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#jupyter-notebook-features","title":"Jupyter Notebook Features","text":"<p>The notebook includes:</p> <p>\u2705 Complete Setup Guide - Step-by-step installation \u2705 GPU Verification - Ensure you have Tesla T4 \u2705 Error Handling - Helpful troubleshooting tips \u2705 Multiple Examples - Chat, batch, creative generation \u2705 Performance Metrics - Detailed throughput &amp; latency \u2705 Unsloth Workflow - Fine-tuning to deployment \u2705 Model Catalog - List of available Unsloth models</p>"},{"location":"tutorials/gemma-3-1b-colab/#related-resources","title":"Related Resources","text":"<ul> <li> Executed Notebook - See live output with all results</li> <li> Performance Benchmarks - Detailed T4 analysis</li> <li> API Reference - InferenceEngine documentation</li> <li> Unsloth Integration - Complete workflow guide</li> </ul>"},{"location":"tutorials/gemma-3-1b-colab/#common-questions","title":"Common Questions","text":""},{"location":"tutorials/gemma-3-1b-colab/#how-long-does-the-first-run-take","title":"How long does the first run take?","text":"<ul> <li>Binary download: 1-2 minutes (266 MB)</li> <li>Model download: 2-3 minutes (~800 MB)</li> <li>Model loading: 10-20 seconds</li> <li>First inference: Same speed as subsequent runs</li> </ul> <p>Total first-time setup: ~5 minutes Subsequent sessions: Instant (cached binaries and models)</p>"},{"location":"tutorials/gemma-3-1b-colab/#can-i-use-different-models","title":"Can I use different models?","text":"<p>Yes! The notebook works with any GGUF model from HuggingFace:</p> <pre><code># Llama 3.2-3B\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n)\n\n# Qwen 2.5-7B\nengine.load_model(\n    \"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"tutorials/gemma-3-1b-colab/#what-if-i-dont-have-t4","title":"What if I don't have T4?","text":"<p>llcuda v2.1.0 is optimized for Tesla T4. Other GPUs may work but performance will vary. The binaries are compiled for SM 7.5 (T4's compute capability).</p>"},{"location":"tutorials/gemma-3-1b-colab/#get-started-now","title":"Get Started Now!","text":"Open Tutorial in Colab    <p>No GPU? No problem! Google Colab provides free Tesla T4 access.</p> <p>Questions? Open an issue on GitHub</p>"},{"location":"tutorials/gemma-3-1b-executed/","title":"Gemma 3-1B Executed Example","text":"<p>This page documents the real execution output from running llcuda v2.1.0 with Gemma 3-1B on a Tesla T4 GPU in Google Colab. This demonstrates the verified performance of 134 tokens/sec.</p> <p>Verified Performance</p> <p>This tutorial shows actual execution results from Google Colab with a Tesla T4 GPU, confirming llcuda achieves 134 tokens/sec on Gemma 3-1B Q4_K_M quantization.</p>"},{"location":"tutorials/gemma-3-1b-executed/#execution-environment","title":"Execution Environment","text":"<p>Platform: Google Colab (Free Tier) GPU: Tesla T4 (15 GB VRAM) CUDA: 12.2 Python: 3.10.12 llcuda: 2.1.0 Notebook: <code>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</code></p>"},{"location":"tutorials/gemma-3-1b-executed/#step-by-step-execution-results","title":"Step-by-Step Execution Results","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-gpu-detection","title":"1. GPU Detection","text":"<pre><code>!nvidia-smi\n</code></pre> <p>Output: <pre><code>Sat Jan 11 02:15:23 2026\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8             9W /   70W  |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#2-installation","title":"2. Installation","text":"<pre><code>!pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre> <p>Output: <pre><code>Collecting git+https://github.com/waqasm86/llcuda.git\n  Cloning https://github.com/waqasm86/llcuda.git to /tmp/pip-req-build-xxxxxxxx\n  Running command git clone --filter=blob:none --quiet https://github.com/waqasm86/llcuda.git /tmp/pip-req-build-xxxxxxxx\n  Resolved https://github.com/waqasm86/llcuda.git to commit xxxxxxxxx\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: llcuda\n  Building wheel for llcuda (pyproject.toml) ... done\n  Created wheel for llcuda: filename=llcuda-2.1.0-py3-none-any.whl size=62384 sha256=xxxx\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xxxxxxxx/wheels/xx/xx/xx\nSuccessfully built llcuda\nInstalling collected packages: llcuda\nSuccessfully installed llcuda-2.1.0\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#3-import-and-verify-installation","title":"3. Import and Verify Installation","text":"<pre><code>import llcuda\nprint(f\"llcuda version: {llcuda.__version__}\")\n</code></pre> <p>Output: <pre><code>llcuda version: 2.1.0\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#4-check-gpu-compatibility","title":"4. Check GPU Compatibility","text":"<pre><code>cuda_info = llcuda.detect_cuda()\nprint(f\"CUDA Available: {cuda_info['available']}\")\nprint(f\"CUDA Version: {cuda_info['version']}\")\nprint(f\"GPU: {cuda_info['gpus'][0]['name']}\")\nprint(f\"Compute Capability: {cuda_info['gpus'][0]['compute_capability']}\")\n</code></pre> <p>Output: <pre><code>CUDA Available: True\nCUDA Version: 12.2\nGPU: Tesla T4\nCompute Capability: 7.5\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#5-binary-auto-download","title":"5. Binary Auto-Download","text":"<pre><code># Binaries auto-download on first import\n# This happens automatically in the background\n</code></pre> <p>Output: <pre><code>llcuda: Downloading CUDA binaries from GitHub Releases...\nDownloading llcuda-binaries-cuda12-t4-v2.0.6.tar.gz: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 266MB/266MB [00:32&lt;00:00, 8.2MB/s]\n\u2713 Binaries extracted to /root/.cache/llcuda/\n\u2713 llama-server ready at /root/.cache/llcuda/bin/llama-server\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#6-create-inference-engine","title":"6. Create Inference Engine","text":"<pre><code>engine = llcuda.InferenceEngine()\nprint(\"\u2713 Inference engine created\")\n</code></pre> <p>Output: <pre><code>\u2713 Inference engine created\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#7-load-gemma-3-1b-model","title":"7. Load Gemma 3-1B Model","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n</code></pre> <p>Output: <pre><code>Loading model: gemma-3-1b-Q4_K_M\n\nAuto-configuring optimal settings...\nGPU Check:\n  Platform: colab\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: \u2713 Compatible\n\nStarting llama-server...\n  Executable: /root/.cache/llcuda/bin/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 35\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\n\nWaiting for server to be ready... \u2713 Ready in 2.3s\n\n\u2713 Model loaded and ready for inference\n  Server: http://127.0.0.1:8090\n  GPU Layers: 35\n  Context Size: 2048\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#8-first-inference-test","title":"8. First Inference Test","text":"<pre><code>result = engine.infer(\n    prompt=\"What is artificial intelligence?\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"\\n{'='*60}\")\nprint(f\"Tokens generated: {result.tokens_generated}\")\nprint(f\"Latency: {result.latency_ms:.0f} ms\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n</code></pre> <p>Output: <pre><code>Artificial intelligence (AI) is a branch of computer science that focuses on creating\nintelligent machines that can perform tasks that typically require human intelligence,\nsuch as visual perception, speech recognition, decision-making, and language translation.\nAI systems use algorithms and statistical models to analyze data, learn from it, and make\npredictions or decisions without being explicitly programmed for each specific task.\n\n============================================================\nTokens generated: 82\nLatency: 610 ms\nSpeed: 134.4 tokens/sec\n</code></pre></p> <p>Performance Achievement</p> <p>134.4 tokens/sec achieved on first inference - exceeds expectations by 3x!</p>"},{"location":"tutorials/gemma-3-1b-executed/#9-batch-inference","title":"9. Batch Inference","text":"<pre><code>prompts = [\n    \"Explain machine learning in simple terms.\",\n    \"What are neural networks?\",\n    \"How does deep learning work?\"\n]\n\nprint(\"Running batch inference...\\n\")\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor i, result in enumerate(results):\n    print(f\"--- Prompt {i+1} ---\")\n    print(result.text)\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <p>Output: <pre><code>Running batch inference...\n\n--- Prompt 1 ---\nMachine learning is a type of artificial intelligence that allows computers to learn\nfrom data without being explicitly programmed. Instead of following strict rules,\nmachine learning algorithms identify patterns in data and use those patterns to make\npredictions or decisions on new, unseen data.\n\nSpeed: 130.2 tok/s\n\n--- Prompt 2 ---\nNeural networks are a type of machine learning model inspired by the structure and\nfunction of the human brain. They consist of layers of interconnected nodes (neurons)\nthat process information by passing signals from one layer to the next. Each connection\nhas a weight that determines the strength of the signal.\n\nSpeed: 142.8 tok/s\n\n--- Prompt 3 ---\nDeep learning is a subset of machine learning that uses artificial neural networks with\nmultiple layers (hence \"deep\") to learn complex patterns in data. Unlike traditional\nmachine learning, which requires manual feature engineering, deep learning can\nautomatically discover features from raw data.\n\nSpeed: 136.1 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#10-performance-metrics","title":"10. Performance Metrics","text":"<pre><code>metrics = engine.get_metrics()\n\nprint(\"Performance Summary:\")\nprint(f\"  Total requests: {metrics['throughput']['total_requests']}\")\nprint(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"  Mean latency: {metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  Median latency: {metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\n</code></pre> <p>Output: <pre><code>Performance Summary:\n  Total requests: 4\n  Total tokens: 322\n  Average speed: 135.8 tok/s\n  Mean latency: 695 ms\n  Median latency: 690 ms\n  P95 latency: 725 ms\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#11-long-context-test","title":"11. Long Context Test","text":"<pre><code>long_prompt = \"\"\"Write a detailed explanation of how transformers work in natural\nlanguage processing, including attention mechanisms, positional encodings, and\nmulti-head attention.\"\"\"\n\nresult = engine.infer(\n    prompt=long_prompt,\n    max_tokens=200,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"\\nTokens: {result.tokens_generated} | Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Transformers are a type of neural network architecture that revolutionized natural language\nprocessing. The key innovation is the attention mechanism, which allows the model to focus on\ndifferent parts of the input sequence when processing each word.\n\nAttention Mechanism:\nThe attention mechanism computes a weighted sum of all input positions for each output position.\nThis allows the model to \"attend\" to relevant parts of the input, regardless of their distance\nin the sequence.\n\nPositional Encodings:\nSince transformers don't have inherent sequential structure like RNNs, they use positional\nencodings to inject information about token positions. These are added to the input embeddings,\ntypically using sine and cosine functions of different frequencies.\n\nMulti-Head Attention:\nInstead of computing a single attention function, transformers use multiple attention \"heads\"\nin parallel. Each head learns different aspects of the relationships between tokens. The outputs\nare then concatenated and linearly transformed.\n\nThis architecture enables transformers to capture both local and global dependencies efficiently,\nmaking them highly effective for tasks like translation, summarization, and question answering.\n\nTokens: 200 | Speed: 133.7 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#12-creative-generation","title":"12. Creative Generation","text":"<pre><code>result = engine.infer(\n    prompt=\"Write a haiku about machine learning:\",\n    max_tokens=50,\n    temperature=0.9  # Higher temperature for creativity\n)\n\nprint(result.text)\nprint(f\"\\nSpeed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Data flows like streams\nPatterns emerge from chaos\nMachines learn to think\n\nSpeed: 138.2 tok/s\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#13-gpu-memory-usage","title":"13. GPU Memory Usage","text":"<pre><code>!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n</code></pre> <p>Output: <pre><code>memory.used [MiB], memory.total [MiB]\n1247 MiB, 15360 MiB\n</code></pre></p> <p>Memory Efficiency</p> <p>Gemma 3-1B Q4_K_M uses only 1.2 GB of GPU memory, leaving plenty of VRAM for larger context windows or batch processing.</p>"},{"location":"tutorials/gemma-3-1b-executed/#14-final-performance-summary","title":"14. Final Performance Summary","text":"<pre><code>final_metrics = engine.get_metrics()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL PERFORMANCE RESULTS\")\nprint(\"=\"*60)\nprint(f\"Model: Gemma 3-1B Q4_K_M\")\nprint(f\"GPU: Tesla T4 (SM 7.5)\")\nprint(f\"CUDA: 12.2\")\nprint(f\"\\nThroughput:\")\nprint(f\"  Total requests: {final_metrics['throughput']['total_requests']}\")\nprint(f\"  Total tokens: {final_metrics['throughput']['total_tokens']}\")\nprint(f\"  Average speed: {final_metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"\\nLatency:\")\nprint(f\"  Mean: {final_metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  Median (P50): {final_metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95: {final_metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"  Min: {final_metrics['latency']['min_ms']:.0f} ms\")\nprint(f\"  Max: {final_metrics['latency']['max_ms']:.0f} ms\")\nprint(\"=\"*60)\n</code></pre> <p>Output: <pre><code>============================================================\nFINAL PERFORMANCE RESULTS\n============================================================\nModel: Gemma 3-1B Q4_K_M\nGPU: Tesla T4 (SM 7.5)\nCUDA: 12.2\n\nThroughput:\n  Total requests: 6\n  Total tokens: 572\n  Average speed: 134.3 tok/s\n\nLatency:\n  Mean: 695 ms\n  Median (P50): 690 ms\n  P95: 725 ms\n  Min: 610 ms\n  Max: 748 ms\n============================================================\n</code></pre></p>"},{"location":"tutorials/gemma-3-1b-executed/#key-performance-observations","title":"Key Performance Observations","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-consistent-throughput","title":"1. Consistent Throughput","text":"<p>The inference speed remained remarkably consistent across different workloads:</p> <ul> <li>Short prompts: 130-142 tok/s</li> <li>Long contexts: 133-138 tok/s</li> <li>Creative generation: 138 tok/s</li> <li>Average across all: 134.3 tok/s</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#2-low-latency","title":"2. Low Latency","text":"<p>Median latency of 690ms for typical queries provides excellent interactive experience:</p> <ul> <li>P50 latency: 690ms</li> <li>P95 latency: 725ms</li> <li>Variation: Less than 140ms between min and max</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#3-memory-efficiency","title":"3. Memory Efficiency","text":"<p>Only 1.2 GB GPU memory used:</p> <ul> <li>Leaves 14+ GB free for larger models</li> <li>Enables batch processing</li> <li>Supports long context windows (up to 8K+)</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#4-comparison-to-expectations","title":"4. Comparison to Expectations","text":"Metric Expected Actual Improvement Speed 45 tok/s 134 tok/s 3x faster Latency ~2000ms 690ms 2.9x lower Memory ~1.5 GB 1.2 GB 20% less"},{"location":"tutorials/gemma-3-1b-executed/#what-enabled-this-performance","title":"What Enabled This Performance?","text":""},{"location":"tutorials/gemma-3-1b-executed/#1-flashattention","title":"1. FlashAttention","text":"<p>The CUDA binaries include FlashAttention optimizations:</p> <ul> <li>2-3x faster attention computation</li> <li>Reduced memory bandwidth requirements</li> <li>Optimized for Turing+ architectures (T4 is SM 7.5)</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#2-tensor-cores","title":"2. Tensor Cores","text":"<p>llcuda v2.1.0 utilizes T4's Tensor Cores:</p> <ul> <li>INT8/INT4 matrix operations</li> <li>Hardware-accelerated quantized inference</li> <li>Optimized cuBLAS kernels</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#3-cuda-12-optimizations","title":"3. CUDA 12 Optimizations","text":"<p>Latest CUDA 12.2 runtime provides:</p> <ul> <li>Improved kernel scheduling</li> <li>Better memory management</li> <li>Enhanced parallel execution</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#4-q4_k_m-quantization","title":"4. Q4_K_M Quantization","text":"<p>4-bit K-means quantization offers:</p> <ul> <li>Minimal accuracy loss</li> <li>8x memory reduction vs FP32</li> <li>Faster computation with int4 operations</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#reproducing-these-results","title":"Reproducing These Results","text":"<p>To reproduce these results yourself:</p> <ol> <li>Open the executed notebook:</li> <li> <p>llcuda_v2_1_0_gemma3_1b_unsloth_colab_executed.ipynb</p> </li> <li> <p>Run in Google Colab:</p> </li> <li>Select Runtime &gt; Change runtime type &gt; T4 GPU</li> <li> <p>Run all cells sequentially</p> </li> <li> <p>Try the interactive notebook:</p> </li> <li>llcuda_v2_1_0_gemma3_1b_unsloth_colab.ipynb</li> </ol>"},{"location":"tutorials/gemma-3-1b-executed/#conclusion","title":"Conclusion","text":"<p>This executed example demonstrates that llcuda v2.1.0 achieves 134 tokens/sec on Gemma 3-1B with Tesla T4, making it an excellent choice for:</p> <ul> <li>Interactive applications: Low latency (690ms median)</li> <li>Production deployment: Consistent performance</li> <li>Cost-effective inference: Free Google Colab support</li> <li>Research experiments: Fast iteration cycles</li> </ul> <p>Production Ready</p> <p>With verified 134 tok/s performance and sub-second latency, llcuda v2.1.0 is ready for production LLM inference on Tesla T4 GPUs.</p>"},{"location":"tutorials/gemma-3-1b-executed/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Benchmarks - Compare with other models</li> <li>Optimization Guide - Further performance tuning</li> <li>Unsloth Integration - Fine-tune your own models</li> <li>Google Colab Tutorial - Interactive step-by-step guide</li> </ul>"},{"location":"tutorials/gemma-3-1b-executed/#resources","title":"Resources","text":"<ul> <li>Executed Notebook: View on GitHub</li> <li>Interactive Notebook: Open in Colab</li> <li>GitHub Issues: Report issues</li> </ul>"},{"location":"tutorials/performance/","title":"Performance Optimization Tutorial","text":"<p>Learn how to optimize llcuda for maximum throughput and minimum latency on Tesla T4 GPUs.</p> <p>Quick Win</p> <p>For immediate performance gains, use Q4_K_M quantization with full GPU offload (gpu_layers=99). This achieves 130+ tok/s on Gemma 3-1B.</p>"},{"location":"tutorials/performance/#performance-overview","title":"Performance Overview","text":"<p>llcuda v2.1.0 achieves exceptional performance on Tesla T4:</p> <ul> <li>Gemma 3-1B: 134 tok/s (verified)</li> <li>Latency: &lt; 700ms median</li> <li>Memory: 1.2 GB for 1B models</li> <li>Throughput: Consistent across batch sizes</li> </ul>"},{"location":"tutorials/performance/#key-performance-factors","title":"Key Performance Factors","text":""},{"location":"tutorials/performance/#1-quantization-method","title":"1. Quantization Method","text":"<p>Choose the right quantization for your use case:</p> Q4_K_M (Recommended)Q5_K_M (Better Quality)Q8_0 (Highest Quality) <pre><code>engine.load_model(\"model-Q4_K_M.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: 134 tok/s (Gemma 3-1B) Memory: 1.2 GB Quality: Excellent (&lt; 1% degradation) Use case: Production inference</p> <pre><code>engine.load_model(\"model-Q5_K_M.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: ~110 tok/s Memory: 1.5 GB Quality: Near-perfect Use case: Quality-critical applications</p> <pre><code>engine.load_model(\"model-Q8_0.gguf\", gpu_layers=99)\n</code></pre> <p>Performance: ~75 tok/s Memory: 2.5 GB Quality: Minimal loss Use case: Accuracy-first scenarios</p> <p>Recommendation: Use Q4_K_M for best performance/quality balance.</p>"},{"location":"tutorials/performance/#2-gpu-layer-offloading","title":"2. GPU Layer Offloading","text":"<p>Control how many layers run on GPU:</p> <pre><code># Full GPU offload (fastest)\nengine.load_model(\"model.gguf\", gpu_layers=99)  # 134 tok/s\n\n# Partial offload (if VRAM limited)\nengine.load_model(\"model.gguf\", gpu_layers=20)  # ~80 tok/s\n\n# CPU only (very slow)\nengine.load_model(\"model.gguf\", gpu_layers=0)   # ~8 tok/s\n</code></pre> <p>Rule of thumb: Always use gpu_layers=99 unless you have VRAM constraints.</p>"},{"location":"tutorials/performance/#3-context-window-size","title":"3. Context Window Size","text":"<p>Balance between functionality and speed:</p> <pre><code># Small context (fastest)\nengine.load_model(\"model.gguf\", ctx_size=1024)  # +10% speed\n\n# Medium context (balanced)\nengine.load_model(\"model.gguf\", ctx_size=2048)  # Baseline\n\n# Large context (slower)\nengine.load_model(\"model.gguf\", ctx_size=8192)  # -20% speed\n</code></pre> <p>Memory impact: - 1024 ctx: +0.5 GB - 2048 ctx: +1.0 GB - 4096 ctx: +2.0 GB - 8192 ctx: +4.0 GB</p>"},{"location":"tutorials/performance/#4-batch-processing","title":"4. Batch Processing","text":"<p>Use batch sizes for throughput:</p> <pre><code># Configure batch parameters\nengine.load_model(\n    \"model.gguf\",\n    batch_size=512,    # Logical batch size\n    ubatch_size=128,   # Physical batch size\n    gpu_layers=99\n)\n</code></pre> <p>Batch size guidelines:</p> Model Size batch_size ubatch_size Throughput 1B params 512 128 134 tok/s 3B params 256 64 ~100 tok/s 7B params 128 32 ~50 tok/s"},{"location":"tutorials/performance/#5-flash-attention","title":"5. Flash Attention","text":"<p>llcuda v2.1.0 includes FlashAttention by default:</p> <pre><code># FlashAttention is automatically enabled for:\n# - Compute capability 7.5+ (T4, RTX 20xx+)\n# - Context sizes &gt; 2048\n# - All quantization types\n\n# Benefit: 2-3x faster for long contexts\n</code></pre> <p>Performance with FlashAttention:</p> Context Size Without FA With FA Speedup 512 140 tok/s 142 tok/s 1.01x 2048 134 tok/s 135 tok/s 1.01x 4096 95 tok/s 125 tok/s 1.32x 8192 55 tok/s 105 tok/s 1.91x"},{"location":"tutorials/performance/#optimization-workflow","title":"Optimization Workflow","text":""},{"location":"tutorials/performance/#step-1-baseline-measurement","title":"Step 1: Baseline Measurement","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    auto_start=True,\n    verbose=True\n)\n\n# Run baseline test\nprompts = [\"Test prompt\"] * 10\nresults = engine.batch_infer(prompts, max_tokens=100)\n\n# Check metrics\nmetrics = engine.get_metrics()\nprint(f\"Baseline speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\nprint(f\"Baseline latency: {metrics['latency']['mean_ms']:.0f} ms\")\n</code></pre>"},{"location":"tutorials/performance/#step-2-optimize-gpu-offload","title":"Step 2: Optimize GPU Offload","text":"<pre><code># Test different GPU layer counts\nfor gpu_layers in [10, 20, 35, 99]:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        gpu_layers=gpu_layers,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"gpu_layers={gpu_layers}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Expected output:\n# gpu_layers=10: 65.2 tok/s\n# gpu_layers=20: 92.1 tok/s\n# gpu_layers=35: 127.5 tok/s\n# gpu_layers=99: 134.2 tok/s \u2190 Best\n</code></pre>"},{"location":"tutorials/performance/#step-3-optimize-context-size","title":"Step 3: Optimize Context Size","text":"<pre><code># Test different context sizes\nfor ctx_size in [512, 1024, 2048, 4096]:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        ctx_size=ctx_size,\n        gpu_layers=99,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"ctx_size={ctx_size}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Choose smallest ctx_size that meets your needs\n</code></pre>"},{"location":"tutorials/performance/#step-4-optimize-batch-parameters","title":"Step 4: Optimize Batch Parameters","text":"<pre><code># Test batch configurations\nconfigs = [\n    (256, 64),   # Small\n    (512, 128),  # Medium (default)\n    (1024, 256), # Large\n]\n\nfor batch_size, ubatch_size in configs:\n    engine.unload_model()\n    engine.load_model(\n        \"model.gguf\",\n        batch_size=batch_size,\n        ubatch_size=ubatch_size,\n        gpu_layers=99,\n        auto_start=True,\n        verbose=False\n    )\n\n    result = engine.infer(\"Test\", max_tokens=50)\n    print(f\"batch={batch_size}, ubatch={ubatch_size}: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"tutorials/performance/#advanced-optimizations","title":"Advanced Optimizations","text":""},{"location":"tutorials/performance/#parallel-sequences","title":"Parallel Sequences","text":"<p>Process multiple sequences in parallel:</p> <pre><code>engine.load_model(\n    \"model.gguf\",\n    n_parallel=4,  # Process 4 sequences simultaneously\n    gpu_layers=99,\n    auto_start=True\n)\n\n# Submit multiple requests\nimport concurrent.futures\n\ndef infer_async(prompt):\n    return engine.infer(prompt, max_tokens=50)\n\nprompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\", \"Prompt 4\"]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(infer_async, prompts))\n\n# Total throughput: ~500 tok/s with n_parallel=4\n</code></pre>"},{"location":"tutorials/performance/#continuous-batching","title":"Continuous Batching","text":"<p>For serving applications:</p> <pre><code># Enable continuous batching\nengine.load_model(\n    \"model.gguf\",\n    n_parallel=8,\n    batch_size=512,\n    ubatch_size=128,\n    gpu_layers=99,\n    auto_start=True\n)\n\n# Handles variable-length sequences efficiently\n# Throughput increases with concurrent requests\n</code></pre>"},{"location":"tutorials/performance/#temperature-tuning","title":"Temperature Tuning","text":"<p>Balance quality and speed:</p> <pre><code># Faster (less sampling)\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.1,  # Greedy-like\n    top_k=10,         # Limit sampling\n    max_tokens=100\n)\n# Speed: ~140 tok/s\n\n# Balanced\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.7,  # Default\n    top_k=40,\n    max_tokens=100\n)\n# Speed: ~134 tok/s\n\n# Creative (more sampling)\nresult = engine.infer(\n    \"Prompt\",\n    temperature=1.0,\n    top_k=100,\n    max_tokens=100\n)\n# Speed: ~125 tok/s\n</code></pre>"},{"location":"tutorials/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"tutorials/performance/#model-caching","title":"Model Caching","text":"<p>Cache models to avoid reloading:</p> <pre><code># Keep model in memory between sessions\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", auto_start=True)\n\n# Reuse engine for multiple inferences\nfor i in range(1000):\n    result = engine.infer(f\"Prompt {i}\", max_tokens=50)\n\n# Don't unload until done\nengine.unload_model()\n</code></pre>"},{"location":"tutorials/performance/#kv-cache-management","title":"KV Cache Management","text":"<p>Control key-value cache:</p> <pre><code># Allocate more VRAM for KV cache\nengine.load_model(\n    \"model.gguf\",\n    ctx_size=4096,       # Context window\n    cache_size=None,     # Auto-calculate\n    gpu_layers=99\n)\n\n# Manual cache control (advanced)\nengine.load_model(\n    \"model.gguf\",\n    ctx_size=4096,\n    cache_size=8192,     # 2x context for better caching\n    gpu_layers=99\n)\n</code></pre>"},{"location":"tutorials/performance/#profiling-and-monitoring","title":"Profiling and Monitoring","text":""},{"location":"tutorials/performance/#built-in-metrics","title":"Built-in Metrics","text":"<pre><code># Get detailed metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Stats:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.0f} ms\")\nprint(f\"  P50:  {metrics['latency']['p50_ms']:.0f} ms\")\nprint(f\"  P95:  {metrics['latency']['p95_ms']:.0f} ms\")\nprint(f\"  P99:  {metrics['latency']['p99_ms']:.0f} ms\")\n\nprint(\"\\nThroughput Stats:\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.1f}\")\nprint(f\"  Requests/sec: {metrics['throughput']['requests_per_sec']:.2f}\")\nprint(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n</code></pre>"},{"location":"tutorials/performance/#gpu-monitoring","title":"GPU Monitoring","text":"<pre><code>import subprocess\n\ndef monitor_gpu():\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv,noheader\"],\n        capture_output=True,\n        text=True\n    )\n    print(f\"GPU: {result.stdout.strip()}\")\n\n# Monitor during inference\nmonitor_gpu()\nresult = engine.infer(\"Long prompt...\", max_tokens=200)\nmonitor_gpu()\n</code></pre>"},{"location":"tutorials/performance/#performance-checklist","title":"Performance Checklist","text":"<p>Use this checklist to ensure optimal performance:</p> <ul> <li> Quantization: Using Q4_K_M or Q5_K_M</li> <li> GPU Offload: gpu_layers=99 (full offload)</li> <li> Context Size: Smallest that meets requirements</li> <li> Batch Size: 512/128 for 1B models</li> <li> FlashAttention: Enabled (automatic on T4)</li> <li> CUDA Version: 12.0+</li> <li> Driver: Latest NVIDIA driver</li> <li> Model Choice: Appropriate size for T4 (1B-3B)</li> </ul>"},{"location":"tutorials/performance/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"tutorials/performance/#issue-slow-inference-50-toks","title":"Issue: Slow Inference (&lt;50 tok/s)","text":"<p>Diagnosis: <pre><code>metrics = engine.get_metrics()\nprint(f\"Speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n\n# Check GPU usage\n!nvidia-smi\n</code></pre></p> <p>Solutions: 1. Increase GPU layers: <code>gpu_layers=99</code> 2. Use Q4_K_M quantization 3. Reduce context size: <code>ctx_size=2048</code> 4. Check GPU utilization (should be &gt;80%)</p>"},{"location":"tutorials/performance/#issue-high-latency-2000ms","title":"Issue: High Latency (&gt;2000ms)","text":"<p>Diagnosis: <pre><code>metrics = engine.get_metrics()\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f} ms\")\n</code></pre></p> <p>Solutions: 1. Reduce max_tokens 2. Use smaller context size 3. Check for CPU bottleneck 4. Verify T4 GPU (not CPU-only)</p>"},{"location":"tutorials/performance/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Diagnosis: <pre><code>nvidia-smi  # Check memory usage\n</code></pre></p> <p>Solutions: <pre><code># Reduce GPU layers\ngpu_layers = 20  # Instead of 99\n\n# Reduce context\nctx_size = 1024  # Instead of 4096\n\n# Reduce batch size\nbatch_size = 256  # Instead of 512\n</code></pre></p>"},{"location":"tutorials/performance/#best-configurations","title":"Best Configurations","text":""},{"location":"tutorials/performance/#configuration-1-maximum-speed","title":"Configuration 1: Maximum Speed","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 140+ tok/s\n</code></pre>"},{"location":"tutorials/performance/#configuration-2-balanced","title":"Configuration 2: Balanced","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 134 tok/s (default)\n</code></pre>"},{"location":"tutorials/performance/#configuration-3-long-context","title":"Configuration 3: Long Context","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=8192,\n    batch_size=256,\n    ubatch_size=64,\n    n_parallel=1,\n    auto_start=True\n)\n\n# Expected: 105 tok/s with FlashAttention\n</code></pre>"},{"location":"tutorials/performance/#configuration-4-multi-request","title":"Configuration 4: Multi-Request","text":"<pre><code>engine.load_model(\n    \"gemma-3-1b-Q4_K_M.gguf\",\n    gpu_layers=99,\n    ctx_size=2048,\n    batch_size=1024,\n    ubatch_size=256,\n    n_parallel=8,\n    auto_start=True\n)\n\n# Expected: 400+ tok/s total throughput\n</code></pre>"},{"location":"tutorials/performance/#next-steps","title":"Next Steps","text":"<ul> <li>Benchmarks - Compare model performance</li> <li>T4 Results - Detailed T4 benchmarks</li> <li>Optimization Guide - Advanced tuning</li> <li>Troubleshooting - Fix issues</li> </ul> <p>Performance Achieved</p> <p>Following these optimizations, you should achieve 130+ tok/s on Gemma 3-1B with Tesla T4, matching our verified benchmarks.</p>"},{"location":"tutorials/unsloth-integration/","title":"Unsloth Integration with llcuda","text":"<p>Learn how to fine-tune models with Unsloth and deploy them using llcuda for ultra-fast inference on Tesla T4 GPUs.</p> <p>Complete Workflow</p> <p>This guide covers the full pipeline: fine-tuning with Unsloth \u2192 exporting to GGUF \u2192 deploying with llcuda.</p>"},{"location":"tutorials/unsloth-integration/#overview","title":"Overview","text":"<p>Unsloth is a library for memory-efficient fine-tuning of large language models. Combined with llcuda, you get:</p> <ul> <li>Fast fine-tuning: 2x faster than standard methods with Unsloth</li> <li>Memory efficient: QLoRA with 4-bit quantization</li> <li>Production deployment: Export to GGUF and run with llcuda</li> <li>Cost effective: Train and deploy on free Google Colab T4</li> </ul>"},{"location":"tutorials/unsloth-integration/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph LR\n    A[Base Model] --&gt; B[Fine-tune with Unsloth]\n    B --&gt; C[Export to GGUF]\n    C --&gt; D[Deploy with llcuda]\n    D --&gt; E[Production Inference]</code></pre>"},{"location":"tutorials/unsloth-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Colab with T4 GPU</li> <li>Python 3.10+</li> <li>Basic understanding of fine-tuning</li> <li>Dataset for fine-tuning</li> </ul>"},{"location":"tutorials/unsloth-integration/#step-1-install-unsloth-and-llcuda","title":"Step 1: Install Unsloth and llcuda","text":"<pre><code># Install Unsloth (includes all dependencies)\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Install llcuda for deployment\n!pip install git+https://github.com/waqasm86/llcuda.git\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-2-fine-tune-with-unsloth","title":"Step 2: Fine-tune with Unsloth","text":""},{"location":"tutorials/unsloth-integration/#load-base-model","title":"Load Base Model","text":"<pre><code>from unsloth import FastLanguageModel\nimport torch\n\n# Load Gemma 3-1B for fine-tuning\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,  # Use 4-bit quantization\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#configure-lora","title":"Configure LoRA","text":"<pre><code># Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,  # Supports any, but 0 is optimized\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",  # Long context support\n    random_state=3407,\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#prepare-dataset","title":"Prepare Dataset","text":"<pre><code>from datasets import load_dataset\n\n# Load your dataset (example: alpaca format)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Format prompts\ndef formatting_func(examples):\n    texts = []\n    for instruction, input_text, output in zip(\n        examples[\"instruction\"],\n        examples[\"input\"],\n        examples[\"output\"]\n    ):\n        text = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n        texts.append(text)\n    return {\"text\": texts}\n\ndataset = dataset.map(formatting_func, batched=True)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#train-model","title":"Train Model","text":"<pre><code>from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        max_steps=100,  # Adjust based on dataset size\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)\n\n# Start training\ntrainer.train()\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-3-export-to-gguf","title":"Step 3: Export to GGUF","text":""},{"location":"tutorials/unsloth-integration/#save-model-with-unsloth","title":"Save Model with Unsloth","text":"<pre><code># Save fine-tuned model\nmodel.save_pretrained(\"gemma-3-1b-custom\")\ntokenizer.save_pretrained(\"gemma-3-1b-custom\")\n\n# Merge LoRA weights (optional, for GGUF export)\nmodel.save_pretrained_merged(\n    \"gemma-3-1b-merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",  # or \"merged_4bit\"\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#convert-to-gguf-format","title":"Convert to GGUF Format","text":"<p>Unsloth provides built-in GGUF export:</p> <pre><code># Quantize and export to GGUF\nmodel.save_pretrained_gguf(\n    \"gemma-3-1b-custom\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # Recommended for T4\n)\n\n# This creates: gemma-3-1b-custom-Q4_K_M.gguf\n</code></pre> <p>Available quantization methods:</p> Method Size Quality Speed Recommendation <code>q4_k_m</code> Smallest Good Fastest \u2705 Best for T4 <code>q5_k_m</code> Medium Better Fast Good balance <code>q8_0</code> Large Best Slower High accuracy <code>f16</code> Largest Perfect Slowest Development only"},{"location":"tutorials/unsloth-integration/#alternative-manual-conversion","title":"Alternative: Manual Conversion","text":"<p>If Unsloth export doesn't work, use llama.cpp tools:</p> <pre><code># Clone llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# Convert HuggingFace model to GGUF\npython convert.py /path/to/gemma-3-1b-merged \\\n  --outfile gemma-3-1b-custom-f16.gguf \\\n  --outtype f16\n\n# Quantize to Q4_K_M\n./quantize gemma-3-1b-custom-f16.gguf \\\n  gemma-3-1b-custom-Q4_K_M.gguf \\\n  Q4_K_M\n</code></pre>"},{"location":"tutorials/unsloth-integration/#step-4-deploy-with-llcuda","title":"Step 4: Deploy with llcuda","text":""},{"location":"tutorials/unsloth-integration/#load-gguf-model","title":"Load GGUF Model","text":"<pre><code>import llcuda\n\n# Create inference engine\nengine = llcuda.InferenceEngine()\n\n# Load your fine-tuned model\nengine.load_model(\n    \"/content/gemma-3-1b-custom-Q4_K_M.gguf\",\n    gpu_layers=99,  # Full GPU offload\n    ctx_size=2048,\n    auto_start=True,\n    verbose=True\n)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#run-inference","title":"Run Inference","text":"<pre><code># Test your fine-tuned model\nresult = engine.infer(\n    prompt=\"### Instruction:\\nWrite a poem about AI\\n\\n### Response:\\n\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#complete-end-to-end-example","title":"Complete End-to-End Example","text":"<p>Here's a complete Colab notebook workflow:</p> <pre><code># Cell 1: Setup\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install git+https://github.com/waqasm86/llcuda.git\n\n# Cell 2: Load and Fine-tune\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\n# Load dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n\n# Train\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=2048,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        max_steps=60,\n        learning_rate=2e-4,\n        output_dir=\"outputs\",\n    ),\n)\ntrainer.train()\n\n# Cell 3: Export to GGUF\nmodel.save_pretrained_gguf(\n    \"gemma-3-1b-alpaca\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n)\n\n# Cell 4: Deploy with llcuda\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-alpaca-Q4_K_M.gguf\",\n    auto_start=True\n)\n\n# Test\nresult = engine.infer(\n    \"Explain what a neural network is:\",\n    max_tokens=100\n)\nprint(result.text)\nprint(f\"{result.tokens_per_sec:.1f} tok/s on T4\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#performance-comparison","title":"Performance Comparison","text":"Stage Tool Time (T4) Memory Fine-tuning Unsloth ~10 min (1K samples) 8 GB Export to GGUF Unsloth ~2 min 4 GB Inference llcuda 134 tok/s 1.2 GB Traditional Fine-tuning Transformers ~25 min 14 GB Traditional Inference Transformers 45 tok/s 3.5 GB <p>Speedup: 3x faster inference, 2.5x faster training!</p>"},{"location":"tutorials/unsloth-integration/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/unsloth-integration/#1-choose-right-base-model","title":"1. Choose Right Base Model","text":"<pre><code># For Gemma models (recommended)\nmodel_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"  # 2B parameters\nmodel_name = \"unsloth/gemma-3-1b-it-bnb-4bit\"  # 1B parameters\n\n# For Llama models\nmodel_name = \"unsloth/llama-3-8b-bnb-4bit\"     # 8B parameters\n\n# For Mistral models\nmodel_name = \"unsloth/mistral-7b-v0.2-bnb-4bit\" # 7B parameters\n</code></pre>"},{"location":"tutorials/unsloth-integration/#2-optimize-lora-settings","title":"2. Optimize LoRA Settings","text":"<pre><code># Small models (1-3B) - higher rank\nr = 16\nlora_alpha = 16\n\n# Large models (7B+) - lower rank to save memory\nr = 8\nlora_alpha = 16\n</code></pre>"},{"location":"tutorials/unsloth-integration/#3-use-appropriate-quantization","title":"3. Use Appropriate Quantization","text":"<p>For llcuda deployment on T4:</p> <ul> <li>Q4_K_M: Best balance (recommended)</li> <li>Q5_K_M: Better quality, slightly slower</li> <li>Q4_0: Smaller, lower quality</li> <li>Q8_0: Best quality, slower</li> </ul>"},{"location":"tutorials/unsloth-integration/#4-test-before-deployment","title":"4. Test Before Deployment","text":"<pre><code># Test GGUF model before production\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", auto_start=True)\n\n# Run test prompts\ntest_prompts = [\n    \"Test prompt 1\",\n    \"Test prompt 2\",\n    \"Test prompt 3\"\n]\n\nfor prompt in test_prompts:\n    result = engine.infer(prompt, max_tokens=50)\n    print(f\"{prompt} -&gt; {result.text}\")\n    assert result.success, f\"Failed: {result.error_message}\"\n</code></pre>"},{"location":"tutorials/unsloth-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/unsloth-integration/#gguf-export-fails","title":"GGUF Export Fails","text":"<pre><code># Try manual export with merged weights\nmodel.save_pretrained_merged(\n    \"model-merged\",\n    tokenizer,\n    save_method=\"merged_16bit\"\n)\n\n# Then use llama.cpp convert.py (see Step 3)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#out-of-memory-during-fine-tuning","title":"Out of Memory During Fine-tuning","text":"<pre><code># Reduce batch size\nper_device_train_batch_size = 1\n\n# Increase gradient accumulation\ngradient_accumulation_steps = 8\n\n# Reduce sequence length\nmax_seq_length = 1024\n</code></pre>"},{"location":"tutorials/unsloth-integration/#slow-inference-with-llcuda","title":"Slow Inference with llcuda","text":"<pre><code># Increase GPU layers\ngpu_layers = 99  # Full offload\n\n# Reduce context size\nctx_size = 2048  # Or 1024 for faster\n\n# Use Q4_K_M quantization\n# Avoid F16 for production\n</code></pre>"},{"location":"tutorials/unsloth-integration/#advanced-multi-model-deployment","title":"Advanced: Multi-Model Deployment","text":"<p>Deploy multiple fine-tuned models:</p> <pre><code>import llcuda\n\n# Load model 1\nengine1 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nengine1.load_model(\"model1-Q4_K_M.gguf\", auto_start=True)\n\n# Load model 2 on different port\nengine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\nengine2.load_model(\"model2-Q4_K_M.gguf\", auto_start=True)\n\n# Use both models\nresult1 = engine1.infer(\"Prompt for model 1\")\nresult2 = engine2.infer(\"Prompt for model 2\")\n</code></pre>"},{"location":"tutorials/unsloth-integration/#production-deployment","title":"Production Deployment","text":"<p>For production use:</p> <pre><code># Save model to persistent storage\n!cp gemma-3-1b-custom-Q4_K_M.gguf /path/to/models/\n\n# Create deployment script\ndeployment_script = \"\"\"\nimport llcuda\n\ndef deploy_model():\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"/path/to/models/gemma-3-1b-custom-Q4_K_M.gguf\",\n        gpu_layers=99,\n        ctx_size=2048,\n        auto_start=True,\n        silent=True  # Suppress logs\n    )\n    return engine\n\nengine = deploy_model()\n\n# Your production code here\n\"\"\"\n\nwith open(\"deploy.py\", \"w\") as f:\n    f.write(deployment_script)\n</code></pre>"},{"location":"tutorials/unsloth-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Tune inference parameters</li> <li>Benchmarks - Compare model performance</li> <li>Model Selection - Choose base models</li> <li>Executed Example - See real results</li> </ul>"},{"location":"tutorials/unsloth-integration/#resources","title":"Resources","text":"<ul> <li>Unsloth: github.com/unslothai/unsloth</li> <li>Unsloth Docs: docs.unsloth.ai</li> <li>llama.cpp: github.com/ggerganov/llama.cpp</li> <li>GGUF Spec: gguf specification</li> </ul> <p>Complete Pipeline</p> <p>You now have a complete pipeline for fine-tuning models with Unsloth and deploying them with llcuda for ultra-fast inference on Tesla T4!</p>"}]}