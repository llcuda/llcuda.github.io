{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"llcuda v2.2.0: CUDA12 Inference Backend for Unsloth","text":"<p>CUDA12-first inference backend for Unsloth with Graphistry network visualization on Kaggle dual Tesla T4 GPUs. Fine-tune with Unsloth \u2192 Export to GGUF \u2192 Deploy on Kaggle \u2192 Visualize with Graphistry.</p>"},{"location":"#what-is-llcuda-v220","title":"What is llcuda v2.2.0?","text":"<p>llcuda is a CUDA 12 inference backend specifically designed for deploying Unsloth-fine-tuned models on Kaggle's dual Tesla T4 GPUs (30GB total VRAM). It provides:</p>"},{"location":"#material-gpu-dual-t4-architecture","title":":material-gpu: Dual T4 Architecture","text":"<p>Run on Kaggle's 2\u00d7 Tesla T4 GPUs (15GB each)</p> <ul> <li>Native CUDA tensor-split for multi-GPU</li> <li>Support for 70B models with IQ3_XS quantization</li> <li>FlashAttention for 2-3x faster inference</li> <li>961MB pre-built CUDA 12.5 binaries</li> </ul>"},{"location":"#split-gpu-design","title":"Split-GPU Design","text":"<p>Unique architecture: LLM on GPU 0 + Graphistry on GPU 1</p> <ul> <li>GPU 0: llama.cpp server for LLM inference</li> <li>GPU 1: RAPIDS cuGraph + Graphistry visualization</li> <li>Extract knowledge graphs from LLM outputs</li> <li>Visualize millions of nodes and edges</li> </ul>"},{"location":"#unsloth-integration","title":"Unsloth Integration","text":"<p>Seamless workflow from training to deployment</p> <ul> <li>Fine-tune with Unsloth (2x faster training)</li> <li>Export to GGUF format with <code>save_pretrained_gguf()</code></li> <li>Deploy with llcuda on Kaggle</li> <li>Complete end-to-end pipeline</li> </ul>"},{"location":"#production-ready","title":"Production Ready","text":"<p>Built for Kaggle production workloads</p> <ul> <li>OpenAI-compatible API via llama-server</li> <li>29 quantization formats (K-quants, I-quants)</li> <li>NCCL support for PyTorch distributed</li> <li>Auto-download binaries from GitHub Releases</li> </ul>"},{"location":"#core-architecture","title":"Core Architecture","text":"<p>llcuda v2.2.0 implements a unique split-GPU architecture for Kaggle's dual T4 environment:</p> <pre><code>%%{init: {'theme':'base', 'themeVariables': {'fontSize':'18px'}}}%%\ngraph TD\n    A[Unsloth Fine-Tuning] --&gt; B[GGUF Export]\n    B --&gt; C[llcuda Deployment]\n    C --&gt; D[GPU 0: LLM Inference]\n    C --&gt; E[GPU 1: Graphistry Visualization]\n    D --&gt; F[Knowledge Extraction]\n    F --&gt; E\n    E --&gt; G[Interactive Graph Visualization]\n\n    style A fill:#9C27B0,stroke:#7B1FA2,stroke-width:3px,color:#fff\n    style B fill:#FF9800,stroke:#F57C00,stroke-width:3px,color:#fff\n    style C fill:#FF5722,stroke:#E64A19,stroke-width:3px,color:#fff\n    style D fill:#4CAF50,stroke:#388E3C,stroke-width:3px,color:#fff\n    style E fill:#2196F3,stroke:#1976D2,stroke-width:3px,color:#fff\n    style F fill:#00BCD4,stroke:#0097A7,stroke-width:3px,color:#fff\n    style G fill:#03A9F4,stroke:#0288D1,stroke-width:3px,color:#fff\n\n    classDef default font-size:16px,padding:15px</code></pre>"},{"location":"#split-gpu-configuration","title":"Split-GPU Configuration","text":"<pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    KAGGLE DUAL T4 SPLIT-GPU ARCHITECTURE                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                            \u2502\n\u2502     GPU 0: Tesla T4 (15GB)                  GPU 1: Tesla T4 (15GB)         \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502     \u2502                        \u2502              \u2502                        \u2502     \u2502\n\u2502     \u2502   llama-server         \u2502              \u2502   RAPIDS cuDF          \u2502     \u2502\n\u2502     \u2502   GGUF Model           \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;  \u2502   cuGraph              \u2502     \u2502\n\u2502     \u2502   LLM Inference        \u2502   extract    \u2502   Graphistry[ai]       \u2502     \u2502\n\u2502     \u2502   ~5-12GB VRAM         \u2502   graphs     \u2502   Network Viz          \u2502     \u2502\n\u2502     \u2502                        \u2502              \u2502                        \u2502     \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                            \u2502\n\u2502     \u2022 tensor-split for multi-GPU          \u2022 Millions of nodes/edges        \u2502\n\u2502     \u2022 FlashAttention enabled              \u2022 GPU-accelerated rendering      \u2502\n\u2502     \u2022 OpenAI API compatible               \u2022 Interactive exploration        \u2502\n\u2502                                                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>"},{"location":"#quick-start-5-minutes","title":"Quick Start (5 Minutes)","text":"<p>Get llcuda v2.2.0 running on Kaggle in just 5 minutes!</p>"},{"location":"#step-1-install-llcuda","title":"Step 1: Install llcuda","text":"<pre><code># On Kaggle notebook\npip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"#step-2-verify-dual-t4-setup","title":"Step 2: Verify Dual T4 Setup","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\n# Check GPU configuration\ngpus = detect_gpus()\nprint(f\"\u2713 Detected {len(gpus)} GPUs\")\nprint_gpu_info()\n\n# Expected output:\n# \u2713 Detected 2 GPUs\n# GPU 0: Tesla T4 (15.0 GB)\n# GPU 1: Tesla T4 (15.0 GB)\n</code></pre>"},{"location":"#step-3-run-basic-inference","title":"Step 3: Run Basic Inference","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Configure for single GPU (GPU 0)\nconfig = ServerConfig(\n    model_path=\"model.gguf\",  # Your GGUF model\n    n_gpu_layers=99,          # Offload all layers to GPU\n    flash_attn=True,          # Enable FlashAttention\n)\n\n# Start server\nserver = ServerManager()\nserver.start_with_config(config)\nserver.wait_until_ready()\n\n# Use OpenAI API\nfrom llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient(\"http://localhost:8080\")\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    max_tokens=200\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Auto-Download Binaries</p> <p>CUDA binaries (961 MB) download automatically from GitHub Releases v2.2.0 on first import. Cached for future runs!</p> <p> Full Installation Guide  Kaggle Setup Tutorial</p>"},{"location":"#key-features-of-v220","title":"Key Features of v2.2.0","text":""},{"location":"#1-multi-gpu-inference-on-kaggle","title":"1. Multi-GPU Inference on Kaggle","text":"<p>Run models up to 70B parameters using both T4 GPUs with native CUDA tensor-split:</p> <pre><code>from llcuda.api.multigpu import kaggle_t4_dual_config\n\n# Optimized dual T4 configuration\nconfig = kaggle_t4_dual_config(model_size_gb=25)  # For 70B IQ3_XS\n\nprint(config.to_cli_args())\n# Output: ['-ngl', '-1', '--split-mode', 'layer', '--tensor-split', '0.5,0.5', '-fa']\n</code></pre> <p>Supported Model Sizes on Dual T4 (30GB VRAM):</p> Model Size Quantization VRAM Required Fits Dual T4? 1-3B Q4_K_M 2-3 GB \u2705 Single T4 7-8B Q4_K_M 5-6 GB \u2705 Single T4 13B Q4_K_M 8-9 GB \u2705 Single T4 32-34B Q4_K_M 20-22 GB \u2705 Dual T4 70B IQ3_XS 25-27 GB \u2705 Dual T4 <p>:material-gpu: Multi-GPU Guide</p>"},{"location":"#2-unsloth-fine-tuning-pipeline","title":"2. Unsloth Fine-Tuning Pipeline","text":"<p>Complete workflow from fine-tuning to deployment:</p> Step 1: Fine-Tune with UnslothStep 2: Export to GGUFStep 3: Deploy with llcuda <pre><code>from unsloth import FastLanguageModel\n\n# Load model for training\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n)\n\n# Train your model...\ntrainer.train()\n</code></pre> <pre><code># Export fine-tuned model to GGUF format\nmodel.save_pretrained_gguf(\n    \"my_finetuned_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"  # Recommended for T4\n)\n\n# Output: my_finetuned_model-Q4_K_M.gguf\n</code></pre> <pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Deploy on Kaggle dual T4\nconfig = ServerConfig(\n    model_path=\"my_finetuned_model-Q4_K_M.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Use both GPUs\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n\n# Now serving at http://localhost:8080 with OpenAI API\n</code></pre> <p> Unsloth Integration Guide</p>"},{"location":"#3-split-gpu-architecture-with-graphistry","title":"3. Split-GPU Architecture with Graphistry","text":"<p>Unique capability: Run LLM inference on GPU 0 while using GPU 1 for RAPIDS/Graphistry visualization</p> <pre><code>from llcuda.graphistry import SplitGPUConfig\nimport graphistry\n\n# Configure split-GPU setup\nconfig = SplitGPUConfig(\n    llm_gpu=0,      # GPU 0 for llama-server\n    graph_gpu=1     # GPU 1 for Graphistry\n)\n\n# Set Graphistry to use GPU 1\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\"\n)\n\n# Now run LLM on GPU 0 and visualize graphs on GPU 1\n</code></pre> <p>Use Cases: - Extract knowledge graphs from LLM outputs \u2192 Visualize with Graphistry - Analyze entity relationships in generated text - Interactive exploration of LLM-generated networks - Real-time graph updates from streaming LLM responses</p> <p> Graphistry Integration Guide</p>"},{"location":"#4-29-gguf-quantization-formats","title":"4. 29 GGUF Quantization Formats","text":"<p>llcuda supports all llama.cpp quantization types:</p> K-Quants (Recommended)I-Quants (Best Compression)Legacy QuantsFull Precision <p>Best quality-to-size ratio with double quantization:</p> <ul> <li>Q4_K_M - 4.8 bpw, best for most models (recommended)</li> <li>Q5_K_M - 5.7 bpw, higher quality</li> <li>Q6_K - 6.6 bpw, near FP16 quality</li> <li>Q8_0 - 8.5 bpw, very high quality</li> </ul> <p>Importance-matrix quantization for 70B models:</p> <ul> <li>IQ3_XS - 3.3 bpw, fits 70B on dual T4</li> <li>IQ4_XS - 4.3 bpw, better quality</li> <li>IQ2_XS - 2.3 bpw, extreme compression</li> <li>IQ1_S - 1.6 bpw, smallest possible</li> </ul> <p>Standard quantization types:</p> <ul> <li>Q4_0 - 4.5 bpw, legacy format</li> <li>Q5_0 - 5.5 bpw, legacy format</li> <li>Q8_0 - 8.5 bpw, high precision</li> </ul> <p>Unquantized formats:</p> <ul> <li>F32 - 32-bit float</li> <li>F16 - 16-bit float</li> <li>BF16 - Brain float 16</li> </ul> <p> GGUF Quantization Guide</p>"},{"location":"#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Real Kaggle dual T4 performance metrics:</p> Model Quantization GPUs Tokens/sec Latency VRAM Usage Gemma 2-2B Q4_K_M 2\u00d7 T4 ~60 tok/s - 4 GB Qwen2.5-7B Q4_K_M 2\u00d7 T4 ~35 tok/s - 10 GB Llama-3.1-70B IQ3_XS 2\u00d7 T4 ~8-12 tok/s - 27 GB Gemma 3-1B Q4_K_M 1\u00d7 T4 ~45 tok/s 690ms 3 GB <p>Performance Optimization</p> <ul> <li>Enable FlashAttention for 2-3x speedup</li> <li>Use tensor-split for models &gt;15GB</li> <li>K-quants provide best quality/speed balance</li> <li>I-quants enable 70B models on 30GB VRAM</li> </ul> <p> Full Benchmarks</p>"},{"location":"#tutorial-notebooks-10-kaggle-notebooks","title":"Tutorial Notebooks (10 Kaggle Notebooks)","text":"<p>Complete tutorial series for Kaggle dual T4 environment:</p> # Notebook Open in Kaggle Description Time 01 Quick Start 5-minute introduction 5 min 02 Server Setup Server configuration &amp; lifecycle 15 min 03 Multi-GPU Dual T4 tensor-split 20 min 04 GGUF Quantization K-quants, I-quants, parsing 20 min 05 Unsloth Integration Fine-tune \u2192 GGUF \u2192 Deploy 30 min 06 Split-GPU + Graphistry LLM + RAPIDS visualization 30 min 07 OpenAI API Drop-in OpenAI SDK 15 min 08 NCCL + PyTorch Distributed PyTorch 25 min 09 Large Models (70B) 70B on dual T4 with IQ3_XS 30 min 10 Complete Workflow End-to-end production 45 min <p> View All Tutorials</p>"},{"location":"#learning-paths","title":"Learning Paths","text":"<p>Choose your path based on experience level:</p> Beginner (1 hour)Intermediate (3 hours)Advanced (2 hours)Unsloth Focus (2 hours) <pre><code>01 Quick Start \u2192 02 Server Setup \u2192 03 Multi-GPU\n</code></pre> <p>Perfect for first-time users. Learn the basics of llcuda on Kaggle.</p> <pre><code>01 \u2192 02 \u2192 03 \u2192 04 \u2192 05 \u2192 06 \u2192 07 \u2192 10\n</code></pre> <p>Complete fundamentals through advanced workflows.</p> <pre><code>01 \u2192 03 \u2192 08 \u2192 09\n</code></pre> <p>Focus on multi-GPU and large model deployment.</p> <pre><code>01 \u2192 04 \u2192 05 \u2192 10\n</code></pre> <p>Fine-tuning and deployment pipeline.</p>"},{"location":"#whats-new-in-v220","title":"What's New in v2.2.0","text":"<p>Major Release Highlights</p> <p>Positioned as Unsloth Inference Backend</p> <ul> <li>llcuda is now the official CUDA12 inference backend for Unsloth</li> <li>Seamless workflow: Unsloth (training) \u2192 llcuda (inference)</li> <li>Complete Kaggle dual T4 build notebook included</li> </ul> <p>New Features:</p> <ul> <li>Kaggle Dual T4 Build - Complete build notebook for reproducible binaries</li> <li>Split-GPU Architecture - LLM on GPU 0 + Graphistry on GPU 1</li> <li>Multi-GPU Clarification - Native CUDA tensor-split (NOT NCCL)</li> <li>961MB Binary Package - Pre-built CUDA 12.5 binaries for T4</li> <li>Graphistry Integration - PyGraphistry for knowledge graph visualization</li> <li>70B Model Support - IQ3_XS quantization for large models</li> <li>FlashAttention All Quants - Enabled for all quantization types</li> </ul> <p>Performance:</p> Platform GPU Model Tokens/sec Kaggle 2\u00d7 T4 Gemma 2-2B Q4_K_M ~60 tok/s Kaggle 2\u00d7 T4 Llama 70B IQ3_XS ~12 tok/s <p> Full Changelog</p>"},{"location":"#technical-architecture","title":"Technical Architecture","text":"<p>llcuda v2.2.0 is built on proven technologies:</p> <ul> <li> <p>llama.cpp Server</p> <ul> <li>Build 7760 (commit 388ce82)</li> <li>OpenAI-compatible API</li> <li>Native CUDA tensor-split</li> <li>FlashAttention support</li> </ul> </li> <li> <p>CUDA 12.5</p> <ul> <li>SM 7.5 (Turing) targeting</li> <li>cuBLAS acceleration</li> <li>Static linking</li> <li>961MB binary package</li> </ul> </li> <li> <p>Python 3.11+</p> <ul> <li>Type-safe APIs</li> <li>Async/await support</li> <li>Modern packaging</li> <li>62KB pip package</li> </ul> </li> <li> <p>RAPIDS + Graphistry</p> <ul> <li>cuDF for GPU DataFrames</li> <li>cuGraph for network analysis</li> <li>PyGraphistry visualization</li> <li>Millions of nodes/edges</li> </ul> </li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>llcuda provides comprehensive Python APIs:</p> Module Description <code>llcuda.api.client</code> OpenAI-compatible llama.cpp client <code>llcuda.api.multigpu</code> Multi-GPU configuration for Kaggle <code>llcuda.api.gguf</code> GGUF parsing and quantization tools <code>llcuda.api.nccl</code> NCCL for distributed PyTorch <code>llcuda.server</code> Server lifecycle management <code>llcuda.graphistry</code> Graphistry integration helpers <p> Full API Documentation</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Repository: github.com/llcuda/llcuda</li> <li>GitHub Releases: v2.2.0 Download</li> <li>Bug Reports: GitHub Issues</li> <li>Email: waqasm86@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - Free for commercial and personal use. See LICENSE.</p>    Built with \u2764\ufe0f by Waqas Muhammad | Powered by llama.cpp | Optimized for Unsloth &amp; Graphistry"},{"location":"api/client/","title":"LlamaCppClient API","text":"<p>OpenAI-compatible client for llama-server.</p>"},{"location":"api/client/#overview","title":"Overview","text":"<p>The <code>LlamaCppClient</code> provides an OpenAI-compatible interface to llama-server.</p>"},{"location":"api/client/#basic-usage","title":"Basic Usage","text":"<pre><code>from llcuda.api.client import LlamaCppClient\n\nclient = LlamaCppClient(base_url=\"http://localhost:8080\")\n\nresponse = client.create_chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    max_tokens=100\n)\n</code></pre>"},{"location":"api/client/#class-reference","title":"Class Reference","text":""},{"location":"api/client/#llamacppclient","title":"<code>LlamaCppClient</code>","text":"<pre><code>class LlamaCppClient:\n    def __init__(self, base_url: str = \"http://localhost:8080\"):\n        \"\"\"Initialize client.\n\n        Args:\n            base_url: Base URL of llama-server\n        \"\"\"\n</code></pre>"},{"location":"api/client/#methods","title":"Methods","text":""},{"location":"api/client/#create_chat_completion","title":"<code>create_chat_completion()</code>","text":"<pre><code>def create_chat_completion(\n    self,\n    messages: List[Dict[str, str]],\n    max_tokens: int = 100,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    stream: bool = False\n) -&gt; Dict:\n    \"\"\"Create chat completion.\n\n    Args:\n        messages: List of message dicts with 'role' and 'content'\n        max_tokens: Maximum tokens to generate\n        temperature: Sampling temperature\n        top_p: Nucleus sampling parameter\n        stream: Enable streaming\n\n    Returns:\n        Response dict with 'choices', 'usage', etc.\n    \"\"\"\n</code></pre>"},{"location":"api/client/#examples","title":"Examples","text":"<p>See API Examples</p>"},{"location":"api/examples/","title":"Code Examples","text":"<p>Complete, production-ready code examples for common llcuda use cases.</p>"},{"location":"api/examples/#quick-reference","title":"Quick Reference","text":"Example Use Case Complexity Basic Inference Single question-answer Beginner Chat Application Interactive conversation Beginner Batch Processing Process multiple prompts Beginner Streaming Inference Real-time token generation Intermediate Custom Parameters Fine-tune generation Intermediate Context Manager Auto-cleanup resources Intermediate Error Handling Production-ready code Advanced Benchmarking Measure performance Advanced"},{"location":"api/examples/#basic-inference","title":"Basic Inference","text":"<p>Simple question-answer inference.</p> <pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\n    \"Explain quantum computing in simple terms\",\n    max_tokens=200,\n    temperature=0.7\n)\n\n# Print results\nprint(f\"Response: {result.text}\")\nprint(f\"\\nPerformance:\")\nprint(f\"  Speed: {result.tokens_per_sec:.1f} tokens/sec\")\nprint(f\"  Latency: {result.latency_ms:.0f}ms\")\nprint(f\"  Tokens: {result.tokens_generated}\")\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Response: Quantum computing uses quantum mechanics principles...\n\nPerformance:\n  Speed: 134.2 tokens/sec\n  Latency: 690ms\n  Tokens: 93\n</code></pre></p>"},{"location":"api/examples/#chat-application","title":"Chat Application","text":"<p>Interactive chat with conversation loop.</p> <pre><code>import llcuda\n\ndef chat_application():\n    \"\"\"Interactive chat application with Gemma 3-1B.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n\n    print(\"Loading Gemma 3-1B model...\")\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    print(\"\\n\ud83e\udd16 Chat with Gemma 3-1B\")\n    print(\"Type 'exit' to quit, 'clear' to reset metrics\\n\")\n\n    while True:\n        # Get user input\n        user_input = input(\"You: \").strip()\n\n        # Handle commands\n        if user_input.lower() == 'exit':\n            print(\"\\nGoodbye!\")\n            break\n\n        if user_input.lower() == 'clear':\n            engine.reset_metrics()\n            print(\"\u2705 Metrics reset\\n\")\n            continue\n\n        if not user_input:\n            continue\n\n        # Generate response\n        result = engine.infer(\n            user_input,\n            max_tokens=300,\n            temperature=0.7\n        )\n\n        # Display response\n        print(f\"\\n\ud83e\udd16 AI: {result.text}\")\n        print(f\"   ({result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms)\\n\")\n\n    # Show final metrics\n    metrics = engine.get_metrics()\n    print(\"\\n\ud83d\udcca Session Statistics:\")\n    print(f\"  Total requests: {metrics['throughput']['total_requests']}\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Run the chat app\nif __name__ == \"__main__\":\n    chat_application()\n</code></pre> <p>Sample Interaction: <pre><code>You: What is machine learning?\n\n\ud83e\udd16 AI: Machine learning is a subset of artificial intelligence that enables\n   computers to learn from data without explicit programming...\n   (134.5 tok/s, 685ms)\n\nYou: Give me an example\n\n\ud83e\udd16 AI: A common example is email spam filtering. The system learns to\n   identify spam by analyzing thousands of emails...\n   (136.2 tok/s, 702ms)\n\nYou: exit\n\n\ud83d\udcca Session Statistics:\n  Total requests: 2\n  Total tokens: 184\n  Avg speed: 135.2 tok/s\n  Avg latency: 694ms\n</code></pre></p>"},{"location":"api/examples/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently.</p> <pre><code>import llcuda\nimport time\n\ndef batch_processing_example():\n    \"\"\"Process multiple prompts with performance tracking.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define prompts\n    prompts = [\n        \"What is artificial intelligence?\",\n        \"Explain neural networks briefly.\",\n        \"What is deep learning?\",\n        \"Define machine learning.\",\n        \"What are transformers in AI?\",\n        \"Explain backpropagation.\",\n        \"What is gradient descent?\",\n        \"Define overfitting in ML.\"\n    ]\n\n    print(f\"Processing {len(prompts)} prompts...\\n\")\n\n    # Reset metrics\n    engine.reset_metrics()\n\n    # Process batch\n    start_time = time.time()\n    results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n    total_time = time.time() - start_time\n\n    # Display results\n    for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n        print(f\"{i}. Q: {prompt}\")\n        print(f\"   A: {result.text[:100]}...\")\n        print(f\"   Performance: {result.tokens_per_sec:.1f} tok/s, {result.latency_ms:.0f}ms\\n\")\n\n    # Show aggregate metrics\n    metrics = engine.get_metrics()\n    print(\"\ud83d\udcca Batch Processing Summary:\")\n    print(f\"  Prompts processed: {len(prompts)}\")\n    print(f\"  Total time: {total_time:.2f}s\")\n    print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n    print(f\"  Avg throughput: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"  Avg latency: {metrics['latency']['mean_ms']:.0f}ms\")\n    print(f\"  P95 latency: {metrics['latency']['p95_ms']:.0f}ms\")\n    print(f\"  Requests/sec: {len(prompts) / total_time:.2f}\")\n\n# Run batch processing\nif __name__ == \"__main__\":\n    batch_processing_example()\n</code></pre> <p>Expected Output: <pre><code>Processing 8 prompts...\n\n1. Q: What is artificial intelligence?\n   A: Artificial intelligence (AI) is the simulation of human intelligence...\n   Performance: 134.8 tok/s, 685ms\n\n2. Q: Explain neural networks briefly.\n   A: Neural networks are computational models inspired by the human brain...\n   Performance: 135.2 tok/s, 692ms\n\n[...]\n\n\ud83d\udcca Batch Processing Summary:\n  Prompts processed: 8\n  Total time: 5.52s\n  Total tokens: 592\n  Avg throughput: 134.5 tok/s\n  Avg latency: 690ms\n  P95 latency: 725ms\n  Requests/sec: 1.45\n</code></pre></p>"},{"location":"api/examples/#streaming-inference","title":"Streaming Inference","text":"<p>Stream tokens as they're generated (simulation).</p> <pre><code>import llcuda\nimport time\n\ndef streaming_inference_example():\n    \"\"\"Demonstrate streaming inference with callback.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Define callback for streaming\n    def stream_callback(chunk):\n        \"\"\"Print each chunk as it arrives.\"\"\"\n        print(chunk, end='', flush=True)\n\n    prompt = \"Write a short story about a robot learning to paint\"\n\n    print(\"\ud83e\udd16 Generating story (streaming):\\n\")\n    print(\"AI: \", end='', flush=True)\n\n    # Stream inference\n    result = engine.infer_stream(\n        prompt,\n        callback=stream_callback,\n        max_tokens=200,\n        temperature=0.8\n    )\n\n    # Show metrics\n    print(f\"\\n\\n\ud83d\udcca Performance:\")\n    print(f\"  Speed: {result.tokens_per_sec:.1f} tok/s\")\n    print(f\"  Latency: {result.latency_ms:.0f}ms\")\n    print(f\"  Tokens: {result.tokens_generated}\")\n\n# Run streaming example\nif __name__ == \"__main__\":\n    streaming_inference_example()\n</code></pre> <p>Note: Current implementation simulates streaming. True token-by-token streaming will be available in a future release.</p>"},{"location":"api/examples/#custom-generation-parameters","title":"Custom Generation Parameters","text":"<p>Fine-tune generation with custom parameters.</p> <pre><code>import llcuda\n\ndef custom_parameters_example():\n    \"\"\"Demonstrate different generation strategies.\"\"\"\n\n    # Initialize engine\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    prompt = \"Once upon a time in a futuristic city\"\n\n    # Strategy 1: Deterministic (low temperature)\n    print(\"1\ufe0f\u20e3 Deterministic Generation (temp=0.1):\")\n    result1 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.1,\n        top_p=0.9,\n        top_k=10\n    )\n    print(f\"{result1.text}\\n\")\n\n    # Strategy 2: Balanced (default)\n    print(\"2\ufe0f\u20e3 Balanced Generation (temp=0.7):\")\n    result2 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=0.7,\n        top_p=0.9,\n        top_k=40\n    )\n    print(f\"{result2.text}\\n\")\n\n    # Strategy 3: Creative (high temperature)\n    print(\"3\ufe0f\u20e3 Creative Generation (temp=1.2):\")\n    result3 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.2,\n        top_p=0.95,\n        top_k=100\n    )\n    print(f\"{result3.text}\\n\")\n\n    # Strategy 4: Very creative (high temp + nucleus sampling)\n    print(\"4\ufe0f\u20e3 Very Creative (temp=1.5, top_p=0.95):\")\n    result4 = engine.infer(\n        prompt,\n        max_tokens=100,\n        temperature=1.5,\n        top_p=0.95,\n        top_k=200\n    )\n    print(f\"{result4.text}\\n\")\n\n    # Compare performance\n    print(\"\ud83d\udcca Performance Comparison:\")\n    for i, result in enumerate([result1, result2, result3, result4], 1):\n        print(f\"  Strategy {i}: {result.tokens_per_sec:.1f} tok/s\")\n\n# Run custom parameters example\nif __name__ == \"__main__\":\n    custom_parameters_example()\n</code></pre> <p>Parameter Guide:</p> Parameter Range Effect Use Case <code>temperature</code> 0.1 - 0.3 Deterministic, focused Code, facts <code>temperature</code> 0.6 - 0.8 Balanced creativity General chat <code>temperature</code> 1.0 - 1.5 Very creative Stories, brainstorming <code>top_p</code> 0.9 - 0.95 Nucleus sampling Quality control <code>top_k</code> 10 - 200 Diversity limit Token variety"},{"location":"api/examples/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Automatic resource cleanup.</p> <pre><code>import llcuda\n\ndef context_manager_example():\n    \"\"\"Use context manager for automatic cleanup.\"\"\"\n\n    # Context manager ensures server cleanup\n    with llcuda.InferenceEngine() as engine:\n        # Load model\n        engine.load_model(\n            \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n            silent=True\n        )\n\n        # Run inferences\n        prompts = [\n            \"What is Python?\",\n            \"What is JavaScript?\",\n            \"What is Rust?\"\n        ]\n\n        for prompt in prompts:\n            result = engine.infer(prompt, max_tokens=50)\n            print(f\"Q: {prompt}\")\n            print(f\"A: {result.text}\\n\")\n\n        # Get final metrics\n        metrics = engine.get_metrics()\n        print(f\"Total tokens: {metrics['throughput']['total_tokens']}\")\n\n    # Server automatically stopped here\n    print(\"\u2705 Server cleaned up automatically\")\n\n# Run context manager example\nif __name__ == \"__main__\":\n    context_manager_example()\n</code></pre>"},{"location":"api/examples/#robust-error-handling","title":"Robust Error Handling","text":"<p>Production-ready error handling.</p> <pre><code>import llcuda\nfrom llcuda import InferenceEngine\n\ndef robust_inference(prompt: str, max_retries: int = 3):\n    \"\"\"Robust inference with error handling and retries.\"\"\"\n\n    engine = None\n\n    try:\n        # Check GPU compatibility\n        compat = llcuda.check_gpu_compatibility()\n        if not compat['compatible']:\n            raise RuntimeError(\n                f\"GPU {compat['gpu_name']} is not compatible: {compat['reason']}\"\n            )\n\n        # Initialize engine\n        engine = InferenceEngine()\n\n        # Load model with error handling\n        try:\n            engine.load_model(\n                \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n                silent=True,\n                auto_start=True\n            )\n        except FileNotFoundError as e:\n            print(f\"Model not found: {e}\")\n            print(\"Please download the model first\")\n            return None\n        except RuntimeError as e:\n            print(f\"Server failed to start: {e}\")\n            return None\n\n        # Run inference with retries\n        for attempt in range(max_retries):\n            result = engine.infer(prompt, max_tokens=200)\n\n            if result.success:\n                return {\n                    'text': result.text,\n                    'tokens_per_sec': result.tokens_per_sec,\n                    'latency_ms': result.latency_ms,\n                    'success': True\n                }\n            else:\n                print(f\"Attempt {attempt + 1} failed: {result.error_message}\")\n                if attempt &lt; max_retries - 1:\n                    print(f\"Retrying... ({max_retries - attempt - 1} attempts left)\")\n                    import time\n                    time.sleep(1)\n\n        # All retries failed\n        return {\n            'text': None,\n            'error': 'All retry attempts failed',\n            'success': False\n        }\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {\n            'text': None,\n            'error': str(e),\n            'success': False\n        }\n\n    finally:\n        # Cleanup\n        if engine is not None:\n            engine.unload_model()\n\n# Example usage\nif __name__ == \"__main__\":\n    result = robust_inference(\"What is quantum computing?\")\n\n    if result and result['success']:\n        print(f\"\u2705 Success!\")\n        print(f\"Response: {result['text']}\")\n        print(f\"Speed: {result['tokens_per_sec']:.1f} tok/s\")\n    else:\n        print(f\"\u274c Failed: {result['error'] if result else 'Unknown error'}\")\n</code></pre>"},{"location":"api/examples/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Comprehensive performance measurement.</p> <pre><code>import llcuda\nimport time\nimport statistics\n\ndef benchmark_inference(num_runs: int = 10):\n    \"\"\"Benchmark inference performance.\"\"\"\n\n    # Initialize\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Warmup\n    print(\"Warming up...\")\n    for _ in range(3):\n        engine.infer(\"Warmup prompt\", max_tokens=10)\n\n    # Benchmark\n    print(f\"Running {num_runs} iterations...\\n\")\n\n    engine.reset_metrics()\n    latencies = []\n    throughputs = []\n\n    test_prompt = \"Explain the concept of recursion in programming\"\n\n    for i in range(num_runs):\n        start = time.time()\n        result = engine.infer(test_prompt, max_tokens=100)\n        elapsed = (time.time() - start) * 1000  # Convert to ms\n\n        latencies.append(result.latency_ms)\n        throughputs.append(result.tokens_per_sec)\n\n        print(f\"Run {i+1}/{num_runs}: \"\n              f\"{result.tokens_per_sec:.1f} tok/s, \"\n              f\"{result.latency_ms:.0f}ms\")\n\n    # Calculate statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83d\udcca Benchmark Results\")\n    print(\"=\"*60)\n\n    print(\"\\nThroughput (tokens/sec):\")\n    print(f\"  Mean:   {statistics.mean(throughputs):.2f}\")\n    print(f\"  Median: {statistics.median(throughputs):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(throughputs):.2f}\")\n    print(f\"  Min:    {min(throughputs):.2f}\")\n    print(f\"  Max:    {max(throughputs):.2f}\")\n\n    print(\"\\nLatency (ms):\")\n    print(f\"  Mean:   {statistics.mean(latencies):.2f}\")\n    print(f\"  Median: {statistics.median(latencies):.2f}\")\n    print(f\"  Stdev:  {statistics.stdev(latencies):.2f}\")\n    print(f\"  Min:    {min(latencies):.2f}\")\n    print(f\"  Max:    {max(latencies):.2f}\")\n\n    # Percentiles\n    sorted_latencies = sorted(latencies)\n    p50_idx = len(sorted_latencies) // 2\n    p95_idx = int(len(sorted_latencies) * 0.95)\n    p99_idx = int(len(sorted_latencies) * 0.99)\n\n    print(\"\\nLatency Percentiles:\")\n    print(f\"  P50:    {sorted_latencies[p50_idx]:.2f}ms\")\n    print(f\"  P95:    {sorted_latencies[p95_idx]:.2f}ms\")\n    print(f\"  P99:    {sorted_latencies[p99_idx]:.2f}ms\")\n\n    # Get metrics from engine\n    metrics = engine.get_metrics()\n    print(f\"\\nTotal tokens generated: {metrics['throughput']['total_tokens']}\")\n    print(f\"Total requests: {metrics['throughput']['total_requests']}\")\n\n    print(\"=\"*60)\n\n# Run benchmark\nif __name__ == \"__main__\":\n    benchmark_inference(num_runs=10)\n</code></pre> <p>Expected Output on Tesla T4: <pre><code>Warming up...\nRunning 10 iterations...\n\nRun 1/10: 134.2 tok/s, 690ms\nRun 2/10: 136.5 tok/s, 685ms\nRun 3/10: 133.8 tok/s, 695ms\n[...]\n\n============================================================\n\ud83d\udcca Benchmark Results\n============================================================\n\nThroughput (tokens/sec):\n  Mean:   134.52\n  Median: 134.30\n  Stdev:  1.24\n  Min:    132.80\n  Max:    136.50\n\nLatency (ms):\n  Mean:   692.45\n  Median: 690.00\n  Stdev:  8.32\n  Min:    685.00\n  Max:    710.00\n\nLatency Percentiles:\n  P50:    690.00ms\n  P95:    705.00ms\n  P99:    710.00ms\n\nTotal tokens generated: 940\nTotal requests: 10\n============================================================\n</code></pre></p>"},{"location":"api/examples/#advanced-custom-chat-engine","title":"Advanced: Custom Chat Engine","text":"<p>Using the ChatEngine for conversations.</p> <pre><code>from llcuda import InferenceEngine\nfrom llcuda.chat import ChatEngine\n\ndef advanced_chat_example():\n    \"\"\"Advanced chat with conversation history.\"\"\"\n\n    # Initialize inference engine\n    engine = InferenceEngine()\n    engine.load_model(\n        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n        silent=True\n    )\n\n    # Create chat engine with system prompt\n    chat = ChatEngine(\n        engine,\n        system_prompt=\"You are a helpful AI coding assistant specialized in Python.\",\n        max_history=20,\n        max_tokens=200,\n        temperature=0.7\n    )\n\n    # Conversation\n    chat.add_user_message(\"How do I read a file in Python?\")\n    response1 = chat.complete()\n    print(f\"AI: {response1}\\n\")\n\n    chat.add_user_message(\"Can you show me an example?\")\n    response2 = chat.complete()\n    print(f\"AI: {response2}\\n\")\n\n    chat.add_user_message(\"What about writing to a file?\")\n    response3 = chat.complete()\n    print(f\"AI: {response3}\\n\")\n\n    # Get conversation history\n    history = chat.get_history()\n    print(f\"Conversation has {len(history)} messages\")\n\n    # Save conversation\n    chat.save_history(\"conversation.json\")\n    print(\"\u2705 Conversation saved to conversation.json\")\n\n    # Token count\n    token_count = chat.count_tokens()\n    print(f\"Approximate token count: {token_count}\")\n\n# Run advanced chat\nif __name__ == \"__main__\":\n    advanced_chat_example()\n</code></pre>"},{"location":"api/examples/#see-also","title":"See Also","text":"<ul> <li>API Overview - Complete API reference</li> <li>InferenceEngine - Detailed engine documentation</li> <li>Quick Start - Getting started guide</li> <li>Tutorials - Step-by-step tutorials</li> <li>Performance - Benchmark results</li> </ul>"},{"location":"api/gguf/","title":"GGUF Tools API","text":"<p>GGUF file parsing and utilities.</p>"},{"location":"api/gguf/#overview","title":"Overview","text":"<p>Tools for working with GGUF model files.</p>"},{"location":"api/gguf/#basic-usage","title":"Basic Usage","text":"<pre><code>from llcuda.utils import GGUFParser\n\nparser = GGUFParser(model_path=\"model.gguf\")\nprint(f\"Parameters: {parser.get_parameter_count() / 1e9:.1f}B\")\nprint(f\"Quantization: {parser.get_quantization()}\")\n</code></pre>"},{"location":"api/gguf/#class-reference","title":"Class Reference","text":""},{"location":"api/gguf/#ggufparser","title":"<code>GGUFParser</code>","text":"<pre><code>class GGUFParser:\n    def __init__(self, model_path: str):\n        \"\"\"Initialize parser.\n\n        Args:\n            model_path: Path to GGUF file\n        \"\"\"\n\n    def get_parameter_count(self) -&gt; int:\n        \"\"\"Get total parameter count.\"\"\"\n\n    def get_quantization(self) -&gt; str:\n        \"\"\"Get quantization type.\"\"\"\n\n    def get_context_length(self) -&gt; int:\n        \"\"\"Get max context length.\"\"\"\n\n    def get_metadata(self) -&gt; Dict:\n        \"\"\"Get all metadata.\"\"\"\n</code></pre>"},{"location":"api/gguf/#functions","title":"Functions","text":""},{"location":"api/gguf/#estimate_vram","title":"<code>estimate_vram()</code>","text":"<pre><code>def estimate_vram(model_size_b: float, quant_type: str) -&gt; float:\n    \"\"\"Estimate VRAM usage.\n\n    Args:\n        model_size_b: Model size in billions\n        quant_type: Quantization type (Q4_K_M, IQ3_XS, etc.)\n\n    Returns:\n        Estimated VRAM in GB\n    \"\"\"\n</code></pre>"},{"location":"api/gguf/#quantization-types","title":"Quantization Types","text":"<p>K-Quants: - Q4_K_M, Q5_K_M, Q6_K, Q8_0</p> <p>I-Quants: - IQ3_XS, IQ2_XXS</p>"},{"location":"api/gguf/#examples","title":"Examples","text":"<p>See GGUF Tutorial</p>"},{"location":"api/graphistry/","title":"Graphistry Integration","text":"<p>Knowledge graph visualization with RAPIDS on GPU 1.</p>"},{"location":"api/graphistry/#overview","title":"Overview","text":"<p>Run Graphistry on GPU 1 while LLM runs on GPU 0.</p>"},{"location":"api/graphistry/#basic-usage","title":"Basic Usage","text":"<pre><code>import os\n\n# LLM on GPU 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom llcuda.server import ServerManager\nserver = ServerManager()\n\n# Graphistry on GPU 1\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport graphistry\ngraphistry.register(api=3)\n</code></pre>"},{"location":"api/graphistry/#split-gpu-workflow","title":"Split-GPU Workflow","text":"<pre><code>GPU 0: llama-server (LLM)\n  \u2193 Extract entities\nGPU 1: RAPIDS + Graphistry\n  \u2192 Visualize graphs\n</code></pre>"},{"location":"api/graphistry/#examples","title":"Examples","text":"<p>See Split-GPU Tutorial</p>"},{"location":"api/models/","title":"Models API Reference","text":"<p>API documentation for model management, discovery, and downloading in llcuda.</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>The <code>llcuda.models</code> module provides utilities for:</p> <ul> <li>Loading models from registry or HuggingFace</li> <li>Downloading and caching GGUF models</li> <li>Getting model metadata and information</li> <li>Recommending optimal inference settings</li> </ul>"},{"location":"api/models/#functions","title":"Functions","text":""},{"location":"api/models/#load_model_smart","title":"<code>load_model_smart()</code>","text":"<p>Smart model loading with automatic download and path resolution.</p> <pre><code>def load_model_smart(\n    model_name_or_path: str,\n    interactive: bool = True\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_name_or_path</code> <code>str</code> Required Model name, HF repo, or local path <code>interactive</code> <code>bool</code> <code>True</code> Ask for confirmation before downloading <p>Returns:</p> <ul> <li><code>Path</code> - Path to model file</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import load_model_smart\n\n# Load from registry\nmodel_path = load_model_smart(\"gemma-3-1b-Q4_K_M\")\n\n# Load from HuggingFace\nmodel_path = load_model_smart(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Load local file\nmodel_path = load_model_smart(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"api/models/#download_model","title":"<code>download_model()</code>","text":"<p>Download a model from HuggingFace.</p> <pre><code>def download_model(\n    repo_id: str,\n    filename: str,\n    cache_dir: Optional[str] = None\n) -&gt; Path\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>repo_id</code> <code>str</code> Required HuggingFace repository ID <code>filename</code> <code>str</code> Required Model filename to download <code>cache_dir</code> <code>Optional[str]</code> <code>None</code> Custom cache directory <p>Returns:</p> <ul> <li><code>Path</code> - Path to downloaded model</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import download_model\n\n# Download from HuggingFace\nmodel_path = download_model(\n    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n    filename=\"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"api/models/#list_registry_models","title":"<code>list_registry_models()</code>","text":"<p>List available models in the registry.</p> <pre><code>def list_registry_models() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Returns:</p> <ul> <li><code>List[Dict]</code> - List of model information dictionaries</li> </ul> <p>Example:</p> <pre><code>from llcuda.models import list_registry_models\n\nmodels = list_registry_models()\nfor model in models:\n    print(f\"{model['name']}: {model['description']}\")\n</code></pre>"},{"location":"api/models/#classes","title":"Classes","text":""},{"location":"api/models/#modelinfo","title":"<code>ModelInfo</code>","text":"<p>Extract metadata from GGUF models.</p> <pre><code>class ModelInfo:\n    def __init__(self, filepath: str)\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>filepath</code> <code>Path</code> Path to GGUF file <code>architecture</code> <code>Optional[str]</code> Model architecture (e.g., \"llama\", \"gemma\") <code>parameter_count</code> <code>Optional[int]</code> Estimated parameter count <code>context_length</code> <code>Optional[int]</code> Maximum context length <code>quantization</code> <code>Optional[str]</code> Quantization type <code>file_size_mb</code> <code>float</code> File size in MB <p>Methods:</p>"},{"location":"api/models/#get_recommended_settings","title":"<code>get_recommended_settings()</code>","text":"<p>Get recommended inference settings based on model and hardware.</p> <pre><code>def get_recommended_settings(\n    vram_gb: float = 8.0\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>vram_gb</code> <code>float</code> <code>8.0</code> Available VRAM in GB <p>Returns:</p> <pre><code>{\n    'gpu_layers': int,\n    'ctx_size': int,\n    'batch_size': int,\n    'ubatch_size': int\n}\n</code></pre> <p>Example:</p> <pre><code>from llcuda.models import ModelInfo\n\n# Load model info\ninfo = ModelInfo(\"gemma-3-1b-Q4_K_M.gguf\")\n\nprint(f\"Architecture: {info.architecture}\")\nprint(f\"Parameters: {info.parameter_count}B\")\nprint(f\"Context: {info.context_length}\")\nprint(f\"Size: {info.file_size_mb:.1f} MB\")\n\n# Get recommended settings for T4 (15GB)\nsettings = info.get_recommended_settings(vram_gb=15.0)\nprint(f\"Recommended gpu_layers: {settings['gpu_layers']}\")\nprint(f\"Recommended ctx_size: {settings['ctx_size']}\")\n</code></pre>"},{"location":"api/models/#model-registry","title":"Model Registry","text":"<p>llcuda includes a built-in model registry with popular models:</p> <pre><code>REGISTRY = {\n    \"gemma-3-1b-Q4_K_M\": {\n        \"repo\": \"unsloth/gemma-3-1b-it-GGUF\",\n        \"file\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n        \"size\": \"700 MB\",\n        \"description\": \"Gemma 3 1B Instruct, Q4_K_M quantized\"\n    },\n    \"llama-3.2-3b-Q4_K_M\": {\n        \"repo\": \"unsloth/Llama-3.2-3B-Instruct-GGUF\",\n        \"file\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n        \"size\": \"1.9 GB\",\n        \"description\": \"Llama 3.2 3B Instruct, Q4_K_M\"\n    }\n}\n</code></pre>"},{"location":"api/models/#see-also","title":"See Also","text":"<ul> <li>InferenceEngine API</li> <li>Model Selection Guide</li> <li>GGUF Format</li> </ul>"},{"location":"api/multigpu/","title":"MultiGPU API","text":"<p>Multi-GPU configuration and utilities for dual T4 setup.</p>"},{"location":"api/multigpu/#overview","title":"Overview","text":"<p>The <code>multigpu</code> module provides GPU detection and configuration for Kaggle dual T4.</p>"},{"location":"api/multigpu/#basic-usage","title":"Basic Usage","text":"<pre><code>from llcuda.api.multigpu import detect_gpus, kaggle_t4_dual_config\n\n# Detect GPUs\ngpus = detect_gpus()\nfor gpu in gpus:\n    print(f\"GPU {gpu.index}: {gpu.name}\")\n\n# Get Kaggle dual T4 configuration\nconfig = kaggle_t4_dual_config(model_path=\"model.gguf\")\n</code></pre>"},{"location":"api/multigpu/#functions","title":"Functions","text":""},{"location":"api/multigpu/#detect_gpus","title":"<code>detect_gpus()</code>","text":"<pre><code>def detect_gpus() -&gt; List[GPUInfo]:\n    \"\"\"Detect available GPUs.\n\n    Returns:\n        List of GPUInfo objects\n    \"\"\"\n</code></pre>"},{"location":"api/multigpu/#kaggle_t4_dual_config","title":"<code>kaggle_t4_dual_config()</code>","text":"<pre><code>def kaggle_t4_dual_config(\n    model_path: str,\n    tensor_split: str = \"0.5,0.5\"\n) -&gt; ServerConfig:\n    \"\"\"Get optimized config for Kaggle dual T4.\n\n    Args:\n        model_path: Path to GGUF model\n        tensor_split: GPU split ratio\n\n    Returns:\n        ServerConfig for dual T4\n    \"\"\"\n</code></pre>"},{"location":"api/multigpu/#estimate_model_vram","title":"<code>estimate_model_vram()</code>","text":"<pre><code>def estimate_model_vram(\n    model_size_b: float,\n    quant_type: str = \"Q4_K_M\"\n) -&gt; float:\n    \"\"\"Estimate VRAM usage in GB.\n\n    Args:\n        model_size_b: Model size in billions\n        quant_type: Quantization type\n\n    Returns:\n        Estimated VRAM in GB\n    \"\"\"\n</code></pre>"},{"location":"api/multigpu/#classes","title":"Classes","text":""},{"location":"api/multigpu/#gpuinfo","title":"<code>GPUInfo</code>","text":"<pre><code>class GPUInfo:\n    index: int\n    name: str\n    compute_capability: Tuple[int, int]\n    memory_total: int  # bytes\n    memory_free: int   # bytes\n</code></pre>"},{"location":"api/multigpu/#examples","title":"Examples","text":"<p>See Multi-GPU Tutorial</p>"},{"location":"api/nccl/","title":"NCCL Integration","text":"<p>NCCL vs tensor-split for distributed workloads.</p>"},{"location":"api/nccl/#overview","title":"Overview","text":"<p>llcuda uses native CUDA tensor-split (NOT NCCL) for multi-GPU inference.</p>"},{"location":"api/nccl/#key-differences","title":"Key Differences","text":"<p>llama-server (llcuda): - Native CUDA layer distribution - NO NCCL required - For LLM inference</p> <p>PyTorch DDP: - NCCL for distributed training - For fine-tuning</p>"},{"location":"api/nccl/#llcuda-tensor-split","title":"llcuda tensor-split","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    tensor_split=\"0.5,0.5\",  # Native CUDA\n    n_gpu_layers=99\n)\n</code></pre>"},{"location":"api/nccl/#pytorch-with-nccl","title":"PyTorch with NCCL","text":"<pre><code>import torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")\n# Training code here\n</code></pre>"},{"location":"api/nccl/#when-to-use-each","title":"When to Use Each","text":"<ul> <li>llcuda tensor-split: Multi-GPU inference</li> <li>PyTorch NCCL: Multi-GPU training</li> </ul>"},{"location":"api/nccl/#examples","title":"Examples","text":"<p>See NCCL Tutorial</p>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Complete API documentation for llcuda v2.2.0.</p>"},{"location":"api/overview/#main-components","title":"Main Components","text":"<p>llcuda provides a simple, PyTorch-style API for GPU-accelerated LLM inference.</p>"},{"location":"api/overview/#core-classes","title":"Core Classes","text":"Class Purpose Documentation <code>InferenceEngine</code> Main interface for model loading and inference Details <code>InferenceResult</code> Container for inference results with metrics Details"},{"location":"api/overview/#utility-functions","title":"Utility Functions","text":"Function Purpose Documentation <code>check_gpu_compatibility()</code> Verify GPU support Details <code>get_device_properties()</code> Get GPU device information Details"},{"location":"api/overview/#quick-api-reference","title":"Quick API Reference","text":""},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>import llcuda\n\n# Create engine\nengine = llcuda.InferenceEngine()\n\n# Load model\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n\n# Run inference\nresult = engine.infer(\"What is AI?\", max_tokens=100)\n\n# Access results\nprint(result.text)                    # Generated text\nprint(result.tokens_per_sec)          # Speed in tokens/sec\nprint(result.latency_ms)              # Latency in milliseconds\nprint(result.tokens_generated)        # Number of tokens generated\n</code></pre>"},{"location":"api/overview/#inferenceengine-methods","title":"InferenceEngine Methods","text":""},{"location":"api/overview/#__init__server_urlnone","title":"<code>__init__(server_url=None)</code>","text":"<p>Create a new inference engine instance.</p> <p>Parameters: - <code>server_url</code> (str, optional): Custom llama-server URL. Default: <code>http://127.0.0.1:8090</code></p>"},{"location":"api/overview/#load_modelmodel_path-silentfalse-auto_starttrue-kwargs","title":"<code>load_model(model_path, silent=False, auto_start=True, **kwargs)</code>","text":"<p>Load a GGUF model for inference.</p> <p>Parameters: - <code>model_path</code> (str): Model identifier or path   - HuggingFace: <code>\"unsloth/repo-name:filename.gguf\"</code>   - Registry: <code>\"gemma-3-1b-Q4_K_M\"</code>   - Local: <code>\"/path/to/model.gguf\"</code> - <code>silent</code> (bool): Suppress llama-server output. Default: <code>False</code> - <code>auto_start</code> (bool): Start server automatically. Default: <code>True</code> - <code>**kwargs</code>: Additional options (context_size, gpu_layers, etc.)</p>"},{"location":"api/overview/#inferprompt-max_tokens512-temperature07-kwargs","title":"<code>infer(prompt, max_tokens=512, temperature=0.7, **kwargs)</code>","text":"<p>Run inference on a single prompt.</p> <p>Parameters: - <code>prompt</code> (str): Input text - <code>max_tokens</code> (int): Maximum tokens to generate. Default: 512 - <code>temperature</code> (float): Sampling temperature. Default: 0.7 - <code>top_p</code> (float): Nucleus sampling threshold. Default: 0.9 - <code>top_k</code> (int): Top-k sampling. Default: 40 - <code>stop_sequences</code> (list): Stop generation at these sequences</p> <p>Returns: - <code>InferenceResult</code>: Result object with text and metrics</p>"},{"location":"api/overview/#batch_inferprompts-max_tokens512-kwargs","title":"<code>batch_infer(prompts, max_tokens=512, **kwargs)</code>","text":"<p>Run inference on multiple prompts.</p> <p>Parameters: - <code>prompts</code> (list[str]): List of input texts - <code>max_tokens</code> (int): Maximum tokens per prompt - <code>**kwargs</code>: Same as <code>infer()</code></p> <p>Returns: - <code>list[InferenceResult]</code>: List of results</p>"},{"location":"api/overview/#get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get aggregated performance metrics.</p> <p>Returns: - <code>dict</code>: Metrics dictionary with throughput and latency stats</p>"},{"location":"api/overview/#inferenceresult-attributes","title":"InferenceResult Attributes","text":"Attribute Type Description <code>text</code> str Generated text <code>tokens_per_sec</code> float Generation speed <code>latency_ms</code> float Total latency in ms <code>tokens_generated</code> int Number of tokens"},{"location":"api/overview/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api/overview/#check_gpu_compatibility","title":"<code>check_gpu_compatibility()</code>","text":"<p>Check if current GPU is compatible with llcuda.</p> <p>Returns: <pre><code>{\n    'gpu_name': str,          # e.g., \"Tesla T4\"\n    'compute_capability': str, # e.g., \"7.5\"\n    'compatible': bool,       # True if supported\n    'platform': str          # e.g., \"kaggle\", \"local\"\n}\n</code></pre></p> <p>Example: <pre><code>compat = llcuda.check_gpu_compatibility()\nif compat['compatible']:\n    print(f\"\u2705 {compat['gpu_name']} is compatible!\")\nelse:\n    print(f\"\u26a0\ufe0f {compat['gpu_name']} may not work\")\n</code></pre></p>"},{"location":"api/overview/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>InferenceEngine - Complete InferenceEngine documentation</li> <li>Models &amp; GGUF - Model loading and GGUF format</li> <li>GPU &amp; Device - GPU management and compatibility</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"api/overview/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Tutorials</li> <li>Performance Benchmarks</li> </ul>"},{"location":"api/server/","title":"ServerManager API","text":"<p>Server lifecycle management for llama-server.</p>"},{"location":"api/server/#overview","title":"Overview","text":"<p><code>ServerManager</code> handles starting, stopping, and monitoring llama-server.</p>"},{"location":"api/server/#basic-usage","title":"Basic Usage","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(model_path=\"model.gguf\", n_gpu_layers=99)\nserver = ServerManager()\nserver.start_with_config(config)\nserver.stop()\n</code></pre>"},{"location":"api/server/#class-reference","title":"Class Reference","text":""},{"location":"api/server/#servermanager","title":"<code>ServerManager</code>","text":"<pre><code>class ServerManager:\n    def start_with_config(self, config: ServerConfig, verbose: bool = False) -&gt; None:\n        \"\"\"Start server with configuration.\"\"\"\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop the running server.\"\"\"\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Check if server is running.\"\"\"\n\n    def wait_until_ready(self, timeout: int = 30) -&gt; bool:\n        \"\"\"Wait for server to be ready.\"\"\"\n\n    def get_base_url(self) -&gt; str:\n        \"\"\"Get server URL.\"\"\"\n\n    def get_logs(self) -&gt; str:\n        \"\"\"Get server logs.\"\"\"\n</code></pre>"},{"location":"api/server/#serverconfig","title":"<code>ServerConfig</code>","text":"<pre><code>class ServerConfig:\n    model_path: str\n    n_gpu_layers: int = 99\n    context_size: int = 4096\n    n_batch: int = 2048\n    flash_attn: bool = True\n    tensor_split: Optional[str] = None\n    split_mode: str = \"layer\"\n    host: str = \"127.0.0.1\"\n    port: int = 8080\n</code></pre>"},{"location":"api/server/#examples","title":"Examples","text":"<p>See Tutorials</p>"},{"location":"architecture/gpu0-llm/","title":"GPU 0 - LLM Inference","text":"<p>Configure GPU 0 for llama.cpp server.</p>"},{"location":"architecture/gpu0-llm/#setup","title":"Setup","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,        # All layers on GPU 0\n    flash_attn=True,\n)\n\n# llama-server uses GPU 0 by default\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"architecture/gpu0-llm/#vram-usage","title":"VRAM Usage","text":"Model Quant VRAM on GPU 0 1-3B Q4_K_M 2-4 GB 7B Q4_K_M 5-6 GB 13B Q4_K_M 8-9 GB"},{"location":"architecture/gpu0-llm/#performance","title":"Performance","text":"<ul> <li>FlashAttention: 2-3x speedup</li> <li>Tensor Cores: FP16/TF32 acceleration</li> <li>Context: Up to 8192 tokens</li> </ul>"},{"location":"architecture/gpu1-graphistry/","title":"GPU 1 - Graphistry","text":"<p>Use GPU 1 for RAPIDS + Graphistry visualization.</p>"},{"location":"architecture/gpu1-graphistry/#setup","title":"Setup","text":"<pre><code>import graphistry\nimport cudf\n\n# Configure Graphistry for GPU 1\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Register Graphistry\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=\"YOUR_KEY\"\n)\n</code></pre>"},{"location":"architecture/gpu1-graphistry/#workflow","title":"Workflow","text":"<ol> <li> <p>Extract from LLM (GPU 0)    <pre><code># Get entities from LLM\nentities = llm_client.extract_entities(text)\n</code></pre></p> </li> <li> <p>Build Graph (GPU 1)    <pre><code># Create graph with cuDF\nnodes_df = cudf.DataFrame(entities)\nedges_df = cudf.DataFrame(relationships)\n</code></pre></p> </li> <li> <p>Visualize (GPU 1)    <pre><code># Render with Graphistry\ng = graphistry.edges(edges_df).nodes(nodes_df)\ng.plot()\n</code></pre></p> </li> </ol>"},{"location":"architecture/gpu1-graphistry/#vram-usage","title":"VRAM Usage","text":"<ul> <li>cuDF: 1-3 GB</li> <li>cuGraph: 2-5 GB</li> <li>Graphistry: 1-4 GB</li> <li>Total: 4-12 GB on GPU 1</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>llcuda v2.2.0 architecture for Kaggle dual T4.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         llcuda v2.2.0 Stack            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Python API (llcuda.api.*)              \u2502\n\u2502  \u251c\u2500 client.py (OpenAI-compatible)      \u2502\n\u2502  \u251c\u2500 multigpu.py (Dual T4 config)       \u2502\n\u2502  \u251c\u2500 gguf.py (Quantization tools)       \u2502\n\u2502  \u2514\u2500 nccl.py (PyTorch distributed)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Server Manager (llcuda.server)         \u2502\n\u2502  \u2514\u2500 Lifecycle management                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  llama.cpp Server (C++/CUDA)            \u2502\n\u2502  \u251c\u2500 Build 7760 (commit 388ce82)        \u2502\n\u2502  \u251c\u2500 OpenAI API endpoints                \u2502\n\u2502  \u2514\u2500 Native CUDA tensor-split            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  CUDA 12.5 / cuBLAS                     \u2502\n\u2502  \u251c\u2500 FlashAttention kernels              \u2502\n\u2502  \u251c\u2500 Tensor Core optimization            \u2502\n\u2502  \u2514\u2500 SM 7.5 (Turing)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Kaggle Dual T4 (30GB VRAM)             \u2502\n\u2502  \u251c\u2500 GPU 0: LLM Inference                \u2502\n\u2502  \u2514\u2500 GPU 1: Graphistry/RAPIDS            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":"<ul> <li>Python APIs: High-level interfaces</li> <li>Server Manager: Process lifecycle  </li> <li>llama.cpp: CUDA inference engine</li> <li>Split-GPU: Dual GPU coordination</li> </ul> <p>See: - Split-GPU Design - GPU0 - LLM - GPU1 - Graphistry</p>"},{"location":"architecture/split-gpu/","title":"Split-GPU Architecture","text":"<p>Run LLM on GPU 0 + Graphistry on GPU 1.</p>"},{"location":"architecture/split-gpu/#architecture","title":"Architecture","text":"<pre><code>GPU 0 (T4 - 15GB)          GPU 1 (T4 - 15GB)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  llama-server    \u2502      \u2502  RAPIDS cuDF     \u2502\n\u2502  GGUF Model      \u2502 \u2500\u2500\u2500\u2500&gt;\u2502  cuGraph         \u2502\n\u2502  LLM Inference   \u2502      \u2502  Graphistry[ai]  \u2502\n\u2502  ~5-12GB VRAM    \u2502      \u2502  Network Viz     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/split-gpu/#configuration","title":"Configuration","text":"<pre><code>from llcuda.graphistry import SplitGPUConfig\nimport os\n\n# Assign GPUs\nconfig = SplitGPUConfig(\n    llm_gpu=0,      # GPU 0 for LLM\n    graph_gpu=1     # GPU 1 for Graphistry\n)\n\n# Set CUDA device for llama-server\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Start llama-server on GPU 0\n# (Graphistry will use GPU 1)\n</code></pre>"},{"location":"architecture/split-gpu/#use-cases","title":"Use Cases","text":"<ol> <li>Knowledge Graph Extraction</li> <li>LLM generates entities/relationships</li> <li> <p>Graphistry visualizes graphs</p> </li> <li> <p>Interactive Analysis</p> </li> <li>LLM answers questions</li> <li> <p>Graphistry shows data patterns</p> </li> <li> <p>Multi-Modal Workflows</p> </li> <li>Text generation (GPU 0)</li> <li>Graph analytics (GPU 1)</li> </ol>"},{"location":"architecture/tensor-split-vs-nccl/","title":"Tensor Split vs NCCL","text":"<p>Understanding multi-GPU approaches.</p>"},{"location":"architecture/tensor-split-vs-nccl/#tensor-split-llamacpp","title":"Tensor Split (llama.cpp)","text":"<p>What: Native CUDA layer distribution Used by: llama.cpp server Purpose: Inference parallelism</p> <pre><code># llama.cpp uses tensor-split\nconfig = ServerConfig(\n    tensor_split=\"0.5,0.5\",\n    split_mode=\"layer\",\n)\n</code></pre>"},{"location":"architecture/tensor-split-vs-nccl/#nccl-pytorch","title":"NCCL (PyTorch)","text":"<p>What: Multi-GPU communication primitives Used by: PyTorch distributed training Purpose: Training parallelism</p> <pre><code># PyTorch uses NCCL\nimport torch.distributed as dist\ndist.init_process_group(backend=\"nccl\")\n</code></pre>"},{"location":"architecture/tensor-split-vs-nccl/#key-differences","title":"Key Differences","text":"Feature Tensor Split NCCL Purpose Inference Training Backend llama.cpp PyTorch Communication Direct CUDA Collectives Use Case Model too large Distributed training"},{"location":"architecture/tensor-split-vs-nccl/#when-to-use-each","title":"When to Use Each","text":"<ul> <li>Tensor Split: Run 70B model on dual T4</li> <li>NCCL: Train model with DDP</li> </ul> <p>See: Tutorial 08 - NCCL</p>"},{"location":"gguf/i-quants/","title":"I-Quants (Importance Quantization)","text":"<p>Ultra-compressed quantization for 70B models on 30GB VRAM.</p>"},{"location":"gguf/i-quants/#overview","title":"Overview","text":"<p>I-quants use importance-based quantization to achieve extreme compression while maintaining quality.</p>"},{"location":"gguf/i-quants/#i-quant-types","title":"I-Quant Types","text":"Type Bits 70B VRAM Quality Use Case IQ3_XS ~3-bit ~28 GB Good 70B on dual T4 IQ2_XXS ~2-bit ~21 GB Fair Ultra-compressed"},{"location":"gguf/i-quants/#when-to-use-i-quants","title":"When to Use I-Quants","text":"<ul> <li>Running 70B models on 30GB VRAM</li> <li>Dual T4 Kaggle setup</li> <li>Prioritize model size over quality</li> </ul>"},{"location":"gguf/i-quants/#example","title":"Example","text":"<pre><code>from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(\n    repo_id=\"unsloth/Llama-3.1-70B-Instruct-GGUF\",\n    filename=\"Llama-3.1-70B-Instruct-IQ3_XS.gguf\"\n)\n</code></pre>"},{"location":"gguf/i-quants/#performance","title":"Performance","text":"<ul> <li>Llama-70B IQ3_XS: ~12 tokens/sec on dual T4</li> <li>VRAM: ~28-29 GB total</li> </ul>"},{"location":"gguf/i-quants/#see-also","title":"See Also","text":"<ul> <li>Large Models Tutorial</li> <li>K-Quants</li> </ul>"},{"location":"gguf/k-quants/","title":"K-Quants Guide","text":"<p>Understanding K-quantization formats.</p>"},{"location":"gguf/k-quants/#what-are-k-quants","title":"What are K-Quants?","text":"<p>K-quants use \"double quantization\": 1. Quantize weights 2. Quantize the quantization parameters</p> <p>Result: Better quality-to-size ratio</p>"},{"location":"gguf/k-quants/#available-k-quants","title":"Available K-Quants","text":""},{"location":"gguf/k-quants/#q4_k_m-recommended","title":"Q4_K_M (Recommended)","text":"<ul> <li>Bits per weight: 4.8</li> <li>Size: ~40% of FP16</li> <li>Quality: Excellent for most uses</li> <li>Use case: Default choice</li> </ul>"},{"location":"gguf/k-quants/#q5_k_m","title":"Q5_K_M","text":"<ul> <li>Bits per weight: 5.7</li> <li>Size: ~48% of FP16</li> <li>Quality: Higher than Q4_K_M</li> <li>Use case: Quality-sensitive</li> </ul>"},{"location":"gguf/k-quants/#q6_k","title":"Q6_K","text":"<ul> <li>Bits per weight: 6.6</li> <li>Size: ~55% of FP16</li> <li>Quality: Near FP16</li> <li>Use case: Maximum quality</li> </ul>"},{"location":"gguf/k-quants/#q8_0","title":"Q8_0","text":"<ul> <li>Bits per weight: 8.5</li> <li>Size: ~70% of FP16</li> <li>Quality: Virtually lossless</li> <li>Use case: Research, validation</li> </ul>"},{"location":"gguf/k-quants/#size-comparison","title":"Size Comparison","text":"Model FP16 Q4_K_M Q5_K_M Q6_K Q8_0 1B 2GB 0.8GB 1GB 1.1GB 1.4GB 7B 14GB 5.6GB 6.7GB 7.7GB 9.8GB 13B 26GB 10.4GB 12.5GB 14.3GB 18.2GB 70B 140GB 56GB 67GB 77GB 98GB"},{"location":"gguf/overview/","title":"GGUF Format Overview","text":"<p>Understanding GGUF quantization in llcuda.</p>"},{"location":"gguf/overview/#what-is-gguf","title":"What is GGUF?","text":"<p>GGUF (GPT-Generated Unified Format): - Binary model format - Efficient quantization - Fast loading - llama.cpp native format</p>"},{"location":"gguf/overview/#quantization-types","title":"Quantization Types","text":""},{"location":"gguf/overview/#k-quants-recommended","title":"K-Quants (Recommended)","text":"<ul> <li>Q4_K_M: 4.8 bpw, best for most models</li> <li>Q5_K_M: 5.7 bpw, higher quality</li> <li>Q6_K: 6.6 bpw, near FP16</li> <li>Q8_0: 8.5 bpw, very high quality</li> </ul>"},{"location":"gguf/overview/#i-quants-compression","title":"I-Quants (Compression)","text":"<ul> <li>IQ3_XS: 3.3 bpw, for 70B models</li> <li>IQ4_XS: 4.3 bpw, better quality</li> <li>IQ2_XS: 2.3 bpw, extreme compression</li> </ul>"},{"location":"gguf/overview/#legacy","title":"Legacy","text":"<ul> <li>Q4_0: 4.5 bpw</li> <li>Q5_0: 5.5 bpw</li> </ul>"},{"location":"gguf/overview/#selection-guide","title":"Selection Guide","text":"VRAM Model Size Recommended Quant 5GB 1-3B Q4_K_M 10GB 7-8B Q4_K_M 15GB 13B Q4_K_M 30GB 70B IQ3_XS <p>See: - K-Quants Guide - I-Quants Guide - Selection Guide</p>"},{"location":"gguf/selection/","title":"GGUF Format Selection Guide","text":"<p>Choose the right quantization for your use case.</p>"},{"location":"gguf/selection/#quick-reference","title":"Quick Reference","text":"Model Size VRAM Recommended Quant Speed 1-3B 3-5 GB Q4_K_M ~60 tok/s 7-8B 6-8 GB Q4_K_M ~25 tok/s 70B (dual T4) 28 GB IQ3_XS ~12 tok/s"},{"location":"gguf/selection/#k-quants-quality-priority","title":"K-Quants (Quality Priority)","text":"<ul> <li>Q4_K_M: Best balance (recommended)</li> <li>Q5_K_M: Higher quality</li> <li>Q6_K: Excellent quality</li> <li>Q8_0: Near-FP16</li> </ul>"},{"location":"gguf/selection/#i-quants-size-priority","title":"I-Quants (Size Priority)","text":"<ul> <li>IQ3_XS: For 70B on dual T4</li> <li>IQ2_XXS: Ultra-compressed</li> </ul>"},{"location":"gguf/selection/#decision-tree","title":"Decision Tree","text":"<pre><code>Model &lt; 8B?\n\u251c\u2500 Yes \u2192 Q4_K_M (single T4)\n\u2514\u2500 No \u2192 Is it 70B?\n    \u251c\u2500 Yes \u2192 IQ3_XS (dual T4)\n    \u2514\u2500 No \u2192 Q4_K_M (dual T4)\n</code></pre>"},{"location":"gguf/selection/#see-also","title":"See Also","text":"<ul> <li>GGUF Tutorial</li> <li>VRAM Estimation</li> </ul>"},{"location":"gguf/vram-estimation/","title":"VRAM Estimation","text":"<p>Estimate VRAM requirements for GGUF models.</p>"},{"location":"gguf/vram-estimation/#formula","title":"Formula","text":"<pre><code>VRAM (GB) \u2248 Parameters (B) \u00d7 Bits / 8\n</code></pre>"},{"location":"gguf/vram-estimation/#common-models","title":"Common Models","text":"Model Quant VRAM Fits Single T4? Fits Dual T4? Gemma 2-2B Q4_K_M ~3 GB \u2705 Yes \u2705 Yes Llama-3.2-3B Q4_K_M ~4 GB \u2705 Yes \u2705 Yes Qwen-2.5-7B Q4_K_M ~7 GB \u2705 Yes \u2705 Yes Llama-70B Q4_K_M ~40 GB \u274c No \u274c No Llama-70B IQ3_XS ~28 GB \u274c No \u2705 Yes"},{"location":"gguf/vram-estimation/#python-estimation","title":"Python Estimation","text":"<pre><code>from llcuda.api.multigpu import estimate_model_vram\n\nvram = estimate_model_vram(model_size_b=7, quant_type=\"Q4_K_M\")\nprint(f\"Est. VRAM: {vram:.1f} GB\")\n</code></pre>"},{"location":"gguf/vram-estimation/#see-also","title":"See Also","text":"<ul> <li>GGUF Tutorial</li> <li>Multi-GPU</li> </ul>"},{"location":"graphistry/examples/","title":"Graphistry Examples","text":"<p>Example workflows for LLM + Graphistry visualization.</p>"},{"location":"graphistry/examples/#extract-entities-from-llm","title":"Extract Entities from LLM","text":"<pre><code>from llcuda.api.client import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.create_chat_completion(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract entities and relationships from: ...\"\n    }]\n)\n\n# Parse LLM output into entities and edges\n</code></pre>"},{"location":"graphistry/examples/#build-graph-on-gpu-1","title":"Build Graph on GPU 1","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nimport cudf\nimport graphistry\n\n# Create edges dataframe\nedges_df = cudf.DataFrame({\n    \"src\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"dst\": [\"Bob\", \"Charlie\", \"Alice\"],\n    \"relation\": [\"knows\", \"works_with\", \"manages\"]\n})\n\n# Visualize\ng = graphistry.edges(edges_df, \"src\", \"dst\")\ng.plot()\n</code></pre>"},{"location":"graphistry/examples/#full-workflow","title":"Full Workflow","text":"<p>See Split-GPU Tutorial</p>"},{"location":"graphistry/knowledge-graphs/","title":"Knowledge Graph Extraction","text":"<p>Extract knowledge graphs from LLM outputs.</p>"},{"location":"graphistry/knowledge-graphs/#workflow","title":"Workflow","text":""},{"location":"graphistry/knowledge-graphs/#1-generate-text-gpu-0","title":"1. Generate Text (GPU 0)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract entities and relationships from: ...\"\n    }]\n)\n\ntext = response.choices[0].message.content\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#2-parse-entities","title":"2. Parse Entities","text":"<pre><code>import json\n\n# Parse LLM output\ndata = json.loads(text)\nentities = data['entities']\nrelationships = data['relationships']\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#3-build-graph-gpu-1","title":"3. Build Graph (GPU 1)","text":"<pre><code>import cudf\n\nnodes_df = cudf.DataFrame(entities)\nedges_df = cudf.DataFrame(relationships)\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#4-visualize-gpu-1","title":"4. Visualize (GPU 1)","text":"<pre><code>import graphistry\n\ng = graphistry.edges(edges_df).nodes(nodes_df)\ng.plot()\n</code></pre>"},{"location":"graphistry/knowledge-graphs/#use-cases","title":"Use Cases","text":"<ul> <li>Document analysis</li> <li>Semantic networks</li> <li>Entity relationship mapping</li> <li>Knowledge base visualization</li> </ul>"},{"location":"graphistry/overview/","title":"Graphistry Integration","text":"<p>GPU-accelerated graph visualization with llcuda.</p>"},{"location":"graphistry/overview/#what-is-graphistry","title":"What is Graphistry?","text":"<p>PyGraphistry provides: - GPU-accelerated graph rendering - Millions of nodes/edges - Interactive exploration - RAPIDS integration (cuDF, cuGraph)</p>"},{"location":"graphistry/overview/#split-gpu-architecture","title":"Split-GPU Architecture","text":"<pre><code>GPU 0: llcuda (LLM)  \u2192  GPU 1: Graphistry (Viz)\n</code></pre>"},{"location":"graphistry/overview/#quick-start","title":"Quick Start","text":"<pre><code>import graphistry\nimport cudf\n\n# Configure for GPU 1\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\n# Register Graphistry\ngraphistry.register(\n    api=3,\n    protocol=\"https\",\n    server=\"hub.graphistry.com\",\n    personal_key_id=\"YOUR_KEY\"\n)\n\n# Create graph\nnodes = cudf.DataFrame({\n    'id': [1, 2, 3],\n    'label': ['A', 'B', 'C']\n})\n\nedges = cudf.DataFrame({\n    'src': [1, 2],\n    'dst': [2, 3]\n})\n\n# Visualize\ng = graphistry.edges(edges, 'src', 'dst').nodes(nodes, 'id')\ng.plot()\n</code></pre> <p>See: - Knowledge Graphs - RAPIDS Integration - Examples</p>"},{"location":"graphistry/rapids/","title":"RAPIDS Integration","text":"<p>Use RAPIDS cuDF/cuGraph on GPU 1 alongside LLM on GPU 0.</p>"},{"location":"graphistry/rapids/#overview","title":"Overview","text":"<p>RAPIDS provides GPU-accelerated dataframes and graph analytics.</p>"},{"location":"graphistry/rapids/#installation","title":"Installation","text":"<pre><code>pip install cudf-cu12 cugraph-cu12 graphistry\n</code></pre>"},{"location":"graphistry/rapids/#basic-usage","title":"Basic Usage","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use GPU 1\n\nimport cudf\nimport cugraph\n\n# Create graph\ndf = cudf.DataFrame({\"src\": [0, 1, 2], \"dst\": [1, 2, 0]})\nG = cugraph.Graph()\nG.from_cudf_edgelist(df, source=\"src\", destination=\"dst\")\n</code></pre>"},{"location":"graphistry/rapids/#with-llm","title":"With LLM","text":"<pre><code># GPU 0: LLM\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom llcuda.server import ServerManager\nserver = ServerManager()\n\n# GPU 1: RAPIDS\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport cudf\n# Process data on GPU 1\n</code></pre>"},{"location":"graphistry/rapids/#see-also","title":"See Also","text":"<ul> <li>Split-GPU Tutorial</li> <li>Graphistry Examples</li> </ul>"},{"location":"graphistry/split-gpu-setup/","title":"Split-GPU Setup","text":"<p>Configure LLM on GPU 0 and Graphistry on GPU 1.</p>"},{"location":"graphistry/split-gpu-setup/#architecture","title":"Architecture","text":"<pre><code>GPU 0: llama-server (15GB)\n  \u2193 Extract knowledge graphs\nGPU 1: RAPIDS + Graphistry (15GB)\n  \u2192 Visualize millions of nodes/edges\n</code></pre>"},{"location":"graphistry/split-gpu-setup/#setup-gpu-0-llm","title":"Setup GPU 0 (LLM)","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nfrom llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(model_path=\"model.gguf\", n_gpu_layers=99)\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"graphistry/split-gpu-setup/#setup-gpu-1-graphistry","title":"Setup GPU 1 (Graphistry)","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nimport graphistry\ngraphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\")\n</code></pre>"},{"location":"graphistry/split-gpu-setup/#workflow","title":"Workflow","text":"<ol> <li>Run LLM inference on GPU 0</li> <li>Extract entities/relations from output</li> <li>Build graph on GPU 1 with cuDF</li> <li>Visualize with Graphistry</li> </ol>"},{"location":"graphistry/split-gpu-setup/#see-also","title":"See Also","text":"<ul> <li>Split-GPU Tutorial</li> <li>Knowledge Graphs</li> </ul>"},{"location":"guides/build-from-source/","title":"Build from Source","text":"<p>Compile llama.cpp binaries for llcuda from source.</p>"},{"location":"guides/build-from-source/#prerequisites","title":"Prerequisites","text":"<ul> <li>CUDA 12.x toolkit</li> <li>CMake 3.18+</li> <li>GCC 11+</li> </ul>"},{"location":"guides/build-from-source/#clone-llamacpp","title":"Clone llama.cpp","text":"<pre><code>git clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\ngit checkout b7760\n</code></pre>"},{"location":"guides/build-from-source/#build-with-cuda","title":"Build with CUDA","text":"<pre><code>cmake -B build \\\n  -DGGML_CUDA=ON \\\n  -DGGML_CUDA_F16=ON \\\n  -DGGML_FLASH_ATTN=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=75 \\\n  -DCMAKE_BUILD_TYPE=Release\n\ncmake --build build --config Release -j$(nproc)\n</code></pre>"},{"location":"guides/build-from-source/#package-binaries","title":"Package Binaries","text":"<pre><code>mkdir -p llcuda-binaries/bin llcuda-binaries/lib\ncp build/bin/llama-* llcuda-binaries/bin/\ncp build/*.so llcuda-binaries/lib/\ntar -czf llcuda-v2.2.0-custom.tar.gz llcuda-binaries/\n</code></pre>"},{"location":"guides/build-from-source/#use-custom-binaries","title":"Use Custom Binaries","text":"<pre><code>import os\nos.environ[\"LLCUDA_BINARY_PATH\"] = \"/path/to/llcuda-binaries\"\n\nfrom llcuda.server import ServerManager\n# Will use custom binaries\n</code></pre>"},{"location":"guides/build-from-source/#see-also","title":"See Also","text":"<ul> <li>Installation Guide</li> </ul>"},{"location":"guides/faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about llcuda v2.2.0.</p>"},{"location":"guides/faq/#general-questions","title":"General Questions","text":""},{"location":"guides/faq/#what-is-llcuda","title":"What is llcuda?","text":"<p>llcuda is a Python library for fast LLM inference on NVIDIA GPUs, specifically optimized for Tesla T4. It provides:</p> <ul> <li>Pre-built CUDA binaries with FlashAttention</li> <li>One-step installation from GitHub</li> <li>134 tokens/sec on Gemma 3-1B (verified)</li> <li>Simple Python API for inference</li> <li>Auto-downloading of models and binaries</li> </ul>"},{"location":"guides/faq/#why-tesla-t4-only","title":"Why Tesla T4 only?","text":"<p>llcuda v2.2.0 is optimized exclusively for Tesla T4 (compute capability 7.5) to maximize performance:</p> <ul> <li>Tensor Core optimizations for SM 7.5</li> <li>FlashAttention tuned for Turing architecture</li> <li>Multi-GPU support for dual T4 setups (Kaggle)</li> <li>Guaranteed compatibility</li> </ul> <p>For other GPUs, use llcuda v1.2.2 which supports SM 5.0-8.9.</p>"},{"location":"guides/faq/#how-does-llcuda-compare-to-other-solutions","title":"How does llcuda compare to other solutions?","text":"Solution Speed (Gemma 3-1B) Setup Ease of Use llcuda v2.2.0 134 tok/s 1 min Excellent transformers 45 tok/s 5 min Good vLLM 85 tok/s 10 min Moderate llama.cpp CLI 128 tok/s 15 min Moderate <p>llcuda is 3x faster than PyTorch and easiest to set up.</p>"},{"location":"guides/faq/#installation","title":"Installation","text":""},{"location":"guides/faq/#how-do-i-install-llcuda","title":"How do I install llcuda?","text":"<pre><code>pip install git+https://github.com/llcuda/llcuda.git\n</code></pre> <p>Binaries auto-download on first import (~961 MB).</p>"},{"location":"guides/faq/#do-i-need-to-install-cuda-toolkit","title":"Do I need to install CUDA Toolkit?","text":"<p>No! llcuda includes all necessary CUDA binaries. You only need:</p> <ul> <li>NVIDIA driver (pre-installed in Kaggle)</li> <li>CUDA runtime (pre-installed in Kaggle)</li> <li>Python 3.11+</li> </ul>"},{"location":"guides/faq/#can-i-install-from-pypi","title":"Can I install from PyPI?","text":"<p>llcuda v2.2.0 is GitHub-only for now. Use: <pre><code>pip install git+https://github.com/llcuda/llcuda.git\n</code></pre></p>"},{"location":"guides/faq/#why-do-binaries-download-on-first-import","title":"Why do binaries download on first import?","text":"<p>To keep the pip package small (~62 KB), CUDA binaries (961 MB) download automatically on first import from GitHub Releases. This is a one-time download, then cached locally.</p>"},{"location":"guides/faq/#compatibility","title":"Compatibility","text":""},{"location":"guides/faq/#which-gpus-are-supported","title":"Which GPUs are supported?","text":"<p>llcuda v2.2.0: Tesla T4 only (SM 7.5, single or dual GPU)</p> <p>llcuda v1.2.2: All GPUs with SM 5.0+ (Maxwell through Ada Lovelace)</p>"},{"location":"guides/faq/#can-i-use-llcuda-on-cpu","title":"Can I use llcuda on CPU?","text":"<p>Yes, but not recommended. Set <code>gpu_layers=0</code> for CPU mode. Performance drops from 134 tok/s to ~8 tok/s.</p>"},{"location":"guides/faq/#does-llcuda-work-on-windows","title":"Does llcuda work on Windows?","text":"<p>llcuda v2.2.0 is Linux-only (Kaggle, Ubuntu). For Windows, compile from source or use WSL2.</p>"},{"location":"guides/faq/#what-python-versions-are-supported","title":"What Python versions are supported?","text":"<p>Python 3.11+ is required. Tested on Python 3.10, 3.11, and 3.12.</p>"},{"location":"guides/faq/#what-cuda-versions-are-supported","title":"What CUDA versions are supported?","text":"<p>CUDA 12.0+ required. Tested with CUDA 12.2, 12.4.</p>"},{"location":"guides/faq/#models","title":"Models","text":""},{"location":"guides/faq/#which-models-can-i-use","title":"Which models can I use?","text":"<p>Any GGUF model compatible with llama.cpp:</p> <ul> <li>Gemma (1B, 2B, 3B, 7B)</li> <li>Llama (3.1, 3.2, 3.3)</li> <li>Qwen (1.5B, 7B, 14B)</li> <li>Mistral (7B, 8x7B)</li> <li>Phi (2, 3)</li> </ul>"},{"location":"guides/faq/#what-quantization-should-i-use","title":"What quantization should I use?","text":"<p>Q4_K_M for best performance/quality balance on T4:</p> <ul> <li>Speed: 134 tok/s</li> <li>VRAM: 1.2 GB (Gemma 3-1B)</li> <li>Quality: &lt; 1% degradation</li> </ul> <p>Other options: - Q5_K_M: Better quality, 18% slower - Q8_0: Best quality, 44% slower</p>"},{"location":"guides/faq/#how-do-i-load-a-model-from-huggingface","title":"How do I load a model from HuggingFace?","text":"<pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"guides/faq/#can-i-use-my-fine-tuned-models","title":"Can I use my fine-tuned models?","text":"<p>Yes! Export to GGUF using Unsloth:</p> <pre><code># After fine-tuning with Unsloth\nmodel.save_pretrained_gguf(\n    \"my-model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# Load with llcuda\nengine.load_model(\"my-model-Q4_K_M.gguf\")\n</code></pre> <p>See Unsloth Integration for details.</p>"},{"location":"guides/faq/#performance","title":"Performance","text":""},{"location":"guides/faq/#what-performance-can-i-expect","title":"What performance can I expect?","text":"<p>On Tesla T4 with Q4_K_M quantization:</p> <ul> <li>Gemma 3-1B: 134 tok/s (verified)</li> <li>Llama 3.2-3B: ~48 tok/s (estimated)</li> <li>Qwen 2.5-7B: ~21 tok/s (estimated)</li> <li>Llama 3.1-8B: ~19 tok/s (estimated)</li> </ul>"},{"location":"guides/faq/#why-is-my-inference-slow","title":"Why is my inference slow?","text":"<p>Common causes:</p> <ol> <li>Not using T4: Other GPUs need v1.2.2</li> <li>Low GPU offload: Set <code>gpu_layers=99</code></li> <li>Wrong quantization: Use Q4_K_M</li> <li>Large context: Reduce <code>ctx_size</code> to 2048</li> <li>CPU mode: Check <code>nvidia-smi</code> shows GPU usage</li> </ol> <p>See Troubleshooting for solutions.</p>"},{"location":"guides/faq/#how-can-i-optimize-performance","title":"How can I optimize performance?","text":"<pre><code># Optimal configuration for T4\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,        # Full GPU offload\n    ctx_size=2048,        # Balanced context\n    batch_size=512,       # Optimal batch\n    ubatch_size=128,\n    auto_configure=True   # Let llcuda optimize\n)\n</code></pre> <p>See Performance Tutorial for details.</p>"},{"location":"guides/faq/#does-llcuda-support-batching","title":"Does llcuda support batching?","text":"<p>Yes: <pre><code>prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = engine.batch_infer(prompts, max_tokens=100)\n</code></pre></p> <p>For concurrent requests, use <code>n_parallel</code>: <pre><code>engine.load_model(\"model.gguf\", n_parallel=4)\n</code></pre></p>"},{"location":"guides/faq/#memory","title":"Memory","text":""},{"location":"guides/faq/#how-much-vram-do-i-need","title":"How much VRAM do I need?","text":"<p>Depends on model size and quantization:</p> Model Q4_K_M Q5_K_M Q8_0 1B 1.2 GB 1.5 GB 2.5 GB 3B 2.0 GB 2.4 GB 4.2 GB 7B 5.0 GB 6.0 GB 10 GB 8B 5.5 GB 6.5 GB 11 GB <p>Tesla T4 has 15 GB, sufficient for models up to 7-8B.</p>"},{"location":"guides/faq/#can-i-run-multiple-models-simultaneously","title":"Can I run multiple models simultaneously?","text":"<p>Yes, on different ports:</p> <pre><code># Model 1\nengine1 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\nengine1.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Model 2\nengine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\nengine2.load_model(\"llama-3.2-3b-Q4_K_M\")\n</code></pre> <p>Watch total VRAM usage with <code>nvidia-smi</code>.</p>"},{"location":"guides/faq/#what-if-i-run-out-of-vram","title":"What if I run out of VRAM?","text":"<ol> <li>Use smaller model (1B instead of 3B)</li> <li>Use Q4_K_M instead of Q8_0</li> <li>Reduce <code>gpu_layers</code> (e.g., 20 instead of 99)</li> <li>Reduce <code>ctx_size</code> (e.g., 1024 instead of 4096)</li> <li>Close other GPU applications</li> </ol>"},{"location":"guides/faq/#usage","title":"Usage","text":""},{"location":"guides/faq/#how-do-i-run-inference","title":"How do I run inference?","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", auto_start=True)\n\nresult = engine.infer(\"What is AI?\", max_tokens=100)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"guides/faq/#can-i-stream-outputs","title":"Can I stream outputs?","text":"<p>Yes: <pre><code>def print_chunk(text):\n    print(text, end='', flush=True)\n\nresult = engine.infer_stream(\n    \"Write a story:\",\n    callback=print_chunk,\n    max_tokens=200\n)\n</code></pre></p>"},{"location":"guides/faq/#how-do-i-stop-generation-early","title":"How do I stop generation early?","text":"<p>Use <code>stop_sequences</code>: <pre><code>result = engine.infer(\n    \"List items:\",\n    max_tokens=200,\n    stop_sequences=[\"\\n\\n\", \"###\"]\n)\n</code></pre></p>"},{"location":"guides/faq/#can-i-control-randomness","title":"Can I control randomness?","text":"<p>Yes, with <code>temperature</code> and <code>seed</code>: <pre><code># Deterministic\nresult = engine.infer(\n    \"Prompt\",\n    temperature=0.1,\n    seed=42\n)\n\n# Creative\nresult = engine.infer(\n    \"Prompt\",\n    temperature=1.0,\n    top_k=100\n)\n</code></pre></p>"},{"location":"guides/faq/#kaggle-notebooks","title":"Kaggle Notebooks","text":""},{"location":"guides/faq/#does-llcuda-work-on-kaggle","title":"Does llcuda work on Kaggle?","text":"<p>Yes! llcuda v2.2.0 is optimized for Kaggle's dual T4 GPUs:</p> <pre><code># In Kaggle notebook\n!pip install git+https://github.com/llcuda/llcuda.git\n\nfrom llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"gemma-3-1b-Q4_K_M.gguf\",\n    tensor_split=\"0.5,0.5\"  # Use both T4 GPUs\n)\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"guides/faq/#how-do-i-enable-dual-t4-gpus-in-kaggle","title":"How do I enable dual T4 GPUs in Kaggle?","text":"<p>Settings \u2192 Accelerator \u2192 GPU T4 x 2 \u2192 Internet \u2192 On</p>"},{"location":"guides/faq/#what-are-the-kaggle-session-limits","title":"What are the Kaggle session limits?","text":"<p>Kaggle provides:</p> <ul> <li>Dual Tesla T4 GPUs (15GB each = 30GB total)</li> <li>12-hour session limit</li> <li>73GB disk space</li> <li>Free internet access</li> </ul>"},{"location":"guides/faq/#can-i-save-models-between-sessions","title":"Can I save models between sessions?","text":"<p>Models cache to <code>/tmp/</code>. In Kaggle, only <code>/kaggle/working</code> persists between runs. Use:</p> <pre><code># Save to working directory\n!mkdir -p /kaggle/working/models\n!cp ~/.cache/llcuda/models/*.gguf /kaggle/working/models/\n\n# Next session: load from working directory\nfrom llcuda.server import ServerConfig\nconfig = ServerConfig(model_path=\"/kaggle/working/models/model.gguf\")\n</code></pre>"},{"location":"guides/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/faq/#import-fails-with-no-module-named-llcuda","title":"Import fails with \"No module named llcuda\"","text":"<pre><code># Reinstall\npip uninstall llcuda -y\npip install git+https://github.com/llcuda/llcuda.git\n</code></pre>"},{"location":"guides/faq/#binary-download-fails","title":"Binary download fails","text":"<p>See Troubleshooting Guide</p>"},{"location":"guides/faq/#server-wont-start","title":"Server won't start","text":"<p>Check port 8090 availability or use different port: <pre><code>engine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/faq/#performance-is-slow","title":"Performance is slow","text":"<p>See Performance Troubleshooting</p>"},{"location":"guides/faq/#contributing","title":"Contributing","text":""},{"location":"guides/faq/#can-i-contribute-to-llcuda","title":"Can I contribute to llcuda?","text":"<p>Yes! Contributions welcome:</p> <ul> <li>Bug reports: GitHub Issues</li> <li>Feature requests: Open an issue</li> <li>Code: Fork and submit PR</li> <li>Documentation: Help improve docs</li> </ul>"},{"location":"guides/faq/#how-do-i-build-binaries","title":"How do I build binaries?","text":"<p>See Build Binaries Tutorial</p>"},{"location":"guides/faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<p>Open a GitHub Issue with:</p> <ul> <li>llcuda version</li> <li>GPU model</li> <li>CUDA version</li> <li>Python version</li> <li>Error message</li> <li>Minimal reproducible code</li> </ul>"},{"location":"guides/faq/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start</li> <li>First Steps</li> <li>Troubleshooting</li> <li>Performance Optimization</li> <li>API Reference</li> </ul>"},{"location":"guides/faq/#still-have-questions","title":"Still have questions?","text":"<p>Ask on GitHub Discussions or open an issue.</p>"},{"location":"guides/first-steps/","title":"First Steps","text":"<p>Your first steps with llcuda v2.2.0 on Kaggle.</p>"},{"location":"guides/first-steps/#1-load-a-model","title":"1. Load a Model","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\n# Basic configuration\nconfig = ServerConfig(\n    model_path=\"/path/to/model.gguf\",\n    n_gpu_layers=99,  # Offload all to GPU\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"guides/first-steps/#2-make-your-first-request","title":"2. Make Your First Request","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n    ],\n    max_tokens=200\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"guides/first-steps/#3-explore-notebooks","title":"3. Explore Notebooks","text":"<p>Try the tutorial notebooks: - 01 - Quick Start - 02 - Server Setup - 03 - Multi-GPU</p>"},{"location":"guides/installation/","title":"Installation Guide","text":"<p>Complete installation guide for llcuda v2.2.0 on Kaggle dual T4 GPUs.</p>"},{"location":"guides/installation/#requirements","title":"Requirements","text":""},{"location":"guides/installation/#hardware","title":"Hardware","text":"Component Requirement GPU NVIDIA Tesla T4 (Kaggle 2\u00d7 T4) VRAM 15GB (single T4) or 30GB (dual T4) RAM 16GB+ recommended"},{"location":"guides/installation/#software","title":"Software","text":"Component Requirement Python 3.11 or higher CUDA 12.x runtime OS Linux (Ubuntu 20.04+, Kaggle) pip 23.0+"},{"location":"guides/installation/#kaggle-installation-recommended","title":"Kaggle Installation (Recommended)","text":""},{"location":"guides/installation/#step-1-configure-notebook-settings","title":"Step 1: Configure Notebook Settings","text":"<ol> <li>Go to kaggle.com/code</li> <li>Create new notebook</li> <li>Settings \u2192 Accelerator \u2192 GPU T4 \u00d7 2 \u2705</li> <li>Settings \u2192 Internet \u2192 On \u2705</li> </ol>"},{"location":"guides/installation/#step-2-install-llcuda","title":"Step 2: Install llcuda","text":"<pre><code># Install from GitHub v2.2.0\npip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/installation/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\n# Check version\nprint(f\"llcuda version: {llcuda.__version__}\")  # 2.2.0\n\n# Verify dual T4 setup\ngpus = detect_gpus()\nprint(f\"Detected {len(gpus)} GPUs\")\nprint_gpu_info()\n</code></pre> <p>Expected output: <pre><code>llcuda version: 2.2.0\nDetected 2 GPUs\n\nGPU 0: Tesla T4\n  Memory: 15.0 / 15.0 GB\n  Compute Capability: 7.5\n\nGPU 1: Tesla T4\n  Memory: 15.0 / 15.0 GB\n  Compute Capability: 7.5\n</code></pre></p>"},{"location":"guides/installation/#binary-download","title":"Binary Download","text":"<p>On first import, llcuda automatically downloads CUDA binaries:</p> <ul> <li>Size: 961 MB</li> <li>Source: GitHub Releases v2.2.0</li> <li>SHA256: Automatically verified</li> <li>Cache: <code>~/.cache/llcuda/</code></li> </ul>"},{"location":"guides/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start</li> <li>Kaggle Setup</li> <li>First Steps</li> </ul>"},{"location":"guides/kaggle-setup/","title":"Kaggle Setup Guide","text":"<p>Complete guide for setting up llcuda v2.2.0 on Kaggle with dual T4 GPUs.</p>"},{"location":"guides/kaggle-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kaggle account</li> <li>Phone verification (for GPU access)</li> </ul>"},{"location":"guides/kaggle-setup/#step-1-create-notebook","title":"Step 1: Create Notebook","text":"<ol> <li>Go to kaggle.com/code</li> <li>Click \"New Notebook\"</li> <li>Choose \"Notebook\" type</li> </ol>"},{"location":"guides/kaggle-setup/#step-2-configure-gpu","title":"Step 2: Configure GPU","text":"<ol> <li>Click Settings (gear icon)</li> <li>Accelerator \u2192 Select GPU T4 x 2</li> <li>Internet \u2192 Toggle On</li> <li>Persistence \u2192 Optional: Enable for faster startups</li> </ol>"},{"location":"guides/kaggle-setup/#step-3-install-llcuda","title":"Step 3: Install llcuda","text":"<pre><code>!pip install -q git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/kaggle-setup/#step-4-verify-setup","title":"Step 4: Verify Setup","text":"<pre><code>import llcuda\nfrom llcuda.api.multigpu import detect_gpus, print_gpu_info\n\nprint(f\"llcuda v{llcuda.__version__}\")\nprint_gpu_info()\n</code></pre>"},{"location":"guides/kaggle-setup/#step-5-test-inference","title":"Step 5: Test Inference","text":"<p>Run the Quick Start notebook to verify everything works.</p>"},{"location":"guides/kaggle-setup/#kaggle-limits","title":"Kaggle Limits","text":"<ul> <li>Session Duration: 12 hours maximum</li> <li>Disk Space: 73 GB available</li> <li>VRAM: 30 GB total (2\u00d7 15GB T4)</li> <li>Internet: Required for pip installs</li> </ul>"},{"location":"guides/kaggle-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-GPU Guide</li> <li>Tutorial Notebooks</li> </ul>"},{"location":"guides/model-selection/","title":"Model Selection Guide","text":"<p>Choose the right model and quantization for your use case with llcuda v2.1.0.</p>"},{"location":"guides/model-selection/#quick-recommendations","title":"Quick Recommendations","text":""},{"location":"guides/model-selection/#for-tesla-t4-15-gb","title":"For Tesla T4 (15 GB)","text":"Priority Model Quantization Speed VRAM Quality Speed Gemma 3-1B Q4_K_M 134 tok/s 1.2 GB Excellent Balance Llama 3.2-3B Q4_K_M 48 tok/s 2.0 GB Very good Quality Qwen 2.5-7B Q4_K_M 21 tok/s 5.0 GB Excellent"},{"location":"guides/model-selection/#for-limited-vram-8-gb","title":"For Limited VRAM (&lt; 8 GB)","text":"GPU VRAM Recommended Model Quantization Expected Speed 4 GB Gemma 3-1B Q4_0 ~140 tok/s 6 GB Gemma 3-1B Q4_K_M ~134 tok/s 8 GB Llama 3.2-3B Q4_K_M ~48 tok/s"},{"location":"guides/model-selection/#model-size-comparison","title":"Model Size Comparison","text":""},{"location":"guides/model-selection/#performance-vs-quality-trade-off","title":"Performance vs Quality Trade-off","text":"Model Family Size Params Tokens/sec (T4) VRAM Best For Gemma 3 1B 1.2B 134 1.2 GB Interactive apps, chatbots Llama 3.2 3B 3.2B 48 2.0 GB Balanced performance Qwen 2.5 7B 7.6B 21 5.0 GB Quality-focused tasks Llama 3.1 8B 8.0B 19 5.5 GB Production quality Mistral 7B 7.2B 22 5.2 GB Code generation"},{"location":"guides/model-selection/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"guides/model-selection/#1b-models-best-for-speed","title":"1B Models (Best for Speed)","text":"<p>Gemma 3-1B-it</p> <ul> <li>Speed: 134 tok/s (Q4_K_M)</li> <li>VRAM: 1.2 GB</li> <li>Strengths:</li> <li>Fastest inference</li> <li>Excellent for interactive chat</li> <li>Low VRAM requirements</li> <li>Good quality for size</li> <li>Weaknesses:</li> <li>Limited reasoning on complex tasks</li> <li>Shorter context understanding</li> <li>Use Cases:</li> <li>Customer service chatbots</li> <li>Quick Q&amp;A systems</li> <li>Real-time code assistance</li> <li>Mobile/edge deployment</li> </ul> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#3b-models-balanced","title":"3B Models (Balanced)","text":"<p>Llama 3.2-3B-Instruct</p> <ul> <li>Speed: 48 tok/s (Q4_K_M)</li> <li>VRAM: 2.0 GB</li> <li>Strengths:</li> <li>Good balance of speed/quality</li> <li>Better reasoning than 1B</li> <li>Handles complex instructions</li> <li>Still fast enough for real-time</li> <li>Weaknesses:</li> <li>3x slower than 1B models</li> <li>Higher VRAM usage</li> <li>Use Cases:</li> <li>Content generation</li> <li>Code explanation</li> <li>Document summarization</li> <li>Educational applications</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#7b-models-quality-focused","title":"7B Models (Quality-Focused)","text":"<p>Qwen 2.5-7B-Instruct</p> <ul> <li>Speed: 21 tok/s (Q4_K_M)</li> <li>VRAM: 5.0 GB</li> <li>Strengths:</li> <li>Excellent quality</li> <li>Strong reasoning abilities</li> <li>Great for complex tasks</li> <li>Multilingual support</li> <li>Weaknesses:</li> <li>6x slower than 1B</li> <li>Requires 5+ GB VRAM</li> <li>Use Cases:</li> <li>Research and analysis</li> <li>Complex reasoning tasks</li> <li>Technical documentation</li> <li>Multi-step problem solving</li> </ul> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n</code></pre> <p>Llama 3.1-8B-Instruct</p> <ul> <li>Speed: 19 tok/s (Q4_K_M)</li> <li>VRAM: 5.5 GB</li> <li>Strengths:</li> <li>State-of-the-art quality</li> <li>Excellent instruction following</li> <li>Long context support (128K)</li> <li>Multilingual</li> <li>Use Cases:</li> <li>Production applications</li> <li>API services</li> <li>Complex workflows</li> <li>Enterprise deployments</li> </ul> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#quantization-guide","title":"Quantization Guide","text":""},{"location":"guides/model-selection/#understanding-quantization-types","title":"Understanding Quantization Types","text":"Quantization Bits Speed Quality VRAM File Size Recommendation Q2_K 2.5 Fastest 85% Lowest ~30% Prototyping only Q3_K_M 3.5 Very fast 92% Very low ~40% Emergency low VRAM Q4_0 4.0 Fast 97% Low ~45% Speed priority Q4_K_M 4.5 Fast 99% Medium ~50% \u2705 Recommended Q5_K_M 5.5 Moderate 99.5% Medium-high ~60% Quality critical Q6_K 6.5 Slow 99.8% High ~70% Rarely needed Q8_0 8.0 Slower 99.95% Very high ~85% Development only F16 16.0 Slowest 100% Maximum 100% Not recommended"},{"location":"guides/model-selection/#choosing-quantization","title":"Choosing Quantization","text":"<p>For most users: <pre><code># Q4_K_M: Best overall choice\nengine.load_model(\n    \"model-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For speed-critical applications: <pre><code># Q4_0: 3-5% faster, slightly lower quality\nengine.load_model(\n    \"model-Q4_0.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For quality-critical work: <pre><code># Q5_K_M: Better quality, 20% slower\nengine.load_model(\n    \"model-Q5_K_M.gguf\",\n    silent=True\n)\n</code></pre></p> <p>For extreme VRAM constraints: <pre><code># Q3_K_M: Smallest usable quantization\nengine.load_model(\n    \"model-Q3_K_M.gguf\",\n    silent=True\n)\n</code></pre></p>"},{"location":"guides/model-selection/#popular-model-collections","title":"Popular Model Collections","text":""},{"location":"guides/model-selection/#unsloth-models-recommended","title":"Unsloth Models (Recommended)","text":"<p>Unsloth provides optimized GGUF models on HuggingFace:</p> <p>Gemma Models: <pre><code># Gemma 3-1B (Best for speed)\n\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n\n# Gemma 2-2B\n\"unsloth/gemma-2-2b-it-GGUF:gemma-2-2b-it-Q4_K_M.gguf\"\n\n# Gemma 2-9B\n\"unsloth/gemma-2-9b-it-GGUF:gemma-2-9b-it-Q4_K_M.gguf\"\n</code></pre></p> <p>Llama Models: <pre><code># Llama 3.2-1B\n\"unsloth/Llama-3.2-1B-Instruct-GGUF:Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.2-3B\n\"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\n# Llama 3.1-8B\n\"unsloth/Llama-3.1-8B-Instruct-GGUF:Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n</code></pre></p> <p>Mistral Models: <pre><code># Mistral 7B v0.3\n\"unsloth/Mistral-7B-Instruct-v0.3-GGUF:Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n\n# Mistral Nemo 12B\n\"unsloth/Mistral-Nemo-Instruct-2407-GGUF:Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#official-huggingface-models","title":"Official HuggingFace Models","text":"<p>Qwen Models: <pre><code># Qwen 2.5-7B (Excellent quality)\n\"Qwen/Qwen2.5-7B-Instruct-GGUF:qwen2.5-7b-instruct-q4_k_m.gguf\"\n\n# Qwen 2.5-14B\n\"Qwen/Qwen2.5-14B-Instruct-GGUF:qwen2.5-14b-instruct-q4_k_m.gguf\"\n</code></pre></p> <p>Phi Models: <pre><code># Phi 3.5-Mini (3.8B)\n\"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\"\n</code></pre></p>"},{"location":"guides/model-selection/#vram-requirements","title":"VRAM Requirements","text":""},{"location":"guides/model-selection/#model-size-to-vram-mapping","title":"Model Size to VRAM Mapping","text":"<p>For Q4_K_M quantization:</p> Model Size Q4_K_M VRAM Q5_K_M VRAM Q8_0 VRAM ctx=2048 1B 1.2 GB 1.5 GB 2.5 GB Add +0.3 GB 3B 2.0 GB 2.4 GB 4.2 GB Add +0.3 GB 7B 5.0 GB 6.2 GB 9.5 GB Add +0.5 GB 8B 5.5 GB 6.8 GB 10.2 GB Add +0.5 GB 13B 9.0 GB 11.0 GB 16.5 GB Add +0.8 GB"},{"location":"guides/model-selection/#gpu-recommendations","title":"GPU Recommendations","text":"GPU VRAM Max Model (Q4_K_M) Recommended Model Tesla T4 15 GB 7B 1B (speed) or 7B (quality) RTX 3060 12 GB 7B 3B RTX 3070 8 GB 3B 1B RTX 3080 10 GB 7B 3B RTX 3090 24 GB 13B 7B RTX 4070 12 GB 7B 3B RTX 4090 24 GB 13B 7B or 13B A100 40 GB 30B 13B A100 80 GB 70B 30B"},{"location":"guides/model-selection/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"guides/model-selection/#interactive-chatbots","title":"Interactive Chatbots","text":"<p>Priority: Speed, low latency</p> <p>Recommended: - Gemma 3-1B Q4_K_M (134 tok/s) - Llama 3.2-1B Q4_K_M (140 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#code-generation","title":"Code Generation","text":"<p>Priority: Accuracy, context understanding</p> <p>Recommended: - Qwen 2.5-7B Q4_K_M (21 tok/s) - Llama 3.1-8B Q4_K_M (19 tok/s)</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    ctx_size=4096,  # Longer context for code\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#document-summarization","title":"Document Summarization","text":"<p>Priority: Context length, quality</p> <p>Recommended: - Llama 3.1-8B Q4_K_M (128K context) - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.1-8B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=8192,  # Long documents\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#question-answering","title":"Question Answering","text":"<p>Priority: Accuracy, speed</p> <p>Recommended: - Llama 3.2-3B Q4_K_M (48 tok/s) - Gemma 3-1B Q4_K_M (134 tok/s)</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#content-generation","title":"Content Generation","text":"<p>Priority: Creativity, quality</p> <p>Recommended: - Qwen 2.5-7B Q5_K_M - Llama 3.1-8B Q5_K_M</p> <pre><code>engine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q5_K_M\",\n    ctx_size=4096,\n    silent=True\n)\n\n# Use creative generation settings\nresult = engine.infer(\n    prompt,\n    temperature=1.0,\n    top_p=0.95,\n    max_tokens=500\n)\n</code></pre>"},{"location":"guides/model-selection/#education-tutoring","title":"Education &amp; Tutoring","text":"<p>Priority: Accuracy, explanations</p> <p>Recommended: - Llama 3.2-3B Q4_K_M - Qwen 2.5-7B Q4_K_M</p> <pre><code>engine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    ctx_size=2048,\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-capabilities","title":"Model Capabilities","text":""},{"location":"guides/model-selection/#multilingual-support","title":"Multilingual Support","text":"Model Languages Notes Gemma 3-1B English primarily Limited multilingual Llama 3.2-3B 8 languages Good multilingual Llama 3.1-8B 8 languages Excellent multilingual Qwen 2.5-7B 29 languages Best multilingual Mistral 7B English, French, German, Spanish, Italian Good European languages"},{"location":"guides/model-selection/#context-window-support","title":"Context Window Support","text":"Model Standard Context Max Context Notes Gemma 3-1B 2K 8K Limited long context Llama 3.2-3B 4K 128K Excellent long context Llama 3.1-8B 8K 128K Best long context Qwen 2.5-7B 8K 32K Good long context Mistral 7B 8K 32K Good long context"},{"location":"guides/model-selection/#special-capabilities","title":"Special Capabilities","text":"Model Code Math Reasoning Function Calling Gemma 3-1B Good Fair Fair No Llama 3.2-3B Very Good Good Good Yes Llama 3.1-8B Excellent Very Good Excellent Yes Qwen 2.5-7B Excellent Excellent Excellent Yes Mistral 7B Very Good Good Good Yes"},{"location":"guides/model-selection/#finding-and-loading-models","title":"Finding and Loading Models","text":""},{"location":"guides/model-selection/#from-unsloth-recommended","title":"From Unsloth (Recommended)","text":"<pre><code># Browse models at: https://huggingface.co/unsloth\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-official-repos","title":"From Official Repos","text":"<pre><code># Qwen\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# Microsoft Phi\nengine.load_model(\n    \"microsoft/Phi-3.5-mini-instruct-gguf:Phi-3.5-mini-instruct-Q4_K_M.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#local-models","title":"Local Models","text":"<pre><code># Load from local path\nengine.load_model(\n    \"/path/to/model.gguf\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#model-evaluation","title":"Model Evaluation","text":""},{"location":"guides/model-selection/#quick-quality-test","title":"Quick Quality Test","text":"<pre><code>import llcuda\n\ndef evaluate_model(model_path):\n    \"\"\"Quick quality evaluation.\"\"\"\n\n    engine = llcuda.InferenceEngine()\n    engine.load_model(model_path, silent=True)\n\n    test_prompts = [\n        \"Explain quantum computing in simple terms.\",\n        \"Write a Python function to calculate factorial.\",\n        \"What are the causes of climate change?\",\n        \"Translate 'Hello, how are you?' to Spanish.\",\n        \"Solve: If x + 5 = 12, what is x?\"\n    ]\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating: {model_path}\")\n    print(f\"{'='*60}\\n\")\n\n    for i, prompt in enumerate(test_prompts, 1):\n        result = engine.infer(prompt, max_tokens=150)\n\n        print(f\"{i}. {prompt}\")\n        print(f\"   Response: {result.text[:100]}...\")\n        print(f\"   Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n\n    metrics = engine.get_metrics()\n    print(f\"Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tok/s\")\n    print(f\"Average latency: {metrics['latency']['mean_ms']:.0f}ms\")\n\n# Test multiple models\nmodels = [\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n]\n\nfor model in models:\n    evaluate_model(model)\n</code></pre>"},{"location":"guides/model-selection/#migration-guide","title":"Migration Guide","text":""},{"location":"guides/model-selection/#from-larger-to-smaller-models","title":"From Larger to Smaller Models","text":"<p>If you need to reduce VRAM:</p> <pre><code># Before: 7B model (5 GB VRAM)\nengine.load_model(\n    \"Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M\",\n    silent=True\n)\n\n# After: 3B model (2 GB VRAM)\nengine.load_model(\n    \"unsloth/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\",\n    silent=True\n)\n</code></pre>"},{"location":"guides/model-selection/#from-higher-to-lower-quantization","title":"From Higher to Lower Quantization","text":"<pre><code># Before: Q5_K_M (better quality, slower)\nengine.load_model(\"model-Q5_K_M.gguf\", silent=True)\n\n# After: Q4_K_M (faster, minimal quality loss)\nengine.load_model(\"model-Q4_K_M.gguf\", silent=True)\n</code></pre>"},{"location":"guides/model-selection/#see-also","title":"See Also","text":"<ul> <li>GGUF Format - Understanding GGUF</li> <li>Performance Benchmarks - Speed comparisons</li> <li>Optimization Guide - Tuning performance</li> <li>Quick Start - Getting started</li> <li>HuggingFace Models - Browse GGUF models</li> </ul>"},{"location":"guides/quickstart/","title":"Quick Start Guide","text":"<p>Get llcuda v2.2.0 running on Kaggle in 5 minutes!</p>"},{"location":"guides/quickstart/#step-1-install-1-minute","title":"Step 1: Install (1 minute)","text":"<pre><code>pip install git+https://github.com/llcuda/llcuda.git@v2.2.0\n</code></pre>"},{"location":"guides/quickstart/#step-2-verify-dual-t4-30-seconds","title":"Step 2: Verify Dual T4 (30 seconds)","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nprint(f\"\u2713 Detected {len(gpus)} GPUs\")\nfor gpu in gpus:\n    print(f\"  GPU {gpu.id}: {gpu.name} ({gpu.memory_total_gb:.1f} GB)\")\n</code></pre>"},{"location":"guides/quickstart/#step-3-start-server-2-minutes","title":"Step 3: Start Server (2 minutes)","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\nserver.wait_until_ready()\n\nprint(\"\u2713 Server running at http://localhost:8080\")\n</code></pre>"},{"location":"guides/quickstart/#step-4-run-inference-1-minute","title":"Step 4: Run Inference (1 minute)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient(\"http://localhost:8080\")\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"guides/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial Notebooks</li> <li>Multi-GPU Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Solutions to common issues with llcuda v2.2.0 on Tesla T4 GPUs.</p>"},{"location":"guides/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"guides/troubleshooting/#pip-install-fails","title":"pip install fails","text":"<p>Symptom: <pre><code>ERROR: Could not find a version that satisfies the requirement llcuda\n</code></pre></p> <p>Solution: <pre><code># Install from GitHub (not PyPI for v2.2.0)\npip install git+https://github.com/llcuda/llcuda.git\n\n# Or use specific release\npip install https://github.com/llcuda/llcuda/releases/download/v2.2.0/llcuda-2.2.0-py3-none-any.whl\n</code></pre></p>"},{"location":"guides/troubleshooting/#binary-download-fails","title":"Binary download fails","text":"<p>Symptom: <pre><code>Failed to download CUDA binaries: HTTP 404\n</code></pre></p> <p>Solution: <pre><code># Manually download binaries\nimport requests\nimport tarfile\nfrom pathlib import Path\n\nurl = \"https://github.com/llcuda/llcuda/releases/download/v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\"\ncache_dir = Path.home() / \".cache\" / \"llcuda\"\ncache_dir.mkdir(parents=True, exist_ok=True)\n\n# Download\nresponse = requests.get(url)\ntar_path = cache_dir / \"binaries.tar.gz\"\ntar_path.write_bytes(response.content)\n\n# Extract\nwith tarfile.open(tar_path, 'r:gz') as tar:\n    tar.extractall(cache_dir)\n</code></pre></p>"},{"location":"guides/troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"guides/troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p>Symptom: <pre><code>CUDA not available\nNo CUDA GPU detected\n</code></pre></p> <p>Solution: <pre><code># Check NVIDIA driver\nnvidia-smi\n\n# If fails in Kaggle, verify accelerator type\n# Settings &gt; Accelerator &gt; GPU T4 x 2\n\n# Verify CUDA version\nnvcc --version  # Should show CUDA 12.x\n</code></pre></p>"},{"location":"guides/troubleshooting/#wrong-gpu-detected","title":"Wrong GPU detected","text":"<p>Symptom: <pre><code>Your GPU is not Tesla T4\nGPU: Tesla P100 (SM 6.0)\n</code></pre></p> <p>Solution: llcuda v2.2.0 is optimized for Kaggle dual Tesla T4. For other GPUs, compatibility may vary.</p>"},{"location":"guides/troubleshooting/#model-loading-issues","title":"Model Loading Issues","text":""},{"location":"guides/troubleshooting/#model-not-found","title":"Model not found","text":"<p>Symptom: <pre><code>FileNotFoundError: Model file not found: gemma-3-1b-Q4_K_M\n</code></pre></p> <p>Solution: <pre><code># Use full HuggingFace path\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\n)\n\n# Or download manually\nfrom llcuda.models import download_model\nmodel_path = download_model(\n    \"unsloth/gemma-3-1b-it-GGUF\",\n    \"gemma-3-1b-it-Q4_K_M.gguf\"\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#out-of-memory","title":"Out of memory","text":"<p>Symptom: <pre><code>CUDA out of memory\nFailed to allocate tensor\n</code></pre></p> <p>Solution: <pre><code># Reduce GPU layers\nengine.load_model(\"model.gguf\", gpu_layers=20)\n\n# Reduce context size\nengine.load_model(\"model.gguf\", ctx_size=1024)\n\n# Use smaller quantization\n# Q4_K_M instead of Q8_0\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-issues","title":"Server Issues","text":""},{"location":"guides/troubleshooting/#server-wont-start","title":"Server won't start","text":"<p>Symptom: <pre><code>RuntimeError: Failed to start llama-server\n</code></pre></p> <p>Solution: <pre><code># Check if port is in use\nimport socket\nsock = socket.socket()\ntry:\n    sock.bind(('127.0.0.1', 8090))\n    print(\"Port 8090 is free\")\nexcept:\n    print(\"Port 8090 is in use - trying different port\")\nsock.close()\n\n# Use different port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p>"},{"location":"guides/troubleshooting/#server-crashes","title":"Server crashes","text":"<p>Symptom: <pre><code>llama-server process died unexpectedly\n</code></pre></p> <p>Solution: <pre><code># Run without silent mode to see errors\nengine.load_model(\"model.gguf\", silent=False, verbose=True)\n\n# Try reducing memory usage\nengine.load_model(\n    \"model.gguf\",\n    gpu_layers=20,\n    ctx_size=1024\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#slow-inference-50-toks","title":"Slow inference (&lt;50 tok/s)","text":"<p>Solutions: <pre><code># 1. Increase GPU offload\nengine.load_model(\"model.gguf\", gpu_layers=99)\n\n# 2. Use Q4_K_M quantization\nengine.load_model(\"model-Q4_K_M.gguf\")\n\n# 3. Reduce context\nengine.load_model(\"model.gguf\", ctx_size=2048)\n\n# 4. Check GPU usage\n!nvidia-smi  # Should show 80%+ GPU utilization\n</code></pre></p>"},{"location":"guides/troubleshooting/#high-latency-2000ms","title":"High latency (&gt;2000ms)","text":"<p>Solution: <pre><code># Reduce max_tokens\nresult = engine.infer(\"Prompt\", max_tokens=50)\n\n# Use smaller model (Gemma 3-1B instead of Llama 3.1-8B)\n\n# Optimize parameters\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=99,\n    ctx_size=1024,\n    batch_size=512\n)\n</code></pre></p>"},{"location":"guides/troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"guides/troubleshooting/#binaries-not-found","title":"\"Binaries not found\"","text":"<pre><code># Reinstall with cache clear\npip uninstall llcuda -y\npip cache purge\npip install git+https://github.com/llcuda/llcuda.git --no-cache-dir\n</code></pre>"},{"location":"guides/troubleshooting/#ld_library_path-not-set","title":"\"LD_LIBRARY_PATH not set\"","text":"<pre><code>import os\nfrom pathlib import Path\n\n# Manually set library path\nlib_dir = Path.home() / \".cache\" / \"llcuda\" / \"lib\"\nos.environ[\"LD_LIBRARY_PATH\"] = f\"{lib_dir}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n</code></pre>"},{"location":"guides/troubleshooting/#cuda-version-mismatch","title":"\"CUDA version mismatch\"","text":"<pre><code># Check CUDA version\nnvcc --version\nnvidia-smi  # Look for \"CUDA Version\"\n\n# llcuda requires CUDA 12.0+\n# Kaggle has CUDA 12.2+ by default\n</code></pre>"},{"location":"guides/troubleshooting/#kaggle-specific","title":"Kaggle Specific","text":""},{"location":"guides/troubleshooting/#t4-gpus-not-available","title":"T4 GPUs not available","text":"<p>Solution: - In Kaggle: Settings &gt; Accelerator &gt; GPU T4 x 2 - Enable Internet access: Settings &gt; Internet &gt; On - Dual T4 GPUs are always available on Kaggle (free tier)</p>"},{"location":"guides/troubleshooting/#session-disconnects-after-12-hours","title":"Session disconnects after 12 hours","text":"<p>Solution: Kaggle has a 12-hour maximum session limit. Save your work to <code>/kaggle/working</code> which persists between sessions.</p>"},{"location":"guides/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"model.gguf\", verbose=True, silent=False)\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li> <p>Check error details: <pre><code>result = engine.infer(\"test\", max_tokens=10)\nif not result.success:\n    print(f\"Error: {result.error_message}\")\n</code></pre></p> </li> <li> <p>GitHub Issues: github.com/llcuda/llcuda/issues</p> </li> <li> <p>Include in bug reports:</p> </li> <li>llcuda version (<code>llcuda.__version__</code>)</li> <li>GPU model (<code>nvidia-smi</code>)</li> <li>CUDA version (<code>nvcc --version</code>)</li> <li>Python version (<code>python --version</code>)</li> <li>Full error message</li> <li>Minimal reproducible code</li> </ol>"},{"location":"guides/troubleshooting/#quick-fixes-checklist","title":"Quick Fixes Checklist","text":"<ul> <li> GPU is Tesla T4 (check with <code>nvidia-smi</code>)</li> <li> CUDA 12.0+ installed (check with <code>nvcc --version</code>)</li> <li> Latest llcuda from GitHub (<code>pip install git+https://github.com/llcuda/llcuda.git</code>)</li> <li> Model exists and is accessible</li> <li> Port 8090 is available</li> <li> Sufficient VRAM for model</li> <li> Using Q4_K_M quantization</li> <li> gpu_layers=99 for full offload</li> </ul>"},{"location":"guides/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>FAQ - Frequently asked questions</li> <li>Performance Optimization - Speed up inference</li> <li>First Steps - Getting started guide</li> <li>GitHub Issues - Report bugs</li> </ul>"},{"location":"kaggle/dual-gpu-setup/","title":"Dual GPU Setup","text":"<p>Configure both T4 GPUs on Kaggle for llcuda.</p>"},{"location":"kaggle/dual-gpu-setup/#enable-dual-t4","title":"Enable Dual T4","text":"<ol> <li>Settings \u2192 Accelerator \u2192 GPU T4 \u00d7 2</li> <li>Settings \u2192 Internet \u2192 On</li> </ol>"},{"location":"kaggle/dual-gpu-setup/#verify-setup","title":"Verify Setup","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nassert len(gpus) == 2, \"Need 2 GPUs!\"\nprint(f\"\u2713 {len(gpus)} T4 GPUs detected\")\n</code></pre>"},{"location":"kaggle/dual-gpu-setup/#gpu-assignment","title":"GPU Assignment","text":"<ul> <li>GPU 0: Primary for LLM inference</li> <li>GPU 1: Secondary for tensor-split OR Graphistry</li> </ul> <p>See: Split-GPU Architecture</p>"},{"location":"kaggle/large-models/","title":"Running 70B Models","text":"<p>Run 70B parameter models on Kaggle dual T4 (30GB VRAM).</p>"},{"location":"kaggle/large-models/#requirements","title":"Requirements","text":"<ul> <li>Quantization: IQ3_XS (3.3 bpw)</li> <li>VRAM: ~25-27 GB</li> <li>Strategy: Dual T4 tensor-split</li> </ul>"},{"location":"kaggle/large-models/#configuration","title":"Configuration","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"llama-70b-IQ3_XS.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.48,0.48\",  # Leave headroom\n    context_size=2048,          # Smaller context\n    batch_size=128,             # Smaller batch\n    flash_attn=True,\n)\n</code></pre>"},{"location":"kaggle/large-models/#performance","title":"Performance","text":"<ul> <li>Speed: ~8-12 tokens/sec</li> <li>Quality: Good with IQ3_XS</li> <li>VRAM: ~27 GB used</li> </ul> <p>See: Tutorial 09 - Large Models</p>"},{"location":"kaggle/multi-gpu-inference/","title":"Multi-GPU Inference","text":"<p>Run models across both T4 GPUs with tensor-split.</p>"},{"location":"kaggle/multi-gpu-inference/#basic-multi-gpu","title":"Basic Multi-GPU","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Equal split\n    split_mode=\"layer\",\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"kaggle/multi-gpu-inference/#kaggle-preset","title":"Kaggle Preset","text":"<pre><code>from llcuda.api.multigpu import kaggle_t4_dual_config\n\nconfig = kaggle_t4_dual_config(model_size_gb=25)\nprint(config.to_cli_args())\n</code></pre>"},{"location":"kaggle/multi-gpu-inference/#performance","title":"Performance","text":"Model Tokens/sec Gemma 2-2B Q4_K_M ~60 tok/s Qwen2.5-7B Q4_K_M ~35 tok/s Llama-70B IQ3_XS ~12 tok/s"},{"location":"kaggle/overview/","title":"Kaggle Dual T4 Overview","text":"<p>llcuda v2.2.0 is optimized for Kaggle's dual Tesla T4 GPU environment.</p>"},{"location":"kaggle/overview/#hardware-specs","title":"Hardware Specs","text":"<ul> <li>2\u00d7 NVIDIA Tesla T4</li> <li>30GB total VRAM (15GB each)</li> <li>SM 7.5 (Turing architecture)</li> <li>FlashAttention support</li> </ul>"},{"location":"kaggle/overview/#what-you-can-run","title":"What You Can Run","text":"Model Size Quantization Strategy 1-13B Q4_K_M Single T4 32-34B Q4_K_M Dual T4 tensor-split 70B IQ3_XS Dual T4 tensor-split <p>See: Multi-GPU Inference</p>"},{"location":"kaggle/tensor-split/","title":"Tensor Split Configuration","text":"<p>Understand tensor-split for dual T4 inference.</p>"},{"location":"kaggle/tensor-split/#what-is-tensor-split","title":"What is Tensor Split?","text":"<p>Native CUDA mechanism to split model layers across GPUs.</p> <p>NOT NCCL - llama.cpp uses native CUDA, not NCCL.</p>"},{"location":"kaggle/tensor-split/#configuration","title":"Configuration","text":"<pre><code>config = ServerConfig(\n    tensor_split=\"0.5,0.5\",  # 50% GPU 0, 50% GPU 1\n    split_mode=\"layer\",       # Split by layers\n)\n</code></pre>"},{"location":"kaggle/tensor-split/#split-modes","title":"Split Modes","text":"<ul> <li>layer: Split layers across GPUs (recommended)</li> <li>row: Split tensor rows (requires special support)</li> </ul>"},{"location":"kaggle/tensor-split/#when-to-use","title":"When to Use","text":"<ul> <li>Models &gt; 15GB (won't fit single T4)</li> <li>32B+ models with Q4_K_M</li> <li>70B models with IQ3_XS</li> </ul>"},{"location":"performance/benchmarks/","title":"Performance Benchmarks","text":"<p>Real-world performance on Kaggle dual T4.</p>"},{"location":"performance/benchmarks/#single-gpu-results","title":"Single GPU Results","text":"Model Quant GPU Tokens/sec VRAM Gemma 3-1B Q4_K_M 1\u00d7 T4 ~45 tok/s 3 GB Qwen2.5-1.5B Q4_K_M 1\u00d7 T4 ~50 tok/s 2.5 GB Llama-3.2-3B Q4_K_M 1\u00d7 T4 ~30 tok/s 4 GB"},{"location":"performance/benchmarks/#dual-gpu-results","title":"Dual GPU Results","text":"Model Quant GPUs Tokens/sec VRAM Gemma 2-2B Q4_K_M 2\u00d7 T4 ~60 tok/s 4 GB Qwen2.5-7B Q4_K_M 2\u00d7 T4 ~35 tok/s 10 GB Llama-70B IQ3_XS 2\u00d7 T4 ~12 tok/s 27 GB"},{"location":"performance/benchmarks/#optimization-impact","title":"Optimization Impact","text":"Optimization Speedup FlashAttention 2-3x Tensor Cores 1.5x CUDA Graphs 1.2x"},{"location":"performance/dual-t4-results/","title":"Dual T4 Performance","text":"<p>Detailed benchmarks for Kaggle dual T4 setup.</p>"},{"location":"performance/dual-t4-results/#configuration","title":"Configuration","text":"<ul> <li>GPUs: 2\u00d7 Tesla T4 (15GB each)</li> <li>CUDA: 12.5</li> <li>Driver: 535.104.05</li> <li>FlashAttention: Enabled</li> </ul>"},{"location":"performance/dual-t4-results/#measured-performance","title":"Measured Performance","text":""},{"location":"performance/dual-t4-results/#gemma-2-2b-q4_k_m","title":"Gemma 2-2B (Q4_K_M)","text":"<ul> <li>Tokens/sec: 58-62</li> <li>Latency: ~16ms/token</li> <li>VRAM: 4.2 GB total</li> <li>Strategy: tensor-split 0.5,0.5</li> </ul>"},{"location":"performance/dual-t4-results/#qwen25-7b-q4_k_m","title":"Qwen2.5-7B (Q4_K_M)","text":"<ul> <li>Tokens/sec: 33-37</li> <li>Latency: ~28ms/token  </li> <li>VRAM: 10.1 GB total</li> <li>Strategy: tensor-split 0.5,0.5</li> </ul>"},{"location":"performance/dual-t4-results/#llama-70b-iq3_xs","title":"Llama-70B (IQ3_XS)","text":"<ul> <li>Tokens/sec: 10-14</li> <li>Latency: ~80ms/token</li> <li>VRAM: 26.8 GB total</li> <li>Strategy: tensor-split 0.48,0.48</li> </ul>"},{"location":"performance/dual-t4-results/#tuning-tips","title":"Tuning Tips","text":"<ol> <li>Enable FlashAttention</li> <li>Use optimal batch size</li> <li>Adjust tensor-split ratios</li> <li>Monitor VRAM usage</li> </ol>"},{"location":"performance/flash-attention/","title":"FlashAttention","text":"<p>FlashAttention v2 optimization in llcuda.</p>"},{"location":"performance/flash-attention/#what-is-flashattention","title":"What is FlashAttention?","text":"<p>Memory-efficient attention algorithm: - 2-3x faster than standard attention - Lower memory usage - Exact (not approximate)</p>"},{"location":"performance/flash-attention/#enable-in-llcuda","title":"Enable in llcuda","text":"<pre><code>config = ServerConfig(\n    flash_attn=True,  # Enable FlashAttention\n)\n</code></pre>"},{"location":"performance/flash-attention/#supported","title":"Supported","text":"<ul> <li>\u2705 All quantization types</li> <li>\u2705 All context sizes</li> <li>\u2705 Both GPUs (tensor-split)</li> </ul>"},{"location":"performance/flash-attention/#performance-impact","title":"Performance Impact","text":"Model Without FA With FA Speedup 7B ~15 tok/s ~35 tok/s 2.3x 13B ~8 tok/s ~18 tok/s 2.3x 70B ~5 tok/s ~12 tok/s 2.4x"},{"location":"performance/flash-attention/#requirements","title":"Requirements","text":"<ul> <li>SM 7.5+ (Tesla T4 \u2705)</li> <li>CUDA 12.x</li> <li>Built with <code>-DGGML_CUDA_FA_ALL_QUANTS=ON</code></li> </ul>"},{"location":"performance/memory/","title":"Memory Management","text":"<p>Manage VRAM efficiently on dual T4.</p>"},{"location":"performance/memory/#vram-budget","title":"VRAM Budget","text":"<p>Total: 30 GB (2\u00d7 15GB T4) Usable: ~28 GB (leave headroom)</p>"},{"location":"performance/memory/#allocation-strategy","title":"Allocation Strategy","text":""},{"location":"performance/memory/#single-gpu-15gb","title":"Single GPU (15GB)","text":"<ul> <li>Model: 8-12 GB</li> <li>KV Cache: 2-4 GB</li> <li>Overhead: 1-2 GB</li> </ul>"},{"location":"performance/memory/#dual-gpu-30gb","title":"Dual GPU (30GB)","text":"<ul> <li>Model: 20-26 GB</li> <li>KV Cache: 2-4 GB</li> <li>Overhead: 2-3 GB</li> </ul>"},{"location":"performance/memory/#reduce-memory-usage","title":"Reduce Memory Usage","text":"<ol> <li>Smaller Quantization</li> <li> <p>Q4_K_M \u2192 IQ3_XS</p> </li> <li> <p>Smaller Context</p> </li> <li> <p>8192 \u2192 2048 tokens</p> </li> <li> <p>Smaller Batch</p> </li> <li> <p>2048 \u2192 512</p> </li> <li> <p>No KV Offload <pre><code>config = ServerConfig(no_kv_offload=True)\n</code></pre></p> </li> </ol>"},{"location":"performance/optimization/","title":"Optimization Guide","text":"<p>Optimize llcuda performance on Kaggle.</p>"},{"location":"performance/optimization/#1-enable-flashattention","title":"1. Enable FlashAttention","text":"<pre><code>config = ServerConfig(\n    flash_attn=True,  # 2-3x speedup\n)\n</code></pre>"},{"location":"performance/optimization/#2-optimize-batch-size","title":"2. Optimize Batch Size","text":"<pre><code>config = ServerConfig(\n    batch_size=2048,   # Larger for throughput\n    ubatch_size=512,   # Smaller for latency\n)\n</code></pre>"},{"location":"performance/optimization/#3-tune-context-size","title":"3. Tune Context Size","text":"<pre><code># Smaller context = faster\nconfig = ServerConfig(\n    context_size=2048,  # vs 8192\n)\n</code></pre>"},{"location":"performance/optimization/#4-use-k-quants","title":"4. Use K-Quants","text":"<ul> <li>Q4_K_M: Best balance</li> <li>Q5_K_M: Higher quality</li> <li>IQ3_XS: For 70B models</li> </ul>"},{"location":"performance/optimization/#5-monitor-vram","title":"5. Monitor VRAM","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nfor gpu in gpus:\n    print(f\"GPU {gpu.id}: {gpu.memory_used_gb:.1f} / {gpu.memory_total_gb:.1f} GB\")\n</code></pre>"},{"location":"tutorials/","title":"Tutorial Notebooks","text":"<p>Complete tutorial series for llcuda v2.2.0 on Kaggle dual T4.</p> # Notebook Open in Kaggle Description Time 01 Quick Start 5-minute introduction 5 min 02 Server Setup Server configuration 15 min 03 Multi-GPU Dual T4 tensor-split 20 min 04 GGUF Quantization K-quants, I-quants 20 min 05 Unsloth Integration Fine-tune \u2192 Deploy 30 min 06 Split-GPU + Graphistry LLM + Visualization 30 min 07 OpenAI API OpenAI SDK 15 min 08 NCCL + PyTorch Distributed PyTorch 25 min 09 Large Models (70B) 70B on dual T4 30 min 10 Complete Workflow End-to-end 45 min"},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":""},{"location":"tutorials/#beginner-1-hour","title":"Beginner (1 hour)","text":"<p>01 \u2192 02 \u2192 03</p>"},{"location":"tutorials/#intermediate-3-hours","title":"Intermediate (3 hours)","text":"<p>01 \u2192 02 \u2192 03 \u2192 04 \u2192 05 \u2192 06 \u2192 07 \u2192 10</p>"},{"location":"tutorials/#advanced-2-hours","title":"Advanced (2 hours)","text":"<p>01 \u2192 03 \u2192 08 \u2192 09</p>"},{"location":"tutorials/01-quickstart/","title":"Quick Start","text":"<p>Get started with llcuda v2.2.0 in 5 minutes on Kaggle dual T4 GPUs.</p> <p>Level: Beginner | Time: 5 minutes | VRAM Required: 3-5 GB (single T4)</p>"},{"location":"tutorials/01-quickstart/#overview","title":"Overview","text":"<p>This tutorial covers the essentials:</p> <ul> <li>Installing llcuda v2.2.0</li> <li>Downloading a GGUF model</li> <li>Starting the llama-server</li> <li>Making your first chat completion</li> <li>Cleaning up resources</li> </ul>"},{"location":"tutorials/01-quickstart/#step-1-install-llcuda","title":"Step 1: Install llcuda","text":"<pre><code>pip install llcuda\n</code></pre>"},{"location":"tutorials/01-quickstart/#step-2-check-gpus","title":"Step 2: Check GPUs","text":"<pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\nprint(f\"GPU 0: {torch.cuda.get_device_name(0)}\")\nif torch.cuda.device_count() &gt; 1:\n    print(f\"GPU 1: {torch.cuda.get_device_name(1)}\")\n</code></pre>"},{"location":"tutorials/01-quickstart/#step-3-download-model","title":"Step 3: Download Model","text":"<pre><code>from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(\n    repo_id=\"unsloth/gemma-2-2b-it-GGUF\",\n    filename=\"gemma-2-2b-it-Q4_K_M.gguf\"\n)\n</code></pre>"},{"location":"tutorials/01-quickstart/#step-4-start-server","title":"Step 4: Start Server","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=model_path,\n    n_gpu_layers=99,\n    flash_attn=True\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"tutorials/01-quickstart/#step-5-make-request","title":"Step 5: Make Request","text":"<pre><code>from llcuda.api.client import LlamaCppClient\n\nclient = LlamaCppClient(base_url=\"http://localhost:8080\")\nresponse = client.create_chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    max_tokens=200\n)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"tutorials/01-quickstart/#step-6-cleanup","title":"Step 6: Cleanup","text":"<pre><code>server.stop()\n</code></pre>"},{"location":"tutorials/01-quickstart/#expected-performance","title":"Expected Performance","text":"<ul> <li>Speed: ~60 tokens/sec (Gemma 2-2B Q4_K_M)</li> <li>Latency: ~500ms</li> <li>VRAM: ~3-4 GB</li> </ul>"},{"location":"tutorials/01-quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>02 - Server Setup</li> <li>03 - Multi-GPU Inference</li> </ul>"},{"location":"tutorials/01-quickstart/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/02-server-setup/","title":"Server Setup","text":"<p>Deep dive into llama-server configuration and lifecycle management.</p> <p>Level: Beginner | Time: 15 minutes | VRAM Required: 5-8 GB (single T4)</p>"},{"location":"tutorials/02-server-setup/#serverconfig-parameters","title":"ServerConfig Parameters","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    context_size=4096,\n    n_batch=2048,\n    flash_attn=True,\n    tensor_split=None,  # Single GPU\n    host=\"127.0.0.1\",\n    port=8080\n)\n</code></pre>"},{"location":"tutorials/02-server-setup/#server-lifecycle","title":"Server Lifecycle","text":"<pre><code>from llcuda.server import ServerManager\n\nserver = ServerManager()\n\n# Start\nserver.start_with_config(config)\n\n# Check status\nprint(f\"Running: {server.is_running()}\")\nprint(f\"URL: {server.get_base_url()}\")\n\n# Wait for ready\nserver.wait_until_ready(timeout=30)\n\n# Get logs\nlogs = server.get_logs()\n\n# Stop\nserver.stop()\n</code></pre>"},{"location":"tutorials/02-server-setup/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":"<pre><code>config = ServerConfig(\n    model_path=\"model.gguf\",\n    tensor_split=\"0.5,0.5\",  # 50/50 split\n    split_mode=\"layer\",\n    n_gpu_layers=99,\n    flash_attn=True\n)\n</code></pre>"},{"location":"tutorials/02-server-setup/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/03-multi-gpu/","title":"Multi-GPU Inference","text":"<p>Use both Kaggle T4 GPUs with tensor-split for larger models.</p> <p>Level: Beginner | Time: 20 minutes | VRAM Required: 15-25 GB (dual T4)</p>"},{"location":"tutorials/03-multi-gpu/#gpu-detection","title":"GPU Detection","text":"<pre><code>from llcuda.api.multigpu import detect_gpus\n\ngpus = detect_gpus()\nfor gpu in gpus:\n    print(f\"GPU {gpu.index}: {gpu.name}, {gpu.memory_total / 1024**3:.1f} GB\")\n</code></pre>"},{"location":"tutorials/03-multi-gpu/#tensor-split-configuration","title":"Tensor-Split Configuration","text":"<pre><code>from llcuda.server import ServerConfig\n\n# Equal split across 2 GPUs\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    tensor_split=\"0.5,0.5\",\n    split_mode=\"layer\",\n    n_gpu_layers=99,\n    flash_attn=True\n)\n</code></pre>"},{"location":"tutorials/03-multi-gpu/#kaggle-dual-t4-preset","title":"Kaggle Dual T4 Preset","text":"<pre><code>from llcuda.api.multigpu import kaggle_t4_dual_config\n\nconfig = kaggle_t4_dual_config(model_path=\"model.gguf\")\n</code></pre>"},{"location":"tutorials/03-multi-gpu/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/04-gguf-quantization/","title":"GGUF Quantization","text":"<p>Understanding GGUF format, K-quants, I-quants, and VRAM estimation.</p> <p>Level: Intermediate | Time: 20 minutes | VRAM Required: Varies</p>"},{"location":"tutorials/04-gguf-quantization/#gguf-formats","title":"GGUF Formats","text":"<p>K-Quants (recommended for quality): - <code>Q4_K_M</code> - Best balance (4-bit) - <code>Q5_K_M</code> - Higher quality (5-bit) - <code>Q6_K</code> - Excellent quality (6-bit) - <code>Q8_0</code> - Near-FP16 quality (8-bit)</p> <p>I-Quants (for 70B models): - <code>IQ3_XS</code> - 3-bit, fits 70B on 30GB - <code>IQ2_XXS</code> - 2-bit, ultra-compressed</p>"},{"location":"tutorials/04-gguf-quantization/#vram-estimation","title":"VRAM Estimation","text":"<pre><code>from llcuda.api.gguf import estimate_vram\n\nvram_gb = estimate_vram(\n    model_size_b=7,  # 7B parameters\n    quant_type=\"Q4_K_M\"\n)\nprint(f\"Est. VRAM: {vram_gb:.1f} GB\")\n</code></pre>"},{"location":"tutorials/04-gguf-quantization/#parse-gguf-files","title":"Parse GGUF Files","text":"<pre><code>from llcuda.utils import GGUFParser\n\nparser = GGUFParser(model_path=\"model.gguf\")\nprint(f\"Parameters: {parser.get_parameter_count() / 1e9:.1f}B\")\nprint(f\"Quantization: {parser.get_quantization()}\")\nprint(f\"Context: {parser.get_context_length()}\")\n</code></pre>"},{"location":"tutorials/04-gguf-quantization/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/05-unsloth-integration/","title":"Unsloth Integration","text":"<p>Complete workflow: Fine-tune with Unsloth \u2192 Export GGUF \u2192 Deploy with llcuda.</p> <p>Level: Intermediate | Time: 30 minutes | VRAM Required: 10-15 GB</p>"},{"location":"tutorials/05-unsloth-integration/#workflow-overview","title":"Workflow Overview","text":"<pre><code>Unsloth (Fine-tune) \u2192 GGUF Export \u2192 llcuda Deployment\n</code></pre>"},{"location":"tutorials/05-unsloth-integration/#step-1-fine-tune-with-unsloth","title":"Step 1: Fine-tune with Unsloth","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-2-2b-it\",\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\n# Fine-tune (add your training code here)\n</code></pre>"},{"location":"tutorials/05-unsloth-integration/#step-2-export-to-gguf","title":"Step 2: Export to GGUF","text":"<pre><code># Save as GGUF Q4_K_M\nmodel.save_pretrained_gguf(\n    \"output_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n</code></pre>"},{"location":"tutorials/05-unsloth-integration/#step-3-deploy-with-llcuda","title":"Step 3: Deploy with llcuda","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"output_model/model-Q4_K_M.gguf\",\n    n_gpu_layers=99,\n    flash_attn=True\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"tutorials/05-unsloth-integration/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/06-split-gpu-graphistry/","title":"Split-GPU with Graphistry","text":"<p>LLM on GPU 0 + RAPIDS/Graphistry visualization on GPU 1.</p> <p>Level: Intermediate | Time: 30 minutes | VRAM Required: GPU0: 5-10 GB, GPU1: 2-8 GB</p>"},{"location":"tutorials/06-split-gpu-graphistry/#split-gpu-architecture","title":"Split-GPU Architecture","text":"<pre><code>GPU 0: llama-server (LLM inference)\n  \u2193 Extract knowledge graphs\nGPU 1: RAPIDS cuDF/cuGraph + Graphistry (visualization)\n</code></pre>"},{"location":"tutorials/06-split-gpu-graphistry/#configure-llm-on-gpu-0","title":"Configure LLM on GPU 0","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nfrom llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    n_gpu_layers=99,\n    flash_attn=True\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"tutorials/06-split-gpu-graphistry/#configure-graphistry-on-gpu-1","title":"Configure Graphistry on GPU 1","text":"<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nimport cudf\nimport graphistry\n\ngraphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\")\n</code></pre>"},{"location":"tutorials/06-split-gpu-graphistry/#knowledge-graph-workflow","title":"Knowledge Graph Workflow","text":"<pre><code># 1. Extract entities from LLM\nfrom llcuda.api.client import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.create_chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Extract entities from: ...\"}]\n)\n\n# 2. Build graph on GPU 1\nedges_df = cudf.DataFrame(...)\ng = graphistry.edges(edges_df)\ng.plot()\n</code></pre>"},{"location":"tutorials/06-split-gpu-graphistry/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/07-openai-api/","title":"OpenAI API Client","text":"<p>Use OpenAI SDK with llama-server for drop-in compatibility.</p> <p>Level: Advanced | Time: 15 minutes | VRAM Required: 5-10 GB</p>"},{"location":"tutorials/07-openai-api/#setup","title":"Setup","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8080/v1\",\n    api_key=\"not-needed\"\n)\n</code></pre>"},{"location":"tutorials/07-openai-api/#chat-completions","title":"Chat Completions","text":"<pre><code>response = client.chat.completions.create(\n    model=\"local-model\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_tokens=100\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"tutorials/07-openai-api/#streaming","title":"Streaming","text":"<pre><code>stream = client.chat.completions.create(\n    model=\"local-model\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"tutorials/07-openai-api/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/08-nccl-pytorch/","title":"NCCL and PyTorch","text":"<p>Understanding NCCL vs tensor-split for distributed workloads.</p> <p>Level: Advanced | Time: 25 minutes | VRAM Required: 15-25 GB</p>"},{"location":"tutorials/08-nccl-pytorch/#key-differences","title":"Key Differences","text":"<p>llama-server tensor-split: - Native CUDA layer distribution - NO NCCL required - For LLM inference</p> <p>PyTorch DDP with NCCL: - Distributed training - Requires NCCL - For fine-tuning</p>"},{"location":"tutorials/08-nccl-pytorch/#llama-server-no-nccl","title":"llama-server (NO NCCL)","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"model.gguf\",\n    tensor_split=\"0.5,0.5\",  # Native CUDA split\n    n_gpu_layers=99\n)\n</code></pre>"},{"location":"tutorials/08-nccl-pytorch/#pytorch-ddp-uses-nccl","title":"PyTorch DDP (Uses NCCL)","text":"<pre><code>import torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")\n\n# DDP training code here\n</code></pre>"},{"location":"tutorials/08-nccl-pytorch/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/09-large-models/","title":"Large Models on Kaggle","text":"<p>Run 70B models on Kaggle's 30GB dual T4 setup with I-quants.</p> <p>Level: Advanced | Time: 30 minutes | VRAM Required: 25-30 GB (dual T4)</p>"},{"location":"tutorials/09-large-models/#70b-model-strategy","title":"70B Model Strategy","text":"<p>Use IQ3_XS quantization to fit 70B models in 30GB VRAM.</p>"},{"location":"tutorials/09-large-models/#download-70b-model","title":"Download 70B Model","text":"<pre><code>from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(\n    repo_id=\"unsloth/Llama-3.1-70B-Instruct-GGUF\",\n    filename=\"Llama-3.1-70B-Instruct-IQ3_XS.gguf\"\n)\n</code></pre>"},{"location":"tutorials/09-large-models/#configure-for-70b","title":"Configure for 70B","text":"<pre><code>from llcuda.server import ServerConfig\n\nconfig = ServerConfig(\n    model_path=model_path,\n    tensor_split=\"0.48,0.48\",  # Leave 2GB each for overhead\n    split_mode=\"layer\",\n    n_gpu_layers=80,  # Adjust as needed\n    context_size=2048,  # Smaller context\n    n_batch=128,       # Smaller batch\n    flash_attn=True\n)\n</code></pre>"},{"location":"tutorials/09-large-models/#expected-performance","title":"Expected Performance","text":"<ul> <li>Speed: ~12 tokens/sec (Llama-70B IQ3_XS)</li> <li>VRAM: ~28-29 GB total</li> <li>Context: 2048 tokens (can increase if VRAM allows)</li> </ul>"},{"location":"tutorials/09-large-models/#vram-monitoring","title":"VRAM Monitoring","text":"<pre><code>import torch\n\nfor i in range(torch.cuda.device_count()):\n    mem_alloc = torch.cuda.memory_allocated(i) / 1024**3\n    mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n    print(f\"GPU {i}: {mem_alloc:.1f} / {mem_total:.1f} GB\")\n</code></pre>"},{"location":"tutorials/09-large-models/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"tutorials/10-complete-workflow/","title":"Complete Workflow","text":"<p>End-to-end production workflow: Unsloth \u2192 GGUF \u2192 Multi-GPU \u2192 Deployment.</p> <p>Level: Advanced | Time: 45 minutes | VRAM Required: Varies</p>"},{"location":"tutorials/10-complete-workflow/#full-workflow","title":"Full Workflow","text":"<pre><code>1. Environment Setup\n2. Model Selection\n3. Unsloth Fine-tuning\n4. GGUF Export\n5. Multi-GPU Deployment\n6. OpenAI API Client\n7. Production Monitoring\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#1-environment-setup","title":"1. Environment Setup","text":"<pre><code>pip install llcuda unsloth graphistry\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#2-model-selection","title":"2. Model Selection","text":"<pre><code># Choose model based on VRAM\n# Gemma 2-2B: ~3-4 GB\n# Llama-3.2-3B: ~4-5 GB\n# Qwen-2.5-7B: ~6-7 GB\n# Llama-70B IQ3_XS: ~28-29 GB (dual T4)\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#3-fine-tune-optional","title":"3. Fine-tune (Optional)","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-2-2b-it\",\n    load_in_4bit=True\n)\n\n# Add LoRA and train\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#4-export-to-gguf","title":"4. Export to GGUF","text":"<pre><code>model.save_pretrained_gguf(\"output\", tokenizer, quantization_method=\"q4_k_m\")\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#5-deploy-multi-gpu","title":"5. Deploy Multi-GPU","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\nfrom llcuda.api.multigpu import kaggle_t4_dual_config\n\nconfig = kaggle_t4_dual_config(model_path=\"model.gguf\")\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#6-use-openai-client","title":"6. Use OpenAI Client","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"none\")\nresponse = client.chat.completions.create(\n    model=\"local\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#7-monitor-performance","title":"7. Monitor Performance","text":"<pre><code># Check logs\nlogs = server.get_logs()\n\n# Monitor VRAM\nimport torch\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.memory_allocated(i) / 1024**3:.1f} GB\")\n</code></pre>"},{"location":"tutorials/10-complete-workflow/#production-best-practices","title":"Production Best Practices","text":"<ul> <li>Use FlashAttention for 2-3x speedup</li> <li>Monitor VRAM usage</li> <li>Configure appropriate context size</li> <li>Use tensor-split for larger models</li> <li>Enable health checking</li> <li>Log all requests</li> </ul>"},{"location":"tutorials/10-complete-workflow/#open-in-kaggle","title":"Open in Kaggle","text":""},{"location":"unsloth/best-practices/","title":"Best Practices","text":"<p>Recommendations for Unsloth + llcuda workflow.</p>"},{"location":"unsloth/best-practices/#model-selection","title":"Model Selection","text":""},{"location":"unsloth/best-practices/#for-single-t4-15gb","title":"For Single T4 (15GB)","text":"<ul> <li>Qwen2.5-1.5B</li> <li>Gemma 2-2B</li> <li>Llama-3.2-3B</li> </ul>"},{"location":"unsloth/best-practices/#for-dual-t4-30gb","title":"For Dual T4 (30GB)","text":"<ul> <li>Qwen2.5-7B</li> <li>Llama-3.1-8B</li> <li>Mistral-7B</li> </ul>"},{"location":"unsloth/best-practices/#quantization","title":"Quantization","text":"Model Size Training Export 1-3B 4-bit QLoRA Q4_K_M 7-8B 4-bit QLoRA Q4_K_M 13B+ 4-bit QLoRA IQ3_XS"},{"location":"unsloth/best-practices/#training-tips","title":"Training Tips","text":"<ol> <li>Use QLoRA (4-bit)</li> <li>70% less VRAM</li> <li> <p>2x faster training</p> </li> <li> <p>Optimal LoRA rank</p> </li> <li>Small models: r=8-16</li> <li> <p>Large models: r=16-32</p> </li> <li> <p>Gradient checkpointing</p> </li> <li>Reduces memory</li> <li>Slightly slower</li> </ol>"},{"location":"unsloth/best-practices/#deployment-tips","title":"Deployment Tips","text":"<ol> <li>Enable FlashAttention</li> <li>Use tensor-split for large models</li> <li>Monitor VRAM usage</li> <li>Test with small batches first</li> </ol>"},{"location":"unsloth/deployment/","title":"Deployment Pipeline","text":"<p>Deploy Unsloth models with llcuda.</p>"},{"location":"unsloth/deployment/#complete-pipeline","title":"Complete Pipeline","text":""},{"location":"unsloth/deployment/#1-fine-tune-unsloth","title":"1. Fine-Tune (Unsloth)","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(...)\n# ... training ...\n</code></pre>"},{"location":"unsloth/deployment/#2-export-unsloth","title":"2. Export (Unsloth)","text":"<pre><code>model.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n</code></pre>"},{"location":"unsloth/deployment/#3-deploy-llcuda","title":"3. Deploy (llcuda)","text":"<pre><code>from llcuda.server import ServerManager, ServerConfig\n\nconfig = ServerConfig(\n    model_path=\"my_model-Q4_K_M.gguf\",\n    n_gpu_layers=99,\n    tensor_split=\"0.5,0.5\",  # Dual T4\n    flash_attn=True,\n)\n\nserver = ServerManager()\nserver.start_with_config(config)\n</code></pre>"},{"location":"unsloth/deployment/#4-serve-openai-api","title":"4. Serve (OpenAI API)","text":"<pre><code>from llcuda.api import LlamaCppClient\n\nclient = LlamaCppClient()\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n)\n</code></pre>"},{"location":"unsloth/deployment/#production-checklist","title":"Production Checklist","text":"<ul> <li> Model exported to GGUF</li> <li> VRAM requirements verified</li> <li> FlashAttention enabled</li> <li> Server health checked</li> <li> API tested</li> </ul>"},{"location":"unsloth/fine-tuning/","title":"Fine-Tuning Workflow","text":"<p>Fine-tune models with Unsloth for llcuda deployment.</p>"},{"location":"unsloth/fine-tuning/#step-1-setup-unsloth","title":"Step 1: Setup Unsloth","text":"<pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,  # QLoRA\n)\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-2-add-lora","title":"Step 2: Add LoRA","text":"<pre><code>model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n)\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-3-train","title":"Step 3: Train","text":"<pre><code>from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    args=training_args,\n)\n\ntrainer.train()\n</code></pre>"},{"location":"unsloth/fine-tuning/#step-4-export-gguf","title":"Step 4: Export GGUF","text":"<pre><code>model.save_pretrained_gguf(\n    \"my_finetuned_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # Recommended for T4\n)\n</code></pre> <p>Output: <code>my_finetuned_model-Q4_K_M.gguf</code></p> <p>See: Tutorial 05</p>"},{"location":"unsloth/gguf-export/","title":"GGUF Export","text":"<p>Export Unsloth models to GGUF format.</p>"},{"location":"unsloth/gguf-export/#basic-export","title":"Basic Export","text":"<pre><code>model.save_pretrained_gguf(\n    \"output_dir\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n)\n</code></pre>"},{"location":"unsloth/gguf-export/#quantization-options","title":"Quantization Options","text":"Method Size Quality Use Case <code>q4_k_m</code> 4.8 bpw Good Recommended <code>q5_k_m</code> 5.7 bpw Better Higher quality <code>q8_0</code> 8.5 bpw Excellent Maximum quality"},{"location":"unsloth/gguf-export/#advanced-options","title":"Advanced Options","text":"<pre><code>model.save_pretrained_gguf(\n    \"output_dir\",\n    tokenizer,\n    quantization_method=\"q4_k_m\",\n\n    # Optional\n    push_to_hub=False,\n    token=None,\n    save_method=\"merged_16bit\",  # or \"lora\"\n)\n</code></pre>"},{"location":"unsloth/gguf-export/#output-files","title":"Output Files","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 my_model-Q4_K_M.gguf      # Quantized model\n\u251c\u2500\u2500 my_model-F16.gguf         # Optional: FP16 version\n\u2514\u2500\u2500 config.json               # Model config\n</code></pre>"},{"location":"unsloth/overview/","title":"Unsloth Integration","text":"<p>llcuda as CUDA12 inference backend for Unsloth.</p>"},{"location":"unsloth/overview/#workflow","title":"Workflow","text":"<pre><code>1. Fine-Tune (Unsloth)\n   \u2193\n2. Export GGUF (Unsloth)\n   \u2193\n3. Deploy (llcuda)\n</code></pre>"},{"location":"unsloth/overview/#why-unsloth-llcuda","title":"Why Unsloth + llcuda?","text":"<ul> <li>Unsloth: 2x faster training, 70% less VRAM</li> <li>llcuda: Fast CUDA12 inference on Kaggle</li> <li>Seamless: Direct GGUF export</li> </ul>"},{"location":"unsloth/overview/#quick-example","title":"Quick Example","text":"<pre><code># 1. Fine-tune with Unsloth\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Qwen2.5-1.5B-Instruct\",\n    load_in_4bit=True,\n)\n# ... train ...\n\n# 2. Export to GGUF\nmodel.save_pretrained_gguf(\n    \"my_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# 3. Deploy with llcuda\nfrom llcuda.server import ServerManager, ServerConfig\n\nserver = ServerManager()\nserver.start_with_config(ServerConfig(\n    model_path=\"my_model-Q4_K_M.gguf\",\n))\n</code></pre> <p>See: Fine-Tuning Workflow</p>"}]}