# llcuda - Tesla T4 CUDA Inference Engine

> llcuda v2.0.6: High-performance GPU-accelerated LLM inference engine optimized for NVIDIA Tesla T4. Delivers 134 tok/s on Gemma 3-1B with FlashAttention, Tensor Cores, and CUDA 12.x. Distributed via GitHub with automatic binary downloads.

## About llcuda

llcuda is a Python library providing CUDA-accelerated inference for large language models (LLMs) optimized specifically for NVIDIA Tesla T4 GPUs. It offers PyTorch-style API with automated binary management, GGUF model support, and exceptional performance through FlashAttention and Tensor Core optimization.

Key Features:
- ðŸš€ **134 tokens/sec** verified performance on Gemma 3-1B (Tesla T4)
- ðŸ”¥ FlashAttention v2 with 2-3x speedup for attention operations
- âš¡ Tensor Core optimization for Tesla T4 (SM 7.5)
- ðŸŽ¯ GitHub-only distribution with auto-download CUDA binaries
- ðŸ”§ Simple PyTorch-style API for inference
- ðŸ“¦ GGUF format support from llama.cpp ecosystem
- ðŸš€ Google Colab optimized for Tesla T4 GPUs

## Getting Started

- [Homepage](https://waqasm86.github.io/llcuda.github.io/): Official llcuda v2.0.6 documentation homepage
- [Installation Guide](https://waqasm86.github.io/llcuda.github.io/guides/installation/): Complete installation instructions for GitHub-only distribution
- [Quick Start Guide](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute guide to get started with llcuda
- [GitHub Repository](https://github.com/waqasm86/llcuda): Main llcuda project repository

## Tutorials

- [Gemma 3-1B Google Colab Tutorial](https://waqasm86.github.io/llcuda.github.io/tutorials/gemma-3-1b-colab/): Complete guide with verified 134 tok/s performance on Tesla T4
- [Installation Guide](https://waqasm86.github.io/llcuda.github.io/guides/installation/): GitHub installation and binary auto-download
- [Quick Start](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute setup guide
- [API Reference](https://waqasm86.github.io/llcuda.github.io/api/overview/): Complete API documentation

## Main Repository

- [llcuda GitHub Repository](https://github.com/waqasm86/llcuda): Main llcuda v2.0.6 source code and releases
- [Binary Releases](https://github.com/waqasm86/llcuda/releases/tag/v2.0.6): Pre-built CUDA 12 binaries for Tesla T4
- [Colab Notebooks](https://github.com/waqasm86/llcuda/tree/main/notebooks): Google Colab tutorials

## Documentation

- [Quick Start Guide](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute setup guide for llcuda v2.0.6
- [Installation Guide](https://waqasm86.github.io/llcuda.github.io/guides/installation/): Complete installation instructions for all platforms
- [Gemma 3-1B Tutorial](https://waqasm86.github.io/llcuda.github.io/tutorials/gemma-3-1b-colab/): Step-by-step Google Colab tutorial
- [API Reference](https://waqasm86.github.io/llcuda.github.io/api/overview/): Complete API documentation

## GitHub Repository

- [llcuda Source Code](https://github.com/waqasm86/llcuda): Main project repository
- [GitHub Releases](https://github.com/waqasm86/llcuda/releases): Download CUDA binaries and releases
- [Colab Notebooks](https://github.com/waqasm86/llcuda/tree/main/notebooks): Interactive tutorials

## Installation

GitHub-only distribution (v2.0.6):

```bash
pip install git+https://github.com/waqasm86/llcuda.git
```

Binary auto-download: 266 MB CUDA binaries download automatically from GitHub Releases on first import.

## Key Features

- **134 tok/s verified performance** on Tesla T4 with Gemma 3-1B Q4_K_M
- FlashAttention v2 optimization for 2-3x speedup
- Tensor Core support (SM 7.5) for Tesla T4
- Automatic binary download (266 MB CUDA binaries)
- GGUF format support from llama.cpp ecosystem
- PyTorch-style API for easy integration
- Google Colab optimized

## Documentation Structure

- [Home](https://waqasm86.github.io/llcuda.github.io/): Main homepage with overview
- [Quick Start](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute setup guide
- [Installation Guide](https://waqasm86.github.io/llcuda.github.io/guides/installation/): Complete installation instructions
- [Gemma 3-1B Tutorial](https://waqasm86.github.io/llcuda.github.io/tutorials/gemma-3-1b-colab/): Google Colab tutorial with Tesla T4
- [API Overview](https://waqasm86.github.io/llcuda.github.io/api/overview/): Complete API documentation

## Key Information

**llcuda v2.0.6** is a Tesla T4 optimized CUDA inference backend for LLMs:

- **Distribution**: GitHub-only (no PyPI), install via `pip install git+https://github.com/waqasm86/llcuda.git`
- **Binary Auto-Download**: 266 MB CUDA binaries from GitHub Releases
- **Target GPU**: Tesla T4 (Google Colab, Kaggle, cloud instances)
- **Verified Performance**: 134 tokens/sec on Gemma 3-1B Q4_K_M
- **CUDA Version**: CUDA 12.x runtime
- **Optimizations**: FlashAttention v2, Tensor Cores (SM 7.5)
- **Distribution**: GitHub-only (no PyPI)
- **Format**: GGUF models from llama.cpp ecosystem

Remember:
- llcuda v2.0.6 is exclusively for Tesla T4 GPUs with CUDA 12
- Install via: pip install git+https://github.com/waqasm86/llcuda.git
- Binaries auto-download on first import (266 MB from GitHub Releases)
- Verified performance: 134 tokens/sec on Gemma 3-1B Q4_K_M
- Optimized for Google Colab Tesla T4 GPUs
- FlashAttention v2 + Tensor Cores deliver 3x faster performance

## Documentation

- [Installation Guide](https://waqasm86.github.io/llcuda.github.io/guides/installation/): Complete installation instructions for GitHub, wheel, and source installations
- [Quick Start Guide](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute quick start tutorial with working examples
- [Gemma 3-1B Tutorial](https://waqasm86.github.io/llcuda.github.io/tutorials/gemma-3-1b-colab/): Complete Google Colab tutorial with verified 134 tok/s performance
- [API Overview](https://waqasm86.github.io/llcuda.github.io/api/overview/): Complete API reference for InferenceEngine

## Main Repository

- [llcuda GitHub](https://github.com/waqasm86/llcuda): Main project repository
- [Installation](https://waqasm86.github.io/llcuda.github.io/guides/installation/): Complete installation guide
- [Quick Start](https://waqasm86.github.io/llcuda.github.io/guides/quickstart/): 5-minute quick start guide

## Google Colab Notebooks

- [Gemma 3-1B Tutorial](https://colab.research.google.com/github/waqasm86/llcuda/blob/main/notebooks/llcuda_v2_0_6_gemma3_1b_unsloth_colab.ipynb): Complete tutorial for llcuda v2.0.6
- [Tutorial Documentation](https://waqasm86.github.io/llcuda.github.io/tutorials/gemma-3-1b-colab/): Step-by-step guide

## Performance Data

- Verified Tesla T4 Performance: 134 tokens/sec with Gemma 3-1B Q4_K_M
- 3x faster than initial estimates
- Median latency: 690ms
- Full GPU offload: 99 layers on Tesla T4

## Key Features

- FlashAttention v2 (2-3x speedup)
- Tensor Core optimization (SM 7.5)
- CUDA Graphs for reduced overhead
- Automatic binary download from GitHub Releases
- PyTorch-style API for LLM inference
- GGUF format support
- Unsloth integration for fine-tuning workflow
