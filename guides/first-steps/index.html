<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Fast LLM inference on Tesla T4 GPUs with CUDA 12, FlashAttention, and Unsloth integration. Verified 134 tokens/sec on Gemma 3-1B. GitHub-only distribution with auto-downloading binaries. Perfect for Google Colab and production deployment."><meta name=author content="Waqas Muhammad"><link href=https://llcuda.github.io/guides/first-steps/ rel=canonical><link href=../installation/ rel=prev><link href=../../tutorials/gemma-3-1b-colab/ rel=next><link rel=icon href=../../assets/images/logo.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>First Steps - llcuda v2.0.6 - Tesla T4 CUDA Inference</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#first-steps-with-llcuda class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="llcuda v2.0.6 - Tesla T4 CUDA Inference" class="md-header__button md-logo" aria-label="llcuda v2.0.6 - Tesla T4 CUDA Inference" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> llcuda v2.0.6 - Tesla T4 CUDA Inference </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> First Steps </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../quickstart/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../tutorials/gemma-3-1b-colab/ class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../../api/overview/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../performance/benchmarks/ class=md-tabs__link> Performance </a> </li> <li class=md-tabs__item> <a href=../model-selection/ class=md-tabs__link> Guides </a> </li> <li class=md-tabs__item> <a href=../../notebooks/ class=md-tabs__link> Notebooks </a> </li> <li class=md-tabs__item> <a href=https://github.com/waqasm86/llcuda class=md-tabs__link> Links </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="llcuda v2.0.6 - Tesla T4 CUDA Inference" class="md-nav__button md-logo" aria-label="llcuda v2.0.6 - Tesla T4 CUDA Inference" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> llcuda v2.0.6 - Tesla T4 CUDA Inference </label> <div class=md-nav__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> First Steps </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> First Steps </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#verify-installation class=md-nav__link> <span class=md-ellipsis> Verify Installation </span> </a> </li> <li class=md-nav__item> <a href=#check-gpu-availability class=md-nav__link> <span class=md-ellipsis> Check GPU Availability </span> </a> </li> <li class=md-nav__item> <a href=#your-first-inference class=md-nav__link> <span class=md-ellipsis> Your First Inference </span> </a> <nav class=md-nav aria-label="Your First Inference"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#step-1-create-the-engine class=md-nav__link> <span class=md-ellipsis> Step 1: Create the Engine </span> </a> </li> <li class=md-nav__item> <a href=#step-2-load-a-model class=md-nav__link> <span class=md-ellipsis> Step 2: Load a Model </span> </a> </li> <li class=md-nav__item> <a href=#step-3-run-inference class=md-nav__link> <span class=md-ellipsis> Step 3: Run Inference </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#understanding-key-concepts class=md-nav__link> <span class=md-ellipsis> Understanding Key Concepts </span> </a> <nav class=md-nav aria-label="Understanding Key Concepts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inferenceengine class=md-nav__link> <span class=md-ellipsis> InferenceEngine </span> </a> </li> <li class=md-nav__item> <a href=#model-loading-options class=md-nav__link> <span class=md-ellipsis> Model Loading Options </span> </a> </li> <li class=md-nav__item> <a href=#inference-parameters class=md-nav__link> <span class=md-ellipsis> Inference Parameters </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#working-with-results class=md-nav__link> <span class=md-ellipsis> Working with Results </span> </a> </li> <li class=md-nav__item> <a href=#batch-processing class=md-nav__link> <span class=md-ellipsis> Batch Processing </span> </a> </li> <li class=md-nav__item> <a href=#performance-metrics class=md-nav__link> <span class=md-ellipsis> Performance Metrics </span> </a> </li> <li class=md-nav__item> <a href=#context-manager-pattern class=md-nav__link> <span class=md-ellipsis> Context Manager Pattern </span> </a> </li> <li class=md-nav__item> <a href=#common-patterns class=md-nav__link> <span class=md-ellipsis> Common Patterns </span> </a> <nav class=md-nav aria-label="Common Patterns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#quick-inference class=md-nav__link> <span class=md-ellipsis> Quick Inference </span> </a> </li> <li class=md-nav__item> <a href=#streaming-generation class=md-nav__link> <span class=md-ellipsis> Streaming Generation </span> </a> </li> <li class=md-nav__item> <a href=#model-switching class=md-nav__link> <span class=md-ellipsis> Model Switching </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#auto-configuration class=md-nav__link> <span class=md-ellipsis> Auto-Configuration </span> </a> </li> <li class=md-nav__item> <a href=#troubleshooting-first-steps class=md-nav__link> <span class=md-ellipsis> Troubleshooting First Steps </span> </a> <nav class=md-nav aria-label="Troubleshooting First Steps"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-not-loading class=md-nav__link> <span class=md-ellipsis> Model Not Loading </span> </a> </li> <li class=md-nav__item> <a href=#server-not-starting class=md-nav__link> <span class=md-ellipsis> Server Not Starting </span> </a> </li> <li class=md-nav__item> <a href=#out-of-memory class=md-nav__link> <span class=md-ellipsis> Out of Memory </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> <li class=md-nav__item> <a href=#quick-reference class=md-nav__link> <span class=md-ellipsis> Quick Reference </span> </a> </li> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Google Colab </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Google Colab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../tutorials/gemma-3-1b-colab/ class=md-nav__link> <span class=md-ellipsis> Gemma 3-1B Tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/gemma-3-1b-executed/ class=md-nav__link> <span class=md-ellipsis> Executed Example </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/build-binaries/ class=md-nav__link> <span class=md-ellipsis> Build Binaries </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../tutorials/unsloth-integration/ class=md-nav__link> <span class=md-ellipsis> Unsloth Integration </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/performance/ class=md-nav__link> <span class=md-ellipsis> Performance Optimization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../api/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../api/inference-engine/ class=md-nav__link> <span class=md-ellipsis> InferenceEngine </span> </a> </li> <li class=md-nav__item> <a href=../../api/models/ class=md-nav__link> <span class=md-ellipsis> Models & GGUF </span> </a> </li> <li class=md-nav__item> <a href=../../api/device/ class=md-nav__link> <span class=md-ellipsis> GPU & Device </span> </a> </li> <li class=md-nav__item> <a href=../../api/examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Performance </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Performance </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../performance/benchmarks/ class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=../../performance/t4-results/ class=md-nav__link> <span class=md-ellipsis> Tesla T4 Results </span> </a> </li> <li class=md-nav__item> <a href=../../performance/optimization/ class=md-nav__link> <span class=md-ellipsis> Optimization Guide </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../model-selection/ class=md-nav__link> <span class=md-ellipsis> Model Selection </span> </a> </li> <li class=md-nav__item> <a href=../gguf-format/ class=md-nav__link> <span class=md-ellipsis> GGUF Format </span> </a> </li> <li class=md-nav__item> <a href=../troubleshooting/ class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> </li> <li class=md-nav__item> <a href=../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <div class="md-nav__link md-nav__container"> <a href=../../notebooks/ class="md-nav__link "> <span class=md-ellipsis> Notebooks </span> </a> <label class="md-nav__link " for=__nav_7 id=__nav_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Notebooks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../notebooks/colab/ class=md-nav__link> <span class=md-ellipsis> Colab Notebooks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex=0> <span class=md-ellipsis> Links </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Links </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda class=md-nav__link> <span class=md-ellipsis> GitHub Repository </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/releases class=md-nav__link> <span class=md-ellipsis> GitHub Releases </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/issues class=md-nav__link> <span class=md-ellipsis> Issues & Support </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <nav class=md-path aria-label=Navigation> <ol class=md-path__list> <li class=md-path__item> <a href=../.. class=md-path__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-path__item> <a href=../quickstart/ class=md-path__link> <span class=md-ellipsis> Getting Started </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <h1 id=first-steps-with-llcuda>First Steps with llcuda<a class=headerlink href=#first-steps-with-llcuda title="Permanent link">&para;</a></h1> <p>After installing llcuda, this guide will help you get started with your first inference tasks and understand the core concepts.</p> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h2> <p>Before proceeding, ensure you have:</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Installed llcuda v2.0.6 from GitHub</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Tesla T4 GPU (Google Colab or compatible)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> CUDA 12.x runtime (pre-installed in Colab)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Python 3.11+</li> </ul> <h2 id=verify-installation>Verify Installation<a class=headerlink href=#verify-installation title="Permanent link">&para;</a></h2> <p>First, verify that llcuda is installed correctly:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;llcuda version: </span><span class=si>{</span><span class=n>llcuda</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=c1># Output: llcuda version: 2.0.6</span>
</span></code></pre></div> <h2 id=check-gpu-availability>Check GPU Availability<a class=headerlink href=#check-gpu-availability title="Permanent link">&para;</a></h2> <p>Verify your GPU is compatible:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.core</span><span class=w> </span><span class=kn>import</span> <span class=n>get_device_properties</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=c1># Check CUDA availability</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=k>if</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>check_cuda_available</span><span class=p>():</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;✓ CUDA is available&quot;</span><span class=p>)</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=k>else</span><span class=p>:</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;✗ CUDA not available&quot;</span><span class=p>)</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=c1># Get device information</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=n>props</span> <span class=o>=</span> <span class=n>get_device_properties</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;GPU: </span><span class=si>{</span><span class=n>props</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Compute: SM </span><span class=si>{</span><span class=n>props</span><span class=o>.</span><span class=n>compute_capability_major</span><span class=si>}</span><span class=s2>.</span><span class=si>{</span><span class=n>props</span><span class=o>.</span><span class=n>compute_capability_minor</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Memory: </span><span class=si>{</span><span class=n>props</span><span class=o>.</span><span class=n>total_global_mem</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=p>(</span><span class=mi>1024</span><span class=o>**</span><span class=mi>3</span><span class=p>)</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> GB&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Expected Output (Google Colab T4):</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>✓ CUDA is available
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>GPU: Tesla T4
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>Compute: SM 7.5
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>Memory: 14.8 GB
</span></code></pre></div></p> <div class="admonition success"> <p class=admonition-title>Compatibility Check</p> <p>llcuda v2.0.6 is optimized exclusively for Tesla T4 GPUs (SM 7.5). If you see a different GPU, consider using llcuda v1.2.2 for broader compatibility.</p> </div> <h2 id=your-first-inference>Your First Inference<a class=headerlink href=#your-first-inference title="Permanent link">&para;</a></h2> <p>Let's run a simple inference using the <code>InferenceEngine</code> class:</p> <h3 id=step-1-create-the-engine>Step 1: Create the Engine<a class=headerlink href=#step-1-create-the-engine title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=c1># Create inference engine</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span>
</span></code></pre></div> <h3 id=step-2-load-a-model>Step 2: Load a Model<a class=headerlink href=#step-2-load-a-model title="Permanent link">&para;</a></h3> <p>llcuda supports loading models from a registry, local paths, or HuggingFace:</p> <div class="tabbed-set tabbed-alternate" data-tabs=1:3><input checked=checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><input id=__tabbed_1_3 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>From Registry</label><label for=__tabbed_1_2>From HuggingFace</label><label for=__tabbed_1_3>From Local Path</label></div> <div class=tabbed-content> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Load Gemma 3-1B Q4_K_M from registry (auto-downloads)</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>    <span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>    <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=p>)</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Load directly from HuggingFace</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    <span class=s2>&quot;unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf&quot;</span><span class=p>,</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>    <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=p>)</span>
</span></code></pre></div> </div> <div class=tabbed-block> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># Load from local GGUF file</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    <span class=s2>&quot;/path/to/model.gguf&quot;</span><span class=p>,</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    <span class=n>gpu_layers</span><span class=o>=</span><span class=mi>99</span><span class=p>,</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    <span class=n>ctx_size</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>    <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=p>)</span>
</span></code></pre></div> </div> </div> </div> <p><strong>What happens during loading:</strong></p> <ol> <li>Model file is downloaded/validated</li> <li>Optimal settings are auto-configured based on your GPU</li> <li>llama-server starts automatically</li> <li>Model loads into GPU memory</li> </ol> <h3 id=step-3-run-inference>Step 3: Run Inference<a class=headerlink href=#step-3-run-inference title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Simple inference</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&quot;What is artificial intelligence?&quot;</span><span class=p>,</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>    <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a><span class=p>)</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Speed: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_per_sec</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tokens/sec&quot;</span><span class=p>)</span>
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Latency: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>latency_ms</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> ms&quot;</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Expected Output:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>Artificial intelligence (AI) is a branch of computer science that focuses on creating
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>intelligent machines that can perform tasks that typically require human intelligence,
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>such as visual perception, speech recognition, decision-making, and language translation...
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>Speed: 134.2 tokens/sec
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>Latency: 745 ms
</span></code></pre></div></p> <h2 id=understanding-key-concepts>Understanding Key Concepts<a class=headerlink href=#understanding-key-concepts title="Permanent link">&para;</a></h2> <h3 id=inferenceengine>InferenceEngine<a class=headerlink href=#inferenceengine title="Permanent link">&para;</a></h3> <p>The <code>InferenceEngine</code> class is the main interface for llcuda:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>(</span><span class=n>server_url</span><span class=o>=</span><span class=s2>&quot;http://127.0.0.1:8090&quot;</span><span class=p>)</span>
</span></code></pre></div> <ul> <li>Manages llama-server lifecycle</li> <li>Handles model loading and configuration</li> <li>Provides high-level inference API</li> <li>Tracks performance metrics</li> </ul> <h3 id=model-loading-options>Model Loading Options<a class=headerlink href=#model-loading-options title="Permanent link">&para;</a></h3> <p>llcuda provides flexible model loading:</p> <table> <thead> <tr> <th>Parameter</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr> <td><code>gpu_layers</code></td> <td>Number of layers to offload to GPU</td> <td>Auto-configured</td> </tr> <tr> <td><code>ctx_size</code></td> <td>Context window size</td> <td>Auto-configured</td> </tr> <tr> <td><code>auto_start</code></td> <td>Automatically start server</td> <td><code>True</code></td> </tr> <tr> <td><code>auto_configure</code></td> <td>Auto-detect optimal settings</td> <td><code>True</code></td> </tr> <tr> <td><code>verbose</code></td> <td>Print status messages</td> <td><code>True</code></td> </tr> <tr> <td><code>silent</code></td> <td>Suppress llama-server output</td> <td><code>False</code></td> </tr> </tbody> </table> <h3 id=inference-parameters>Inference Parameters<a class=headerlink href=#inference-parameters title="Permanent link">&para;</a></h3> <p>Control generation with these parameters:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&quot;Your prompt here&quot;</span><span class=p>,</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>    <span class=n>max_tokens</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>        <span class=c1># Maximum tokens to generate</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span>       <span class=c1># Sampling temperature (0.0-2.0)</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>            <span class=c1># Nucleus sampling threshold</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a>    <span class=n>top_k</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span>             <span class=c1># Top-k sampling limit</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>              <span class=c1># Random seed (0=random)</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a>    <span class=n>stop_sequences</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>]</span> <span class=c1># Stop generation at these sequences</span>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a><span class=p>)</span>
</span></code></pre></div> <h2 id=working-with-results>Working with Results<a class=headerlink href=#working-with-results title="Permanent link">&para;</a></h2> <p>The <code>InferResult</code> object contains generation output and metrics:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Write a haiku about AI&quot;</span><span class=p>)</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=c1># Access generated text</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=c1># Check if inference succeeded</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=k>if</span> <span class=n>result</span><span class=o>.</span><span class=n>success</span><span class=p>:</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;✓ Generated </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_generated</span><span class=si>}</span><span class=s2> tokens&quot;</span><span class=p>)</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;✓ Speed: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_per_sec</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;✓ Latency: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>latency_ms</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> ms&quot;</span><span class=p>)</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a><span class=k>else</span><span class=p>:</span>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;✗ Error: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>error_message</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=batch-processing>Batch Processing<a class=headerlink href=#batch-processing title="Permanent link">&para;</a></h2> <p>Process multiple prompts efficiently:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>    <span class=s2>&quot;What is machine learning?&quot;</span><span class=p>,</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>    <span class=s2>&quot;Explain neural networks.&quot;</span><span class=p>,</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>    <span class=s2>&quot;What is deep learning?&quot;</span>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a><span class=p>]</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a><span class=n>results</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>batch_infer</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>result</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>results</span><span class=p>):</span>
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>--- Prompt </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2> ---&quot;</span><span class=p>)</span>
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>    <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Speed: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_per_sec</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=performance-metrics>Performance Metrics<a class=headerlink href=#performance-metrics title="Permanent link">&para;</a></h2> <p>Track inference performance:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># Get current metrics</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>metrics</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>get_metrics</span><span class=p>()</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Total requests: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;throughput&#39;</span><span class=p>][</span><span class=s1>&#39;total_requests&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Total tokens: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;throughput&#39;</span><span class=p>][</span><span class=s1>&#39;total_tokens&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Average speed: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;throughput&#39;</span><span class=p>][</span><span class=s1>&#39;tokens_per_sec&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Mean latency: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;latency&#39;</span><span class=p>][</span><span class=s1>&#39;mean_ms&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> ms&quot;</span><span class=p>)</span>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;P95 latency: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;latency&#39;</span><span class=p>][</span><span class=s1>&#39;p95_ms&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> ms&quot;</span><span class=p>)</span>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a>
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a><span class=c1># Reset metrics</span>
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a><span class=n>engine</span><span class=o>.</span><span class=n>reset_metrics</span><span class=p>()</span>
</span></code></pre></div> <h2 id=context-manager-pattern>Context Manager Pattern<a class=headerlink href=#context-manager-pattern title="Permanent link">&para;</a></h2> <p>Use context managers for automatic cleanup:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=k>with</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span> <span class=k>as</span> <span class=n>engine</span><span class=p>:</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>    <span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span> <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>    <span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Hello, AI!&quot;</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>    <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=c1># Server stops automatically when exiting context</span>
</span></code></pre></div> <h2 id=common-patterns>Common Patterns<a class=headerlink href=#common-patterns title="Permanent link">&para;</a></h2> <h3 id=quick-inference>Quick Inference<a class=headerlink href=#quick-inference title="Permanent link">&para;</a></h3> <p>For one-off inferences:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda</span><span class=w> </span><span class=kn>import</span> <span class=n>quick_infer</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=n>text</span> <span class=o>=</span> <span class=n>quick_infer</span><span class=p>(</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&quot;What is Python?&quot;</span><span class=p>,</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>    <span class=n>model_path</span><span class=o>=</span><span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>    <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>    <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span>
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a><span class=p>)</span>
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a><span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></code></pre></div> <h3 id=streaming-generation>Streaming Generation<a class=headerlink href=#streaming-generation title="Permanent link">&para;</a></h3> <p>For real-time output:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=k>def</span><span class=w> </span><span class=nf>print_chunk</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>    <span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>flush</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer_stream</span><span class=p>(</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&quot;Write a story about AI&quot;</span><span class=p>,</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>    <span class=n>callback</span><span class=o>=</span><span class=n>print_chunk</span><span class=p>,</span>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a>    <span class=n>max_tokens</span><span class=o>=</span><span class=mi>200</span>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a><span class=p>)</span>
</span></code></pre></div> <h3 id=model-switching>Model Switching<a class=headerlink href=#model-switching title="Permanent link">&para;</a></h3> <p>Switch between models:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=c1># Unload current model</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a><span class=n>engine</span><span class=o>.</span><span class=n>unload_model</span><span class=p>()</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a><span class=c1># Load different model</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;llama-3.2-3b-Q4_K_M&quot;</span><span class=p>,</span> <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a><span class=c1># Run inference with new model</span>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Test prompt&quot;</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></code></pre></div> <h2 id=auto-configuration>Auto-Configuration<a class=headerlink href=#auto-configuration title="Permanent link">&para;</a></h2> <p>llcuda automatically configures optimal settings:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=c1># Auto-configuration enabled (default)</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a>    <span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a>    <span class=n>auto_configure</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># Detects optimal gpu_layers, ctx_size, batch_size</span>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a><span class=p>)</span>
</span><span id=__span-18-6><a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a>
</span><span id=__span-18-7><a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a><span class=c1># Manual configuration</span>
</span><span id=__span-18-8><a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-18-9><a id=__codelineno-18-9 name=__codelineno-18-9 href=#__codelineno-18-9></a>    <span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span>
</span><span id=__span-18-10><a id=__codelineno-18-10 name=__codelineno-18-10 href=#__codelineno-18-10></a>    <span class=n>gpu_layers</span><span class=o>=</span><span class=mi>35</span><span class=p>,</span>
</span><span id=__span-18-11><a id=__codelineno-18-11 name=__codelineno-18-11 href=#__codelineno-18-11></a>    <span class=n>ctx_size</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span>
</span><span id=__span-18-12><a id=__codelineno-18-12 name=__codelineno-18-12 href=#__codelineno-18-12></a>    <span class=n>auto_configure</span><span class=o>=</span><span class=kc>False</span>
</span><span id=__span-18-13><a id=__codelineno-18-13 name=__codelineno-18-13 href=#__codelineno-18-13></a><span class=p>)</span>
</span></code></pre></div> <p>Auto-configuration analyzes:</p> <ul> <li>Model size and architecture</li> <li>Available GPU memory</li> <li>GPU compute capability</li> <li>Optimal batch sizes</li> </ul> <h2 id=troubleshooting-first-steps>Troubleshooting First Steps<a class=headerlink href=#troubleshooting-first-steps title="Permanent link">&para;</a></h2> <h3 id=model-not-loading>Model Not Loading<a class=headerlink href=#model-not-loading title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=c1># Check model file exists</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=kn>from</span><span class=w> </span><span class=nn>pathlib</span><span class=w> </span><span class=kn>import</span> <span class=n>Path</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a><span class=n>model_path</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&quot;~/.cache/llcuda/models/gemma-3-1b-it-Q4_K_M.gguf&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>expanduser</span><span class=p>()</span>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Model exists: </span><span class=si>{</span><span class=n>model_path</span><span class=o>.</span><span class=n>exists</span><span class=p>()</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a><span class=c1># Try manual download</span>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.models</span><span class=w> </span><span class=kn>import</span> <span class=n>download_model</span>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a><span class=n>download_model</span><span class=p>(</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a>    <span class=s2>&quot;unsloth/gemma-3-1b-it-GGUF&quot;</span><span class=p>,</span>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a>    <span class=s2>&quot;gemma-3-1b-it-Q4_K_M.gguf&quot;</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a><span class=p>)</span>
</span></code></pre></div> <h3 id=server-not-starting>Server Not Starting<a class=headerlink href=#server-not-starting title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># Check if server is already running</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=k>if</span> <span class=n>engine</span><span class=o>.</span><span class=n>check_server</span><span class=p>():</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Server is running&quot;</span><span class=p>)</span>
</span><span id=__span-20-4><a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a><span class=k>else</span><span class=p>:</span>
</span><span id=__span-20-5><a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Server is not running&quot;</span><span class=p>)</span>
</span><span id=__span-20-6><a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a>
</span><span id=__span-20-7><a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a><span class=c1># Force restart</span>
</span><span id=__span-20-8><a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=n>engine</span><span class=o>.</span><span class=n>unload_model</span><span class=p>()</span>  <span class=c1># Stop current server</span>
</span><span id=__span-20-9><a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span> <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <h3 id=out-of-memory>Out of Memory<a class=headerlink href=#out-of-memory title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=c1># Reduce GPU layers</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>    <span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span>
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a>    <span class=n>gpu_layers</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>  <span class=c1># Reduce from 99</span>
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a>    <span class=n>ctx_size</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>  <span class=c1># Reduce context</span>
</span><span id=__span-21-6><a id=__codelineno-21-6 name=__codelineno-21-6 href=#__codelineno-21-6></a>    <span class=n>auto_configure</span><span class=o>=</span><span class=kc>False</span>
</span><span id=__span-21-7><a id=__codelineno-21-7 name=__codelineno-21-7 href=#__codelineno-21-7></a><span class=p>)</span>
</span><span id=__span-21-8><a id=__codelineno-21-8 name=__codelineno-21-8 href=#__codelineno-21-8></a>
</span><span id=__span-21-9><a id=__codelineno-21-9 name=__codelineno-21-9 href=#__codelineno-21-9></a><span class=c1># Check GPU memory</span>
</span><span id=__span-21-10><a id=__codelineno-21-10 name=__codelineno-21-10 href=#__codelineno-21-10></a><span class=n>props</span> <span class=o>=</span> <span class=n>get_device_properties</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-21-11><a id=__codelineno-21-11 name=__codelineno-21-11 href=#__codelineno-21-11></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Total memory: </span><span class=si>{</span><span class=n>props</span><span class=o>.</span><span class=n>total_global_mem</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=p>(</span><span class=mi>1024</span><span class=o>**</span><span class=mi>3</span><span class=p>)</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> GB&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <p>Now that you've completed your first inference, explore:</p> <ul> <li><a href=../model-selection/ >Model Selection Guide</a> - Choose the right model for your task</li> <li><a href=../../performance/optimization/ >Performance Optimization</a> - Maximize throughput</li> <li><a href=../../api/inference-engine/ >API Reference</a> - Detailed API documentation</li> <li><a href=../../tutorials/unsloth-integration/ >Unsloth Integration</a> - Fine-tune and deploy models</li> <li><a href=../../tutorials/gemma-3-1b-colab/ >Google Colab Tutorial</a> - Complete hands-on tutorial</li> </ul> <h2 id=quick-reference>Quick Reference<a class=headerlink href=#quick-reference title="Permanent link">&para;</a></h2> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=c1># Basic workflow</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a>
</span><span id=__span-22-4><a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a><span class=c1># 1. Create engine</span>
</span><span id=__span-22-5><a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span>
</span><span id=__span-22-6><a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a>
</span><span id=__span-22-7><a id=__codelineno-22-7 name=__codelineno-22-7 href=#__codelineno-22-7></a><span class=c1># 2. Load model</span>
</span><span id=__span-22-8><a id=__codelineno-22-8 name=__codelineno-22-8 href=#__codelineno-22-8></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span> <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-22-9><a id=__codelineno-22-9 name=__codelineno-22-9 href=#__codelineno-22-9></a>
</span><span id=__span-22-10><a id=__codelineno-22-10 name=__codelineno-22-10 href=#__codelineno-22-10></a><span class=c1># 3. Run inference</span>
</span><span id=__span-22-11><a id=__codelineno-22-11 name=__codelineno-22-11 href=#__codelineno-22-11></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Your prompt&quot;</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span><span id=__span-22-12><a id=__codelineno-22-12 name=__codelineno-22-12 href=#__codelineno-22-12></a>
</span><span id=__span-22-13><a id=__codelineno-22-13 name=__codelineno-22-13 href=#__codelineno-22-13></a><span class=c1># 4. Use result</span>
</span><span id=__span-22-14><a id=__codelineno-22-14 name=__codelineno-22-14 href=#__codelineno-22-14></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span><span id=__span-22-15><a id=__codelineno-22-15 name=__codelineno-22-15 href=#__codelineno-22-15></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_per_sec</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span><span id=__span-22-16><a id=__codelineno-22-16 name=__codelineno-22-16 href=#__codelineno-22-16></a>
</span><span id=__span-22-17><a id=__codelineno-22-17 name=__codelineno-22-17 href=#__codelineno-22-17></a><span class=c1># 5. Cleanup</span>
</span><span id=__span-22-18><a id=__codelineno-22-18 name=__codelineno-22-18 href=#__codelineno-22-18></a><span class=n>engine</span><span class=o>.</span><span class=n>unload_model</span><span class=p>()</span>
</span></code></pre></div> <div class="admonition tip"> <p class=admonition-title>Performance Tip</p> <p>For best performance on Tesla T4, use Q4_K_M quantization with full GPU offload (gpu_layers=99). This achieves 130+ tokens/sec on Gemma 3-1B.</p> </div> <h2 id=resources>Resources<a class=headerlink href=#resources title="Permanent link">&para;</a></h2> <ul> <li><a href=../installation/ >Installation Guide</a></li> <li><a href=../quickstart/ >Quick Start</a></li> <li><a href=../faq/ >FAQ</a></li> <li><a href=../troubleshooting/ >Troubleshooting</a></li> <li><a href=https://github.com/waqasm86/llcuda/issues>GitHub Issues</a></li> </ul> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve by <a href=https://github.com/waqasm86/llcuda/issues/new target=_blank rel=noopener>opening an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../installation/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Installation"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Installation </div> </div> </a> <a href=../../tutorials/gemma-3-1b-colab/ class="md-footer__link md-footer__link--next" aria-label="Next: Gemma 3-1B Tutorial"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Gemma 3-1B Tutorial </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024-2026 Waqas Muhammad </div> </div> <div class=md-social> <a href=https://github.com/waqasm86 target=_blank rel=noopener title="GitHub - waqasm86" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=mailto:waqasm86@gmail.com target=_blank rel=noopener title="Email - waqasm86@gmail.com" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l208 156a48 48 0 0 0 57.6 0l208-156c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 196v188c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V196L313.6 344.8c-34.1 25.6-81.1 25.6-115.2 0z"/></svg> </a> <a href=https://www.linkedin.com/in/waqasm86 target=_blank rel=noopener title="LinkedIn - Waqas Muhammad" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/schema.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>