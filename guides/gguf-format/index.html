<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="CUDA 12 inference backend for Unsloth with multi-GPU support on Kaggle. Deploy fine-tuned models on dual Tesla T4 GPUs with llama.cpp server, GGUF quantization, and Graphistry visualization. Run 70B models with tensor-split architecture."><meta name=author content="Waqas Muhammad"><link href=https://llcuda.github.io/guides/gguf-format/ rel=canonical><link rel=icon href=../../assets/images/logo.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>GGUF Format Guide - llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#gguf-format-guide class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" class="md-header__button md-logo" aria-label="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> GGUF Format Guide </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../quickstart/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../kaggle/overview/ class=md-tabs__link> Kaggle Dual T4 </a> </li> <li class=md-tabs__item> <a href=../../tutorials/ class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../../architecture/overview/ class=md-tabs__link> Architecture </a> </li> <li class=md-tabs__item> <a href=../../api/overview/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../unsloth/overview/ class=md-tabs__link> Unsloth Integration </a> </li> <li class=md-tabs__item> <a href=../../graphistry/overview/ class=md-tabs__link> Graphistry & Visualization </a> </li> <li class=md-tabs__item> <a href=../../performance/benchmarks/ class=md-tabs__link> Performance </a> </li> <li class=md-tabs__item> <a href=../../gguf/overview/ class=md-tabs__link> GGUF & Quantization </a> </li> <li class=md-tabs__item> <a href=../model-selection/ class=md-tabs__link> Guides </a> </li> <li class=md-tabs__item> <a href=https://github.com/llcuda/llcuda class=md-tabs__link> Links </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" class="md-nav__button md-logo" aria-label="llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> llcuda v2.2.0 - CUDA12 Inference Backend for Unsloth </label> <div class=md-nav__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../first-steps/ class=md-nav__link> <span class=md-ellipsis> First Steps </span> </a> </li> <li class=md-nav__item> <a href=../kaggle-setup/ class=md-nav__link> <span class=md-ellipsis> Kaggle Setup </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Kaggle Dual T4 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Kaggle Dual T4 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../kaggle/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/dual-gpu-setup/ class=md-nav__link> <span class=md-ellipsis> Dual GPU Setup </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/multi-gpu-inference/ class=md-nav__link> <span class=md-ellipsis> Multi-GPU Inference </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/tensor-split/ class=md-nav__link> <span class=md-ellipsis> Tensor Split Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../kaggle/large-models/ class=md-nav__link> <span class=md-ellipsis> Large Models (70B) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../tutorials/ class="md-nav__link "> <span class=md-ellipsis> Tutorials </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../tutorials/01-quickstart.md class=md-nav__link> <span class=md-ellipsis> 01 - Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/02-server-setup.md class=md-nav__link> <span class=md-ellipsis> 02 - Server Setup </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/03-multi-gpu.md class=md-nav__link> <span class=md-ellipsis> 03 - Multi-GPU </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/04-gguf-quantization.md class=md-nav__link> <span class=md-ellipsis> 04 - GGUF Quantization </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/05-unsloth-integration.md class=md-nav__link> <span class=md-ellipsis> 05 - Unsloth Integration </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/06-split-gpu-graphistry.md class=md-nav__link> <span class=md-ellipsis> 06 - Split-GPU + Graphistry </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/07-openai-api.md class=md-nav__link> <span class=md-ellipsis> 07 - OpenAI API </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/08-nccl-pytorch.md class=md-nav__link> <span class=md-ellipsis> 08 - NCCL + PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/09-large-models.md class=md-nav__link> <span class=md-ellipsis> 09 - Large Models </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/10-complete-workflow.md class=md-nav__link> <span class=md-ellipsis> 10 - Complete Workflow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Architecture </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Architecture </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../architecture/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/split-gpu/ class=md-nav__link> <span class=md-ellipsis> Split-GPU Design </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/gpu0-llm/ class=md-nav__link> <span class=md-ellipsis> GPU0 - LLM Inference </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/gpu1-graphistry/ class=md-nav__link> <span class=md-ellipsis> GPU1 - Graphistry </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/tensor-split-vs-nccl/ class=md-nav__link> <span class=md-ellipsis> Tensor Split vs NCCL </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../api/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../api/client.md class=md-nav__link> <span class=md-ellipsis> LlamaCppClient </span> </a> </li> <li class=md-nav__item> <a href=../../api/multigpu.md class=md-nav__link> <span class=md-ellipsis> Multi-GPU Config </span> </a> </li> <li class=md-nav__item> <a href=../../api/gguf.md class=md-nav__link> <span class=md-ellipsis> GGUF Tools </span> </a> </li> <li class=md-nav__item> <a href=../../api/nccl.md class=md-nav__link> <span class=md-ellipsis> NCCL Integration </span> </a> </li> <li class=md-nav__item> <a href=../../api/server.md class=md-nav__link> <span class=md-ellipsis> Server Manager </span> </a> </li> <li class=md-nav__item> <a href=../../api/graphistry.md class=md-nav__link> <span class=md-ellipsis> Graphistry Integration </span> </a> </li> <li class=md-nav__item> <a href=../../api/models/ class=md-nav__link> <span class=md-ellipsis> Models & Loading </span> </a> </li> <li class=md-nav__item> <a href=../../api/examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Unsloth Integration </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Unsloth Integration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../unsloth/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../unsloth/fine-tuning/ class=md-nav__link> <span class=md-ellipsis> Fine-Tuning Workflow </span> </a> </li> <li class=md-nav__item> <a href=../../unsloth/gguf-export/ class=md-nav__link> <span class=md-ellipsis> GGUF Export </span> </a> </li> <li class=md-nav__item> <a href=../../unsloth/deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment Pipeline </span> </a> </li> <li class=md-nav__item> <a href=../../unsloth/best-practices/ class=md-nav__link> <span class=md-ellipsis> Best Practices </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex=0> <span class=md-ellipsis> Graphistry & Visualization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Graphistry & Visualization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../graphistry/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../graphistry/knowledge-graphs/ class=md-nav__link> <span class=md-ellipsis> Knowledge Graph Extraction </span> </a> </li> <li class=md-nav__item> <a href=../../graphistry/rapids.md class=md-nav__link> <span class=md-ellipsis> RAPIDS Integration </span> </a> </li> <li class=md-nav__item> <a href=../../graphistry/split-gpu-setup.md class=md-nav__link> <span class=md-ellipsis> Split-GPU Architecture </span> </a> </li> <li class=md-nav__item> <a href=../../graphistry/examples.md class=md-nav__link> <span class=md-ellipsis> Visualization Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_9> <label class=md-nav__link for=__nav_9 id=__nav_9_label tabindex=0> <span class=md-ellipsis> Performance </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=false> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> Performance </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../performance/benchmarks/ class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=../../performance/dual-t4-results/ class=md-nav__link> <span class=md-ellipsis> Dual T4 Results </span> </a> </li> <li class=md-nav__item> <a href=../../performance/optimization/ class=md-nav__link> <span class=md-ellipsis> Optimization Guide </span> </a> </li> <li class=md-nav__item> <a href=../../performance/memory/ class=md-nav__link> <span class=md-ellipsis> Memory Management </span> </a> </li> <li class=md-nav__item> <a href=../../performance/flash-attention/ class=md-nav__link> <span class=md-ellipsis> FlashAttention </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_10> <label class=md-nav__link for=__nav_10 id=__nav_10_label tabindex=0> <span class=md-ellipsis> GGUF & Quantization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_10_label aria-expanded=false> <label class=md-nav__title for=__nav_10> <span class="md-nav__icon md-icon"></span> GGUF & Quantization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../gguf/overview/ class=md-nav__link> <span class=md-ellipsis> GGUF Format Overview </span> </a> </li> <li class=md-nav__item> <a href=../../gguf/k-quants/ class=md-nav__link> <span class=md-ellipsis> K-Quants Guide </span> </a> </li> <li class=md-nav__item> <a href=../../gguf/i-quants.md class=md-nav__link> <span class=md-ellipsis> I-Quants Guide </span> </a> </li> <li class=md-nav__item> <a href=../../gguf/selection.md class=md-nav__link> <span class=md-ellipsis> Quantization Selection </span> </a> </li> <li class=md-nav__item> <a href=../../gguf/vram-estimation.md class=md-nav__link> <span class=md-ellipsis> VRAM Estimation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_11> <label class=md-nav__link for=__nav_11 id=__nav_11_label tabindex=0> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_11_label aria-expanded=false> <label class=md-nav__title for=__nav_11> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../model-selection/ class=md-nav__link> <span class=md-ellipsis> Model Selection </span> </a> </li> <li class=md-nav__item> <a href=../troubleshooting/ class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> </li> <li class=md-nav__item> <a href=../faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> <li class=md-nav__item> <a href=../build-from-source.md class=md-nav__link> <span class=md-ellipsis> Build from Source </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_12> <label class=md-nav__link for=__nav_12 id=__nav_12_label tabindex=0> <span class=md-ellipsis> Links </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_12_label aria-expanded=false> <label class=md-nav__title for=__nav_12> <span class="md-nav__icon md-icon"></span> Links </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda class=md-nav__link> <span class=md-ellipsis> GitHub Repository </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/releases class=md-nav__link> <span class=md-ellipsis> GitHub Releases </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/releases/tag/v2.2.0 class=md-nav__link> <span class=md-ellipsis> v2.2.0 Release </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/issues class=md-nav__link> <span class=md-ellipsis> Issues & Support </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/llcuda/llcuda/blob/main/CHANGELOG.md class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=gguf-format-guide>GGUF Format Guide<a class=headerlink href=#gguf-format-guide title="Permanent link">&para;</a></h1> <p>Complete guide to the GGUF (GPT-Generated Unified Format) model format used by llcuda v2.1.0.</p> <h2 id=what-is-gguf>What is GGUF?<a class=headerlink href=#what-is-gguf title="Permanent link">&para;</a></h2> <p><strong>GGUF</strong> (GPT-Generated Unified Format) is a binary format for storing large language models developed by the <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> project.</p> <h3 id=key-features>Key Features<a class=headerlink href=#key-features title="Permanent link">&para;</a></h3> <p>✅ <strong>Single-file distribution</strong> - Everything in one portable file ✅ <strong>Efficient storage</strong> - Compact binary format with compression ✅ <strong>Memory mapping</strong> - Fast loading without full RAM allocation ✅ <strong>Quantization support</strong> - Multiple precision levels (INT4, INT8, FP16) ✅ <strong>Metadata included</strong> - Model architecture, tokenizer, and configuration ✅ <strong>Cross-platform</strong> - Works on Linux, macOS, Windows ✅ <strong>GPU acceleration</strong> - Full CUDA support for inference</p> <h3 id=why-gguf>Why GGUF?<a class=headerlink href=#why-gguf title="Permanent link">&para;</a></h3> <p>GGUF replaced the older GGML format and offers significant improvements:</p> <table> <thead> <tr> <th>Feature</th> <th>GGML (Old)</th> <th>GGUF (Current)</th> </tr> </thead> <tbody> <tr> <td>Metadata</td> <td>External files</td> <td>Embedded</td> </tr> <tr> <td>Versioning</td> <td>Limited</td> <td>Full versioning</td> </tr> <tr> <td>Tokenizer</td> <td>Separate file</td> <td>Included</td> </tr> <tr> <td>Architecture</td> <td>Hard-coded</td> <td>Dynamic</td> </tr> <tr> <td>Compatibility</td> <td>Breaking changes</td> <td>Forward compatible</td> </tr> </tbody> </table> <h2 id=gguf-file-structure>GGUF File Structure<a class=headerlink href=#gguf-file-structure title="Permanent link">&para;</a></h2> <p>A GGUF file contains:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>GGUF File (.gguf)
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>├── Header (magic number, version)
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>├── Metadata (KV pairs)
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>│   ├── Architecture (llama, gemma, qwen, etc.)
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>│   ├── Model parameters (layers, heads, etc.)
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>│   ├── Tokenizer (vocabulary, special tokens)
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>│   ├── Quantization method
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>│   └── Author, license, source
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>├── Tensor info (names, shapes, offsets)
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>└── Tensor data (model weights)
</span></code></pre></div> <h3 id=example-gguf-metadata>Example GGUF Metadata<a class=headerlink href=#example-gguf-metadata title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.gguf_parser</span><span class=w> </span><span class=kn>import</span> <span class=n>GGUFReader</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=c1># Read GGUF metadata</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>reader</span> <span class=o>=</span> <span class=n>GGUFReader</span><span class=p>(</span><span class=s2>&quot;gemma-3-1b-it-Q4_K_M.gguf&quot;</span><span class=p>)</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Architecture: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>architecture</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Parameter count: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>parameter_count</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Quantization: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>quantization</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Context length: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>context_length</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=quantization-types>Quantization Types<a class=headerlink href=#quantization-types title="Permanent link">&para;</a></h2> <p>GGUF supports multiple quantization methods that trade off quality for size and speed.</p> <h3 id=quantization-comparison>Quantization Comparison<a class=headerlink href=#quantization-comparison title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Type</th> <th>Bits</th> <th>Size Multiplier</th> <th>Quality</th> <th>Speed</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>F16</strong></td> <td>16</td> <td>1.0x (largest)</td> <td>Best</td> <td>Slowest</td> <td>Reference quality</td> </tr> <tr> <td><strong>Q8_0</strong></td> <td>8</td> <td>0.5x</td> <td>Excellent</td> <td>Slow</td> <td>High quality needed</td> </tr> <tr> <td><strong>Q6_K</strong></td> <td>6</td> <td>0.4x</td> <td>Very good</td> <td>Medium</td> <td>Balanced</td> </tr> <tr> <td><strong>Q5_K_M</strong></td> <td>5</td> <td>0.35x</td> <td>Good</td> <td>Medium-fast</td> <td>Good balance</td> </tr> <tr> <td><strong>Q4_K_M</strong></td> <td>4</td> <td>0.25x</td> <td>Good</td> <td><strong>Fast</strong></td> <td><strong>Recommended</strong></td> </tr> <tr> <td><strong>Q4_K_S</strong></td> <td>4</td> <td>0.25x</td> <td>Acceptable</td> <td>Fast</td> <td>Smaller variant</td> </tr> <tr> <td><strong>Q3_K_M</strong></td> <td>3</td> <td>0.2x</td> <td>Fair</td> <td>Very fast</td> <td>Experimental</td> </tr> <tr> <td><strong>Q2_K</strong></td> <td>2</td> <td>0.15x (smallest)</td> <td>Poor</td> <td>Fastest</td> <td>Testing only</td> </tr> </tbody> </table> <h3 id=recommended-q4_k_m>Recommended: Q4_K_M<a class=headerlink href=#recommended-q4_k_m title="Permanent link">&para;</a></h3> <p>For Tesla T4 GPUs, <strong>Q4_K_M</strong> provides the best balance:</p> <p>✅ <strong>Good quality</strong> - Minimal accuracy loss vs FP16 ✅ <strong>Fast inference</strong> - 134 tok/s on Gemma 3-1B ✅ <strong>Small size</strong> - 4 bits per parameter ✅ <strong>Low VRAM</strong> - Fits larger models in 16 GB</p> <p><strong>Example sizes for Gemma 3-1B:</strong> - F16: ~2.6 GB - Q8_0: ~1.4 GB - <strong>Q4_K_M: ~650 MB</strong> ← Recommended - Q2_K: ~400 MB</p> <h2 id=using-gguf-models-with-llcuda>Using GGUF Models with llcuda<a class=headerlink href=#using-gguf-models-with-llcuda title="Permanent link">&para;</a></h2> <h3 id=method-1-from-huggingface-recommended>Method 1: From HuggingFace (Recommended)<a class=headerlink href=#method-1-from-huggingface-recommended title="Permanent link">&para;</a></h3> <p>Load directly from Unsloth or other HuggingFace repositories:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># Load from Unsloth repository</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>    <span class=s2>&quot;unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf&quot;</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=p>)</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=c1># Format: repo_id:filename</span>
</span></code></pre></div> <p><strong>Popular Unsloth GGUF models:</strong> - <code>unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf</code></p> <h3 id=method-2-from-local-file>Method 2: From Local File<a class=headerlink href=#method-2-from-local-file title="Permanent link">&para;</a></h3> <p>Use a downloaded GGUF file:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;/path/to/model.gguf&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=method-3-from-url>Method 3: From URL<a class=headerlink href=#method-3-from-url title="Permanent link">&para;</a></h3> <p>Direct download from any URL:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>    <span class=s2>&quot;https://huggingface.co/user/repo/resolve/main/model.gguf&quot;</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=p>)</span>
</span></code></pre></div> <h2 id=converting-models-to-gguf>Converting Models to GGUF<a class=headerlink href=#converting-models-to-gguf title="Permanent link">&para;</a></h2> <h3 id=from-pytorchhuggingface>From PyTorch/HuggingFace<a class=headerlink href=#from-pytorchhuggingface title="Permanent link">&para;</a></h3> <p>Use the <code>convert_hf_to_gguf.py</code> script from llama.cpp:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Clone llama.cpp</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>git<span class=w> </span>clone<span class=w> </span>https://github.com/ggerganov/llama.cpp.git
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=nb>cd</span><span class=w> </span>llama.cpp
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=c1># Install dependencies</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>pip<span class=w> </span>install<span class=w> </span>-r<span class=w> </span>requirements.txt
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=c1># Convert model</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>python<span class=w> </span>convert_hf_to_gguf.py<span class=w> </span><span class=se>\</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a><span class=w>    </span>/path/to/huggingface/model<span class=w> </span><span class=se>\</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=w>    </span>--outfile<span class=w> </span>model-f16.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=w>    </span>--outtype<span class=w> </span>f16
</span></code></pre></div> <h3 id=from-unsloth-fine-tuned-models>From Unsloth Fine-Tuned Models<a class=headerlink href=#from-unsloth-fine-tuned-models title="Permanent link">&para;</a></h3> <p>Export directly from Unsloth:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=kn>from</span><span class=w> </span><span class=nn>unsloth</span><span class=w> </span><span class=kn>import</span> <span class=n>FastLanguageModel</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=c1># After fine-tuning</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained_gguf</span><span class=p>(</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    <span class=s2>&quot;my_model&quot;</span><span class=p>,</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>    <span class=n>tokenizer</span><span class=p>,</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>    <span class=n>quantization_method</span><span class=o>=</span><span class=s2>&quot;q4_k_m&quot;</span>  <span class=c1># Creates Q4_K_M GGUF</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=p>)</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a><span class=c1># Output: my_model/unsloth.Q4_K_M.gguf</span>
</span></code></pre></div> <p><strong>Supported quantization methods:</strong> - <code>"f16"</code> - Full precision - <code>"q8_0"</code> - 8-bit quantization - <code>"q6_k"</code> - 6-bit K-quant - <code>"q5_k_m"</code> - 5-bit K-quant medium - <code>"q4_k_m"</code> - 4-bit K-quant medium (recommended) - <code>"q4_k_s"</code> - 4-bit K-quant small - <code>"q3_k_m"</code> - 3-bit K-quant medium - <code>"q2_k"</code> - 2-bit K-quant</p> <h2 id=quantizing-existing-gguf>Quantizing Existing GGUF<a class=headerlink href=#quantizing-existing-gguf title="Permanent link">&para;</a></h2> <p>Convert between quantization levels:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Using llama-quantize (included with llcuda binaries)</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>~/.cache/llcuda/bin/llama-quantize<span class=w> </span><span class=se>\</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=w>    </span>model-f16.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=w>    </span>model-q4_k_m.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=w>    </span>Q4_K_M
</span></code></pre></div> <p><strong>Available quantization types:</strong> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>Q4_0, Q4_1, Q5_0, Q5_1, Q8_0
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>IQ1_S, IQ2_XXS, IQ2_XS, IQ2_S, IQ3_XXS, IQ3_S
</span></code></pre></div></p> <h2 id=gguf-inspection-tools>GGUF Inspection Tools<a class=headerlink href=#gguf-inspection-tools title="Permanent link">&para;</a></h2> <h3 id=using-llcuda>Using llcuda<a class=headerlink href=#using-llcuda title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.gguf_parser</span><span class=w> </span><span class=kn>import</span> <span class=n>GGUFReader</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a><span class=n>reader</span> <span class=o>=</span> <span class=n>GGUFReader</span><span class=p>(</span><span class=s2>&quot;model.gguf&quot;</span><span class=p>)</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Architecture: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>architecture</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Quantization: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>quantization</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Parameter count: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>parameter_count</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Context length: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>context_length</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Embedding size: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>embedding_size</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Layers: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>num_layers</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Heads: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>num_heads</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;File size: </span><span class=si>{</span><span class=n>reader</span><span class=o>.</span><span class=n>file_size</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=mi>1024</span><span class=o>**</span><span class=mi>3</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> GB&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=using-llamacpp-tools>Using llama.cpp Tools<a class=headerlink href=#using-llamacpp-tools title="Permanent link">&para;</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=c1># Check GGUF metadata</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a>~/.cache/llcuda/bin/llama-cli<span class=w> </span><span class=se>\</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=w>    </span>--model<span class=w> </span>model.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=w>    </span>--verbose
</span></code></pre></div> <h2 id=model-compatibility>Model Compatibility<a class=headerlink href=#model-compatibility title="Permanent link">&para;</a></h2> <h3 id=supported-architectures>Supported Architectures<a class=headerlink href=#supported-architectures title="Permanent link">&para;</a></h3> <p>llcuda v2.1.0 supports these model architectures via GGUF:</p> <p>✅ <strong>LLaMA</strong> (LLaMA, LLaMA-2, LLaMA-3, LLaMA-3.1, LLaMA-3.2) ✅ <strong>Gemma</strong> (Gemma, Gemma-2, Gemma-3) ✅ <strong>Qwen</strong> (Qwen, Qwen-2, Qwen-2.5) ✅ <strong>Mistral</strong> (Mistral, Mistral-7B) ✅ <strong>Mixtral</strong> (Mixtral 8x7B, 8x22B) ✅ <strong>Phi</strong> (Phi-2, Phi-3) ✅ <strong>Yi</strong> (Yi-6B, Yi-34B) ✅ <strong>StableLM</strong> (StableLM-2, StableLM-3)</p> <h3 id=checking-compatibility>Checking Compatibility<a class=headerlink href=#checking-compatibility title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=c1># Check if model is compatible</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=n>compat</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>check_model_compatibility</span><span class=p>(</span><span class=s2>&quot;model.gguf&quot;</span><span class=p>)</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Compatible: </span><span class=si>{</span><span class=n>compat</span><span class=p>[</span><span class=s1>&#39;compatible&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Architecture: </span><span class=si>{</span><span class=n>compat</span><span class=p>[</span><span class=s1>&#39;architecture&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Warnings: </span><span class=si>{</span><span class=n>compat</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;warnings&#39;</span><span class=p>,</span><span class=w> </span><span class=p>[])</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=gguf-best-practices>GGUF Best Practices<a class=headerlink href=#gguf-best-practices title="Permanent link">&para;</a></h2> <h3 id=1-choose-right-quantization>1. Choose Right Quantization<a class=headerlink href=#1-choose-right-quantization title="Permanent link">&para;</a></h3> <p><strong>For Tesla T4:</strong> - <strong>Small models (1-3B):</strong> Q4_K_M or Q5_K_M - <strong>Medium models (7-8B):</strong> Q4_K_M (fits in VRAM) - <strong>Large models (13B+):</strong> Q4_K_M or Q3_K_M (if needed)</p> <h3 id=2-verify-gguf-integrity>2. Verify GGUF Integrity<a class=headerlink href=#2-verify-gguf-integrity title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.gguf_parser</span><span class=w> </span><span class=kn>import</span> <span class=n>GGUFReader</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=k>try</span><span class=p>:</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>    <span class=n>reader</span> <span class=o>=</span> <span class=n>GGUFReader</span><span class=p>(</span><span class=s2>&quot;model.gguf&quot;</span><span class=p>)</span>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;✅ Valid GGUF file&quot;</span><span class=p>)</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a><span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;❌ Invalid GGUF: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=3-test-before-production>3. Test Before Production<a class=headerlink href=#3-test-before-production title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=c1># Quick test</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;model.gguf&quot;</span><span class=p>,</span> <span class=n>silent</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a><span class=n>result</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Test prompt&quot;</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Output: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>text</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Speed: </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>tokens_per_sec</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span></code></pre></div> <h3 id=4-optimize-storage>4. Optimize Storage<a class=headerlink href=#4-optimize-storage title="Permanent link">&para;</a></h3> <p><strong>Use Q4_K_M for distribution:</strong> - Smaller download size - Faster loading - Good quality - Better inference speed</p> <h2 id=gguf-vs-other-formats>GGUF vs Other Formats<a class=headerlink href=#gguf-vs-other-formats title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th>Format</th> <th>Size</th> <th>Speed</th> <th>Compatibility</th> <th>Ease of Use</th> </tr> </thead> <tbody> <tr> <td><strong>GGUF</strong></td> <td>Small</td> <td>Fast</td> <td>llama.cpp</td> <td>✅ Easy</td> </tr> <tr> <td><strong>SafeTensors</strong></td> <td>Large</td> <td>Medium</td> <td>PyTorch</td> <td>Medium</td> </tr> <tr> <td><strong>PyTorch (.pt)</strong></td> <td>Large</td> <td>Medium</td> <td>PyTorch only</td> <td>Medium</td> </tr> <tr> <td><strong>ONNX</strong></td> <td>Large</td> <td>Fast</td> <td>ONNX Runtime</td> <td>Complex</td> </tr> <tr> <td><strong>TensorRT</strong></td> <td>Custom</td> <td>Fastest</td> <td>NVIDIA only</td> <td>Complex</td> </tr> </tbody> </table> <p><strong>Why GGUF for llcuda:</strong> - ✅ Smallest file size (with quantization) - ✅ Fast inference on CPU and GPU - ✅ Single-file distribution - ✅ Works with llama.cpp ecosystem - ✅ Easy to share and deploy</p> <h2 id=finding-gguf-models>Finding GGUF Models<a class=headerlink href=#finding-gguf-models title="Permanent link">&para;</a></h2> <h3 id=unsloth-huggingface>Unsloth HuggingFace<a class=headerlink href=#unsloth-huggingface title="Permanent link">&para;</a></h3> <p>Most popular source for GGUF models:</p> <p><a href=https://huggingface.co/unsloth>https://huggingface.co/unsloth</a></p> <p><strong>Example repositories:</strong> - <code>unsloth/gemma-3-1b-it-GGUF</code> - <code>unsloth/Llama-3.2-3B-Instruct-GGUF</code> - <code>unsloth/Qwen2.5-7B-Instruct-GGUF</code> - <code>unsloth/Meta-Llama-3.1-8B-Instruct-GGUF</code></p> <h3 id=thebloke-legacy>TheBloke (Legacy)<a class=headerlink href=#thebloke-legacy title="Permanent link">&para;</a></h3> <p>Older GGUF models (pre-Unsloth era):</p> <p><a href=https://huggingface.co/TheBloke>https://huggingface.co/TheBloke</a></p> <h3 id=bartowski>Bartowski<a class=headerlink href=#bartowski title="Permanent link">&para;</a></h3> <p>Recent high-quality quantizations:</p> <p><a href=https://huggingface.co/bartowski>https://huggingface.co/bartowski</a></p> <h2 id=troubleshooting-gguf-issues>Troubleshooting GGUF Issues<a class=headerlink href=#troubleshooting-gguf-issues title="Permanent link">&para;</a></h2> <h3 id=issue-invalid-gguf-magic-number>Issue: Invalid GGUF Magic Number<a class=headerlink href=#issue-invalid-gguf-magic-number title="Permanent link">&para;</a></h3> <p><strong>Error:</strong> <code>Invalid GGUF file: wrong magic number</code></p> <p><strong>Solution:</strong> - File is corrupted or incomplete - Re-download the GGUF file - Verify SHA256 checksum</p> <h3 id=issue-unsupported-quantization>Issue: Unsupported Quantization<a class=headerlink href=#issue-unsupported-quantization title="Permanent link">&para;</a></h3> <p><strong>Error:</strong> <code>Quantization type not supported</code></p> <p><strong>Solution:</strong> - Use Q4_K_M, Q5_K_M, or Q8_0 - Avoid experimental quantizations (IQ types) - Re-quantize with llama-quantize</p> <h3 id=issue-model-too-large>Issue: Model Too Large<a class=headerlink href=#issue-model-too-large title="Permanent link">&para;</a></h3> <p><strong>Error:</strong> <code>CUDA out of memory</code></p> <p><strong>Solution:</strong> - Use lower quantization (Q4_K_M instead of Q8_0) - Use smaller model variant - Clear GPU cache before loading</p> <h2 id=advanced-gguf-topics>Advanced GGUF Topics<a class=headerlink href=#advanced-gguf-topics title="Permanent link">&para;</a></h2> <h3 id=custom-metadata>Custom Metadata<a class=headerlink href=#custom-metadata title="Permanent link">&para;</a></h3> <p>Add custom metadata to GGUF:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=kn>from</span><span class=w> </span><span class=nn>llcuda.gguf_parser</span><span class=w> </span><span class=kn>import</span> <span class=n>GGUFWriter</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=n>writer</span> <span class=o>=</span> <span class=n>GGUFWriter</span><span class=p>(</span><span class=s2>&quot;output.gguf&quot;</span><span class=p>)</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=n>writer</span><span class=o>.</span><span class=n>add_metadata</span><span class=p>(</span><span class=s2>&quot;author&quot;</span><span class=p>,</span> <span class=s2>&quot;Your Name&quot;</span><span class=p>)</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=n>writer</span><span class=o>.</span><span class=n>add_metadata</span><span class=p>(</span><span class=s2>&quot;description&quot;</span><span class=p>,</span> <span class=s2>&quot;Fine-tuned for specific task&quot;</span><span class=p>)</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a><span class=n>writer</span><span class=o>.</span><span class=n>add_metadata</span><span class=p>(</span><span class=s2>&quot;license&quot;</span><span class=p>,</span> <span class=s2>&quot;MIT&quot;</span><span class=p>)</span>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=n>writer</span><span class=o>.</span><span class=n>finalize</span><span class=p>()</span>
</span></code></pre></div> <h3 id=merging-gguf-models>Merging GGUF Models<a class=headerlink href=#merging-gguf-models title="Permanent link">&para;</a></h3> <p>Combine multiple LoRA adapters (experimental):</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=c1># Using llama.cpp tools</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>llama-export-lora<span class=w> </span><span class=se>\</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=w>    </span>base-model.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a><span class=w>    </span>lora-adapter.gguf<span class=w> </span><span class=se>\</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a><span class=w>    </span>merged-model.gguf
</span></code></pre></div> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">&para;</a></h2> <ul> <li><strong>GGUF Specification:</strong> <a href=https://github.com/ggerganov/ggml/blob/master/docs/gguf.md>github.com/ggerganov/ggml/blob/master/docs/gguf.md</a></li> <li><strong>llama.cpp:</strong> <a href=https://github.com/ggerganov/llama.cpp>github.com/ggerganov/llama.cpp</a></li> <li><strong>Unsloth GGUF Export:</strong> <a href=https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf>docs.unsloth.ai/basics/saving-to-gguf</a></li> </ul> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <ul> <li><a href=../model-selection/ ><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 3h1v2H3V4a1 1 0 0 1 1-1m16 0a1 1 0 0 1 1 1v1h-2V3zm-5 2V3h2v2zm-4 0V3h2v2zM7 5V3h2v2zm14 15a1 1 0 0 1-1 1h-1v-2h2zm-6 1v-2h2v2zm-4 0v-2h2v2zm-4 0v-2h2v2zm-3 0a1 1 0 0 1-1-1v-1h2v2zm-1-6h2v2H3zm18 0v2h-2v-2zM3 11h2v2H3zm18 0v2h-2v-2zM3 7h2v2H3zm18 0v2h-2V7z"/></svg></span> Model Selection Guide</a> - Choose the right model</li> <li><a href=../quickstart/ ><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m20 22-3.86-1.55c.7-1.53 1.2-3.11 1.51-4.72zM7.86 20.45 4 22l2.35-6.27c.31 1.61.81 3.19 1.51 4.72M12 2s5 2 5 10c0 3.1-.75 5.75-1.67 7.83A2 2 0 0 1 13.5 21h-3a2 2 0 0 1-1.83-1.17C7.76 17.75 7 15.1 7 12c0-8 5-10 5-10m0 10c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2"/></svg></span> Quick Start</a> - Start using GGUF models</li> <li><a href=../../performance/benchmarks/ ><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 16a3 3 0 0 1-3-3c0-1.12.61-2.1 1.5-2.61l9.71-5.62-5.53 9.58c-.5.98-1.51 1.65-2.68 1.65m0-13c1.81 0 3.5.5 4.97 1.32l-2.1 1.21C14 5.19 13 5 12 5a8 8 0 0 0-8 8c0 2.21.89 4.21 2.34 5.65h.01c.39.39.39 1.02 0 1.41s-1.03.39-1.42.01A9.97 9.97 0 0 1 2 13 10 10 0 0 1 12 3m10 10c0 2.76-1.12 5.26-2.93 7.07-.39.38-1.02.38-1.41-.01a.996.996 0 0 1 0-1.41A7.95 7.95 0 0 0 20 13c0-1-.19-2-.54-2.9L20.67 8C21.5 9.5 22 11.18 22 13"/></svg></span> Performance</a> - Benchmark GGUF models</li> <li><a href=../../tutorials/unsloth-integration/ ><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 12h7v1.5h-7m0-4h7V11h-7m0 3.5h7V16h-7m8-12H3a2 2 0 0 0-2 2v13a2 2 0 0 0 2 2h18a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2m0 15h-9V6h9"/></svg></span> Unsloth Integration</a> - Create GGUF from fine-tuned models</li> </ul> <hr> <p><strong>GGUF makes LLM deployment simple and efficient!</strong> 🚀</p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve by <a href=https://github.com/llcuda/llcuda/issues/new target=_blank rel=noopener>opening an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024-2026 Waqas Muhammad </div> </div> <div class=md-social> <a href=https://github.com/llcuda/llcuda target=_blank rel=noopener title="GitHub - llcuda/llcuda" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/waqasm86 target=_blank rel=noopener title="GitHub - waqasm86" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=mailto:waqasm86@gmail.com target=_blank rel=noopener title="Email - waqasm86@gmail.com" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l208 156a48 48 0 0 0 57.6 0l208-156c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 196v188c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V196L313.6 344.8c-34.1 25.6-81.1 25.6-115.2 0z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/schema.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>