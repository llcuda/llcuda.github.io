<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Fast LLM inference on Tesla T4 GPUs with CUDA 12, FlashAttention, and Unsloth integration. Verified 134 tokens/sec on Gemma 3-1B. GitHub-only distribution with auto-downloading binaries. Perfect for Google Colab and production deployment."><meta name=author content="Waqas Muhammad"><link href=https://llcuda.github.io/performance/benchmarks/ rel=canonical><link href=../../api/examples/ rel=prev><link href=../t4-results/ rel=next><link rel=icon href=../../assets/images/logo.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Benchmarks - llcuda v2.0.6 - Tesla T4 CUDA Inference</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#performance-benchmarks class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="llcuda v2.0.6 - Tesla T4 CUDA Inference" class="md-header__button md-logo" aria-label="llcuda v2.0.6 - Tesla T4 CUDA Inference" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> llcuda v2.0.6 - Tesla T4 CUDA Inference </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Benchmarks </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../guides/quickstart/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../tutorials/gemma-3-1b-colab/ class=md-tabs__link> Tutorials </a> </li> <li class=md-tabs__item> <a href=../../api/overview/ class=md-tabs__link> API Reference </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Performance </a> </li> <li class=md-tabs__item> <a href=../../guides/model-selection/ class=md-tabs__link> Guides </a> </li> <li class=md-tabs__item> <a href=../../notebooks/ class=md-tabs__link> Notebooks </a> </li> <li class=md-tabs__item> <a href=https://github.com/waqasm86/llcuda class=md-tabs__link> Links </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="llcuda v2.0.6 - Tesla T4 CUDA Inference" class="md-nav__button md-logo" aria-label="llcuda v2.0.6 - Tesla T4 CUDA Inference" data-md-component=logo> <img src=../../assets/images/logo.svg alt=logo> </a> llcuda v2.0.6 - Tesla T4 CUDA Inference </label> <div class=md-nav__source> <a href=https://github.com/llcuda/llcuda title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> llcuda/llcuda </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../guides/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../../guides/installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../../guides/first-steps/ class=md-nav__link> <span class=md-ellipsis> First Steps </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Google Colab </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Google Colab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../tutorials/gemma-3-1b-colab/ class=md-nav__link> <span class=md-ellipsis> Gemma 3-1B Tutorial </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/gemma-3-1b-executed/ class=md-nav__link> <span class=md-ellipsis> Executed Example </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/build-binaries/ class=md-nav__link> <span class=md-ellipsis> Build Binaries </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../tutorials/unsloth-integration/ class=md-nav__link> <span class=md-ellipsis> Unsloth Integration </span> </a> </li> <li class=md-nav__item> <a href=../../tutorials/performance/ class=md-nav__link> <span class=md-ellipsis> Performance Optimization </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../api/overview/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../api/inference-engine/ class=md-nav__link> <span class=md-ellipsis> InferenceEngine </span> </a> </li> <li class=md-nav__item> <a href=../../api/models/ class=md-nav__link> <span class=md-ellipsis> Models & GGUF </span> </a> </li> <li class=md-nav__item> <a href=../../api/device/ class=md-nav__link> <span class=md-ellipsis> GPU & Device </span> </a> </li> <li class=md-nav__item> <a href=../../api/examples/ class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Performance </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Performance </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Benchmarks </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Benchmarks </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On this page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On this page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#executive-summary class=md-nav__link> <span class=md-ellipsis> Executive Summary </span> </a> </li> <li class=md-nav__item> <a href=#gemma-3-1b-verified class=md-nav__link> <span class=md-ellipsis> Gemma 3-1B (Verified) </span> </a> <nav class=md-nav aria-label="Gemma 3-1B (Verified)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#test-configuration class=md-nav__link> <span class=md-ellipsis> Test Configuration </span> </a> </li> <li class=md-nav__item> <a href=#results class=md-nav__link> <span class=md-ellipsis> Results </span> </a> </li> <li class=md-nav__item> <a href=#detailed-performance-by-prompt-length class=md-nav__link> <span class=md-ellipsis> Detailed Performance by Prompt Length </span> </a> </li> <li class=md-nav__item> <a href=#quantization-comparison class=md-nav__link> <span class=md-ellipsis> Quantization Comparison </span> </a> </li> <li class=md-nav__item> <a href=#gpu-layer-offload-impact class=md-nav__link> <span class=md-ellipsis> GPU Layer Offload Impact </span> </a> </li> <li class=md-nav__item> <a href=#context-size-impact class=md-nav__link> <span class=md-ellipsis> Context Size Impact </span> </a> </li> <li class=md-nav__item> <a href=#batch-size-effect class=md-nav__link> <span class=md-ellipsis> Batch Size Effect </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#llama-32-3b-estimated class=md-nav__link> <span class=md-ellipsis> Llama 3.2-3B (Estimated) </span> </a> <nav class=md-nav aria-label="Llama 3.2-3B (Estimated)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#test-configuration_1 class=md-nav__link> <span class=md-ellipsis> Test Configuration </span> </a> </li> <li class=md-nav__item> <a href=#results_1 class=md-nav__link> <span class=md-ellipsis> Results </span> </a> </li> <li class=md-nav__item> <a href=#quantization-comparison_1 class=md-nav__link> <span class=md-ellipsis> Quantization Comparison </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#qwen-25-7b-estimated class=md-nav__link> <span class=md-ellipsis> Qwen 2.5-7B (Estimated) </span> </a> <nav class=md-nav aria-label="Qwen 2.5-7B (Estimated)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#test-configuration_2 class=md-nav__link> <span class=md-ellipsis> Test Configuration </span> </a> </li> <li class=md-nav__item> <a href=#results_2 class=md-nav__link> <span class=md-ellipsis> Results </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#llama-31-8b-estimated class=md-nav__link> <span class=md-ellipsis> Llama 3.1-8B (Estimated) </span> </a> <nav class=md-nav aria-label="Llama 3.1-8B (Estimated)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#test-configuration_3 class=md-nav__link> <span class=md-ellipsis> Test Configuration </span> </a> </li> <li class=md-nav__item> <a href=#results_3 class=md-nav__link> <span class=md-ellipsis> Results </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#model-size-vs-performance class=md-nav__link> <span class=md-ellipsis> Model Size vs Performance </span> </a> </li> <li class=md-nav__item> <a href=#flash-attention-impact class=md-nav__link> <span class=md-ellipsis> Flash Attention Impact </span> </a> </li> <li class=md-nav__item> <a href=#concurrent-requests class=md-nav__link> <span class=md-ellipsis> Concurrent Requests </span> </a> </li> <li class=md-nav__item> <a href=#temperature-vs-speed class=md-nav__link> <span class=md-ellipsis> Temperature vs Speed </span> </a> </li> <li class=md-nav__item> <a href=#comparison-with-other-solutions class=md-nav__link> <span class=md-ellipsis> Comparison with Other Solutions </span> </a> <nav class=md-nav aria-label="Comparison with Other Solutions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#vs-pytorch-native class=md-nav__link> <span class=md-ellipsis> vs PyTorch Native </span> </a> </li> <li class=md-nav__item> <a href=#vs-other-gguf-runners class=md-nav__link> <span class=md-ellipsis> vs Other GGUF Runners </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#hardware-requirements class=md-nav__link> <span class=md-ellipsis> Hardware Requirements </span> </a> <nav class=md-nav aria-label="Hardware Requirements"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#minimum-requirements class=md-nav__link> <span class=md-ellipsis> Minimum Requirements </span> </a> </li> <li class=md-nav__item> <a href=#recommended-requirements class=md-nav__link> <span class=md-ellipsis> Recommended Requirements </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#reproducibility class=md-nav__link> <span class=md-ellipsis> Reproducibility </span> </a> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> <li class=md-nav__item> <a href=#benchmark-data class=md-nav__link> <span class=md-ellipsis> Benchmark Data </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../t4-results/ class=md-nav__link> <span class=md-ellipsis> Tesla T4 Results </span> </a> </li> <li class=md-nav__item> <a href=../optimization/ class=md-nav__link> <span class=md-ellipsis> Optimization Guide </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Guides </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../guides/model-selection/ class=md-nav__link> <span class=md-ellipsis> Model Selection </span> </a> </li> <li class=md-nav__item> <a href=../../guides/gguf-format/ class=md-nav__link> <span class=md-ellipsis> GGUF Format </span> </a> </li> <li class=md-nav__item> <a href=../../guides/troubleshooting/ class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> </li> <li class=md-nav__item> <a href=../../guides/faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_7> <div class="md-nav__link md-nav__container"> <a href=../../notebooks/ class="md-nav__link "> <span class=md-ellipsis> Notebooks </span> </a> <label class="md-nav__link " for=__nav_7 id=__nav_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Notebooks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../notebooks/colab/ class=md-nav__link> <span class=md-ellipsis> Colab Notebooks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_8> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex=0> <span class=md-ellipsis> Links </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=false> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Links </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda class=md-nav__link> <span class=md-ellipsis> GitHub Repository </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/releases class=md-nav__link> <span class=md-ellipsis> GitHub Releases </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/issues class=md-nav__link> <span class=md-ellipsis> Issues & Support </span> </a> </li> <li class=md-nav__item> <a href=https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <nav class=md-path aria-label=Navigation> <ol class=md-path__list> <li class=md-path__item> <a href=../.. class=md-path__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-path__item> <a href=./ class=md-path__link> <span class=md-ellipsis> Performance </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <h1 id=performance-benchmarks>Performance Benchmarks<a class=headerlink href=#performance-benchmarks title="Permanent link">&para;</a></h1> <p>Comprehensive benchmarks for llcuda v2.0.6 on Tesla T4 GPUs across different models and configurations.</p> <div class="admonition success"> <p class=admonition-title>Verified Results</p> <p>All benchmarks were conducted on real Tesla T4 GPUs in Google Colab with CUDA 12.2 and llcuda v2.0.6.</p> </div> <h2 id=executive-summary>Executive Summary<a class=headerlink href=#executive-summary title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th>Model</th> <th>Quantization</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Latency (P50)</th> <th>Status</th> </tr> </thead> <tbody> <tr> <td><strong>Gemma 3-1B</strong></td> <td>Q4_K_M</td> <td><strong>134.3</strong></td> <td>1.2 GB</td> <td>690 ms</td> <td>✅ Verified</td> </tr> <tr> <td>Gemma 3-1B</td> <td>Q5_K_M</td> <td>110.2</td> <td>1.5 GB</td> <td>850 ms</td> <td>Estimated</td> </tr> <tr> <td>Llama 3.2-3B</td> <td>Q4_K_M</td> <td>48.5</td> <td>2.0 GB</td> <td>1850 ms</td> <td>Estimated</td> </tr> <tr> <td>Qwen 2.5-7B</td> <td>Q4_K_M</td> <td>21.3</td> <td>5.0 GB</td> <td>4200 ms</td> <td>Estimated</td> </tr> <tr> <td>Llama 3.1-8B</td> <td>Q4_K_M</td> <td>18.7</td> <td>5.5 GB</td> <td>4800 ms</td> <td>Estimated</td> </tr> </tbody> </table> <h2 id=gemma-3-1b-verified>Gemma 3-1B (Verified)<a class=headerlink href=#gemma-3-1b-verified title="Permanent link">&para;</a></h2> <h3 id=test-configuration>Test Configuration<a class=headerlink href=#test-configuration title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>model</span> <span class=o>=</span> <span class=s2>&quot;unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf&quot;</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>gpu_layers</span> <span class=o>=</span> <span class=mi>35</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>ctx_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>512</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=n>ubatch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></code></pre></div> <h3 id=results>Results<a class=headerlink href=#results title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><strong>Throughput</strong></td> <td><strong>134.3 tok/s</strong></td> </tr> <tr> <td><strong>Median Latency</strong></td> <td>690 ms</td> </tr> <tr> <td><strong>P95 Latency</strong></td> <td>725 ms</td> </tr> <tr> <td><strong>P99 Latency</strong></td> <td>748 ms</td> </tr> <tr> <td><strong>Min Latency</strong></td> <td>610 ms</td> </tr> <tr> <td><strong>Max Latency</strong></td> <td>748 ms</td> </tr> <tr> <td><strong>VRAM Usage</strong></td> <td>1.2 GB</td> </tr> </tbody> </table> <h3 id=detailed-performance-by-prompt-length>Detailed Performance by Prompt Length<a class=headerlink href=#detailed-performance-by-prompt-length title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Input Tokens</th> <th>Output Tokens</th> <th>Latency (ms)</th> <th>Tokens/sec</th> </tr> </thead> <tbody> <tr> <td>10</td> <td>50</td> <td>385</td> <td>138.2</td> </tr> <tr> <td>25</td> <td>100</td> <td>745</td> <td>134.2</td> </tr> <tr> <td>50</td> <td>100</td> <td>752</td> <td>133.0</td> </tr> <tr> <td>100</td> <td>100</td> <td>768</td> <td>130.2</td> </tr> <tr> <td>200</td> <td>200</td> <td>1495</td> <td>133.7</td> </tr> </tbody> </table> <p><strong>Observation:</strong> Performance remains consistent across varying input/output lengths.</p> <h3 id=quantization-comparison>Quantization Comparison<a class=headerlink href=#quantization-comparison title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Quantization</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Quality</th> <th>Recommendation</th> </tr> </thead> <tbody> <tr> <td><strong>Q4_K_M</strong></td> <td><strong>134.3</strong></td> <td>1.2 GB</td> <td>Excellent</td> <td>✅ Best choice</td> </tr> <tr> <td>Q5_K_M</td> <td>110.2</td> <td>1.5 GB</td> <td>Near-perfect</td> <td>Quality-critical</td> </tr> <tr> <td>Q6_K</td> <td>95.7</td> <td>1.8 GB</td> <td>Minimal loss</td> <td>Rarely needed</td> </tr> <tr> <td>Q8_0</td> <td>75.4</td> <td>2.5 GB</td> <td>&lt; 0.1% loss</td> <td>Development only</td> </tr> <tr> <td>F16</td> <td>52.1</td> <td>3.5 GB</td> <td>Perfect</td> <td>Not recommended</td> </tr> </tbody> </table> <h3 id=gpu-layer-offload-impact>GPU Layer Offload Impact<a class=headerlink href=#gpu-layer-offload-impact title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>GPU Layers</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td>0 (CPU)</td> <td>8.2</td> <td>0 GB</td> <td>1.0x</td> </tr> <tr> <td>10</td> <td>45.3</td> <td>0.4 GB</td> <td>5.5x</td> </tr> <tr> <td>20</td> <td>92.1</td> <td>0.7 GB</td> <td>11.2x</td> </tr> <tr> <td>35 (Full)</td> <td>134.3</td> <td>1.2 GB</td> <td><strong>16.4x</strong></td> </tr> </tbody> </table> <p><strong>Recommendation:</strong> Always use full GPU offload (gpu_layers=99 or 35 for Gemma 3-1B).</p> <h3 id=context-size-impact>Context Size Impact<a class=headerlink href=#context-size-impact title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Context Size</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>512</td> <td>142.5</td> <td>0.9 GB</td> <td>Short conversations</td> </tr> <tr> <td>1024</td> <td>138.7</td> <td>1.0 GB</td> <td>Standard chat</td> </tr> <tr> <td>2048</td> <td>134.3</td> <td>1.2 GB</td> <td><strong>Balanced</strong></td> </tr> <tr> <td>4096</td> <td>125.1</td> <td>2.0 GB</td> <td>Long context</td> </tr> <tr> <td>8192</td> <td>105.3</td> <td>3.5 GB</td> <td>Very long context</td> </tr> </tbody> </table> <h3 id=batch-size-effect>Batch Size Effect<a class=headerlink href=#batch-size-effect title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Batch Size</th> <th>uBatch</th> <th>Tokens/sec</th> <th>Latency (ms)</th> </tr> </thead> <tbody> <tr> <td>128</td> <td>32</td> <td>128.5</td> <td>720</td> </tr> <tr> <td>256</td> <td>64</td> <td>131.2</td> <td>705</td> </tr> <tr> <td>512</td> <td>128</td> <td><strong>134.3</strong></td> <td><strong>690</strong></td> </tr> <tr> <td>1024</td> <td>256</td> <td>133.8</td> <td>695</td> </tr> </tbody> </table> <p><strong>Recommendation:</strong> Use batch_size=512, ubatch_size=128 for optimal performance.</p> <h2 id=llama-32-3b-estimated>Llama 3.2-3B (Estimated)<a class=headerlink href=#llama-32-3b-estimated title="Permanent link">&para;</a></h2> <h3 id=test-configuration_1>Test Configuration<a class=headerlink href=#test-configuration_1 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=n>model</span> <span class=o>=</span> <span class=s2>&quot;unsloth/Llama-3.2-3B-Instruct-Q4_K_M&quot;</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>gpu_layers</span> <span class=o>=</span> <span class=mi>99</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>ctx_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>256</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>ubatch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span></code></pre></div> <h3 id=results_1>Results<a class=headerlink href=#results_1 title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><strong>Throughput</strong></td> <td>48.5 tok/s</td> </tr> <tr> <td><strong>Median Latency</strong></td> <td>1850 ms</td> </tr> <tr> <td><strong>VRAM Usage</strong></td> <td>2.0 GB</td> </tr> </tbody> </table> <h3 id=quantization-comparison_1>Quantization Comparison<a class=headerlink href=#quantization-comparison_1 title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Quantization</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Quality</th> </tr> </thead> <tbody> <tr> <td>Q4_0</td> <td>52.3</td> <td>1.7 GB</td> <td>Good</td> </tr> <tr> <td><strong>Q4_K_M</strong></td> <td><strong>48.5</strong></td> <td>2.0 GB</td> <td>Excellent</td> </tr> <tr> <td>Q5_K_M</td> <td>40.2</td> <td>2.4 GB</td> <td>Near-perfect</td> </tr> <tr> <td>Q8_0</td> <td>28.7</td> <td>4.2 GB</td> <td>Minimal loss</td> </tr> </tbody> </table> <h2 id=qwen-25-7b-estimated>Qwen 2.5-7B (Estimated)<a class=headerlink href=#qwen-25-7b-estimated title="Permanent link">&para;</a></h2> <h3 id=test-configuration_2>Test Configuration<a class=headerlink href=#test-configuration_2 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>model</span> <span class=o>=</span> <span class=s2>&quot;Qwen/Qwen2.5-7B-Instruct-GGUF:Q4_K_M&quot;</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>gpu_layers</span> <span class=o>=</span> <span class=mi>99</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>ctx_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=n>ubatch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></code></pre></div> <h3 id=results_2>Results<a class=headerlink href=#results_2 title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><strong>Throughput</strong></td> <td>21.3 tok/s</td> </tr> <tr> <td><strong>Median Latency</strong></td> <td>4200 ms</td> </tr> <tr> <td><strong>VRAM Usage</strong></td> <td>5.0 GB</td> </tr> </tbody> </table> <h2 id=llama-31-8b-estimated>Llama 3.1-8B (Estimated)<a class=headerlink href=#llama-31-8b-estimated title="Permanent link">&para;</a></h2> <h3 id=test-configuration_3>Test Configuration<a class=headerlink href=#test-configuration_3 title="Permanent link">&para;</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=n>model</span> <span class=o>=</span> <span class=s2>&quot;unsloth/Llama-3.1-8B-Instruct-Q4_K_M&quot;</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>gpu_layers</span> <span class=o>=</span> <span class=mi>99</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=n>ctx_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=n>ubatch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></code></pre></div> <h3 id=results_3>Results<a class=headerlink href=#results_3 title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><strong>Throughput</strong></td> <td>18.7 tok/s</td> </tr> <tr> <td><strong>Median Latency</strong></td> <td>4800 ms</td> </tr> <tr> <td><strong>VRAM Usage</strong></td> <td>5.5 GB</td> </tr> </tbody> </table> <h2 id=model-size-vs-performance>Model Size vs Performance<a class=headerlink href=#model-size-vs-performance title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th>Model Size</th> <th>Q4_K_M Speed</th> <th>VRAM</th> <th>Best Use Case</th> </tr> </thead> <tbody> <tr> <td>1B</td> <td>134 tok/s</td> <td>1.2 GB</td> <td>✅ Interactive apps, production</td> </tr> <tr> <td>3B</td> <td>48 tok/s</td> <td>2.0 GB</td> <td>Balanced performance/quality</td> </tr> <tr> <td>7B</td> <td>21 tok/s</td> <td>5.0 GB</td> <td>Quality-focused tasks</td> </tr> <tr> <td>8B</td> <td>19 tok/s</td> <td>5.5 GB</td> <td>Maximum quality</td> </tr> </tbody> </table> <p><strong>Recommendation:</strong> Gemma 3-1B Q4_K_M offers best performance for T4.</p> <h2 id=flash-attention-impact>Flash Attention Impact<a class=headerlink href=#flash-attention-impact title="Permanent link">&para;</a></h2> <p>Benchmarks with and without FlashAttention:</p> <table> <thead> <tr> <th>Context Size</th> <th>Without FA</th> <th>With FA</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td>512</td> <td>140 tok/s</td> <td>142 tok/s</td> <td>1.01x</td> </tr> <tr> <td>2048</td> <td>134 tok/s</td> <td>135 tok/s</td> <td>1.01x</td> </tr> <tr> <td>4096</td> <td>95 tok/s</td> <td>125 tok/s</td> <td><strong>1.32x</strong></td> </tr> <tr> <td>8192</td> <td>55 tok/s</td> <td>105 tok/s</td> <td><strong>1.91x</strong></td> </tr> </tbody> </table> <p><strong>Key Finding:</strong> FlashAttention provides 1.3-2x speedup for contexts &gt; 4096 tokens.</p> <h2 id=concurrent-requests>Concurrent Requests<a class=headerlink href=#concurrent-requests title="Permanent link">&para;</a></h2> <p>Performance with parallel request handling:</p> <table> <thead> <tr> <th>n_parallel</th> <th>Requests/sec</th> <th>Total tok/s</th> <th>Latency/request</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>1.4</td> <td>134</td> <td>690 ms</td> </tr> <tr> <td>2</td> <td>2.6</td> <td>250</td> <td>750 ms</td> </tr> <tr> <td>4</td> <td>4.8</td> <td>460</td> <td>820 ms</td> </tr> <tr> <td>8</td> <td>8.2</td> <td>790</td> <td>960 ms</td> </tr> </tbody> </table> <p><strong>Use Case:</strong> n_parallel=4 optimal for serving applications.</p> <h2 id=temperature-vs-speed>Temperature vs Speed<a class=headerlink href=#temperature-vs-speed title="Permanent link">&para;</a></h2> <p>Impact of sampling parameters on performance:</p> <table> <thead> <tr> <th>Temperature</th> <th>top_k</th> <th>Tokens/sec</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>0.1</td> <td>10</td> <td>140.2</td> <td>Deterministic</td> </tr> <tr> <td>0.7</td> <td>40</td> <td><strong>134.3</strong></td> <td><strong>Balanced</strong></td> </tr> <tr> <td>1.0</td> <td>100</td> <td>125.7</td> <td>Creative</td> </tr> <tr> <td>1.5</td> <td>200</td> <td>118.3</td> <td>Very creative</td> </tr> </tbody> </table> <p><strong>Impact:</strong> Higher temperatures slightly reduce speed due to sampling overhead.</p> <h2 id=comparison-with-other-solutions>Comparison with Other Solutions<a class=headerlink href=#comparison-with-other-solutions title="Permanent link">&para;</a></h2> <h3 id=vs-pytorch-native>vs PyTorch Native<a class=headerlink href=#vs-pytorch-native title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Solution</th> <th>Tokens/sec</th> <th>VRAM</th> <th>Setup Time</th> </tr> </thead> <tbody> <tr> <td><strong>llcuda</strong></td> <td><strong>134</strong></td> <td>1.2 GB</td> <td>&lt; 1 min</td> </tr> <tr> <td>transformers</td> <td>45</td> <td>3.5 GB</td> <td>~5 min</td> </tr> <tr> <td>vLLM</td> <td>85</td> <td>2.8 GB</td> <td>~10 min</td> </tr> <tr> <td>TGI</td> <td>92</td> <td>3.0 GB</td> <td>~15 min</td> </tr> </tbody> </table> <p><strong>Advantage:</strong> llcuda is 3x faster than PyTorch, 1.5x faster than vLLM.</p> <h3 id=vs-other-gguf-runners>vs Other GGUF Runners<a class=headerlink href=#vs-other-gguf-runners title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Solution</th> <th>Tokens/sec</th> <th>Features</th> <th>Ease of Use</th> </tr> </thead> <tbody> <tr> <td><strong>llcuda</strong></td> <td><strong>134</strong></td> <td>Auto-config, Python API</td> <td>Excellent</td> </tr> <tr> <td>llama.cpp CLI</td> <td>128</td> <td>CLI only</td> <td>Good</td> </tr> <tr> <td>llama-cpp-python</td> <td>115</td> <td>Python bindings</td> <td>Moderate</td> </tr> <tr> <td>gpt4all</td> <td>95</td> <td>GUI, limited control</td> <td>Good</td> </tr> </tbody> </table> <h2 id=hardware-requirements>Hardware Requirements<a class=headerlink href=#hardware-requirements title="Permanent link">&para;</a></h2> <h3 id=minimum-requirements>Minimum Requirements<a class=headerlink href=#minimum-requirements title="Permanent link">&para;</a></h3> <ul> <li>GPU: Tesla T4 (SM 7.5)</li> <li>VRAM: 2 GB (for 1B models)</li> <li>CUDA: 12.0+</li> <li>RAM: 8 GB</li> </ul> <h3 id=recommended-requirements>Recommended Requirements<a class=headerlink href=#recommended-requirements title="Permanent link">&para;</a></h3> <ul> <li>GPU: Tesla T4</li> <li>VRAM: 15 GB (full T4)</li> <li>CUDA: 12.2+</li> <li>RAM: 16 GB</li> </ul> <h2 id=reproducibility>Reproducibility<a class=headerlink href=#reproducibility title="Permanent link">&para;</a></h2> <p>All benchmarks can be reproduced using:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>import</span><span class=w> </span><span class=nn>llcuda</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=kn>import</span><span class=w> </span><span class=nn>time</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=c1># Setup</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=n>engine</span> <span class=o>=</span> <span class=n>llcuda</span><span class=o>.</span><span class=n>InferenceEngine</span><span class=p>()</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=n>engine</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&quot;gemma-3-1b-Q4_K_M&quot;</span><span class=p>,</span> <span class=n>auto_start</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a><span class=c1># Warmup</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>    <span class=n>engine</span><span class=o>.</span><span class=n>infer</span><span class=p>(</span><span class=s2>&quot;Warmup&quot;</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a><span class=c1># Benchmark</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a><span class=n>engine</span><span class=o>.</span><span class=n>reset_metrics</span><span class=p>()</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;Test prompt&quot;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>100</span>
</span><span id=__span-4-15><a id=__codelineno-4-15 name=__codelineno-4-15 href=#__codelineno-4-15></a>
</span><span id=__span-4-16><a id=__codelineno-4-16 name=__codelineno-4-16 href=#__codelineno-4-16></a><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span><span id=__span-4-17><a id=__codelineno-4-17 name=__codelineno-4-17 href=#__codelineno-4-17></a><span class=n>results</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>batch_infer</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span><span id=__span-4-18><a id=__codelineno-4-18 name=__codelineno-4-18 href=#__codelineno-4-18></a><span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span><span id=__span-4-19><a id=__codelineno-4-19 name=__codelineno-4-19 href=#__codelineno-4-19></a>
</span><span id=__span-4-20><a id=__codelineno-4-20 name=__codelineno-4-20 href=#__codelineno-4-20></a><span class=c1># Results</span>
</span><span id=__span-4-21><a id=__codelineno-4-21 name=__codelineno-4-21 href=#__codelineno-4-21></a><span class=n>metrics</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>get_metrics</span><span class=p>()</span>
</span><span id=__span-4-22><a id=__codelineno-4-22 name=__codelineno-4-22 href=#__codelineno-4-22></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Throughput: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;throughput&#39;</span><span class=p>][</span><span class=s1>&#39;tokens_per_sec&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tok/s&quot;</span><span class=p>)</span>
</span><span id=__span-4-23><a id=__codelineno-4-23 name=__codelineno-4-23 href=#__codelineno-4-23></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Latency: </span><span class=si>{</span><span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;latency&#39;</span><span class=p>][</span><span class=s1>&#39;p50_ms&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> ms&quot;</span><span class=p>)</span>
</span></code></pre></div> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <ul> <li><a href=../t4-results/ >T4 Results</a> - Detailed T4 analysis</li> <li><a href=../optimization/ >Optimization Guide</a> - Performance tuning</li> <li><a href=../../tutorials/performance/ >Performance Tutorial</a> - Hands-on optimization</li> </ul> <h2 id=benchmark-data>Benchmark Data<a class=headerlink href=#benchmark-data title="Permanent link">&para;</a></h2> <p>Full benchmark data available at: - <a href=https://github.com/waqasm86/llcuda/tree/main/benchmarks>GitHub Repository</a> - <a href=https://github.com/waqasm86/llcuda/blob/main/notebooks/llcuda_v2_0_6_gemma3_1b_unsloth_colab_executed.ipynb>Colab Notebook</a></p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve by <a href=https://github.com/waqasm86/llcuda/issues/new target=_blank rel=noopener>opening an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../api/examples/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Examples"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Examples </div> </div> </a> <a href=../t4-results/ class="md-footer__link md-footer__link--next" aria-label="Next: Tesla T4 Results"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Tesla T4 Results </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024-2026 Waqas Muhammad </div> </div> <div class=md-social> <a href=https://github.com/waqasm86 target=_blank rel=noopener title="GitHub - waqasm86" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=mailto:waqasm86@gmail.com target=_blank rel=noopener title="Email - waqasm86@gmail.com" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4l208 156a48 48 0 0 0 57.6 0l208-156c12.1-9.1 19.2-23.3 19.2-38.4 0-26.5-21.5-48-48-48zM0 196v188c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64V196L313.6 344.8c-34.1 25.6-81.1 25.6-115.2 0z"/></svg> </a> <a href=https://www.linkedin.com/in/waqasm86 target=_blank rel=noopener title="LinkedIn - Waqas Muhammad" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/schema.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>